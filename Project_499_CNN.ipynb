{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Project_499_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NongNNew/Project_499/blob/main/Project_499_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW4AHGmEVrk8"
      },
      "source": [
        "# **CNN**"
      ],
      "id": "hW4AHGmEVrk8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkYRb02rScfH"
      },
      "source": [
        "## Setup"
      ],
      "id": "nkYRb02rScfH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq11bNFAbCci",
        "outputId": "d788482d-830a-4e3b-ec94-5cdb308ad35f"
      },
      "source": [
        "!pip install python_speech_features"
      ],
      "id": "Nq11bNFAbCci",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python_speech_features\n",
            "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
            "Building wheels for collected packages: python-speech-features\n",
            "  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-speech-features: filename=python_speech_features-0.6-py3-none-any.whl size=5888 sha256=8a7f7ade11ab2f8cdd94f0c15b16db7bdc94d29ed87043a9b3527f816c5e54a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/0e/94/28cd6afa3cd5998a63eef99fe31777acd7d758f59cf24839eb\n",
            "Successfully built python-speech-features\n",
            "Installing collected packages: python-speech-features\n",
            "Successfully installed python-speech-features-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fssuU2ucR1vy"
      },
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "import python_speech_features\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal.windows import hamming\n",
        "from sklearn.metrics import classification_report,  plot_confusion_matrix"
      ],
      "id": "fssuU2ucR1vy",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1vuUlgJSKkC"
      },
      "source": [
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "id": "y1vuUlgJSKkC",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJVdtdww87RO",
        "outputId": "dc74c871-faa4-4da4-b780-3826febdf0f6"
      },
      "source": [
        "# Upload audio files from github\n",
        "!git clone https://github.com/NongNNew/Project_499.git"
      ],
      "id": "gJVdtdww87RO",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Project_499'...\n",
            "remote: Enumerating objects: 1565, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 1565 (delta 51), reused 96 (delta 39), pack-reused 1444\u001b[K\n",
            "Receiving objects: 100% (1565/1565), 237.73 MiB | 28.38 MiB/s, done.\n",
            "Resolving deltas: 100% (199/199), done.\n",
            "Checking out files: 100% (1145/1145), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRWP2IrOShCH"
      },
      "source": [
        "## Prepare dataset"
      ],
      "id": "BRWP2IrOShCH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHnDUT5YejVV"
      },
      "source": [
        "# Name of vegetables and fruits 52 types of 56 classes\n",
        "fruit_veget = ['กระชาย','กระท้อน','กระเทียม1','กระเทียม2','กระเพรา','กล้วยน้ำว้า','กล้วยหอม',\n",
        "               'ข้าวโพด','ไข่น้ำ',\n",
        "               'ตะขบไทย','ตะไคร้',\n",
        "               'ถั่วฝักยาว','ถั่วลันเตา','ถั่วลิสง',\n",
        "               'ทับทิม',\n",
        "               'น้อยหน่า','น้ำเต้า',\n",
        "               'ผักกระเฉด','ผักกุยช่าย','ผักขึ้นช่าย','ผักชะอม','ผักชี','ผักชีฝรั่ง','ผักตำลึง',\n",
        "               'มะปราง','มะพลับ','มะละกอ','มะกรูด','มะเขือพวง','มะเขือเทศ','มะระ','มะรุม','มะตูม','มันแกว1','มันแกว2','มันเทศ','มันฝรั่ง','มันสำปะหลัง',\n",
        "               'บวบ','ใบชะพลู1','ใบชะพลู2','ใบบัวบก','ใบแมงลัก','ใบยอ',\n",
        "               'พริกขี้หนู','พริกสด','พุทรา','เพกา',\n",
        "               'ฝรั่ง',\n",
        "               'ฟัก','ฟักทอง',\n",
        "               'สับปะรด','สะเดา','สะระแหน่',\n",
        "               'หัวหอม1','หัวหอม2']\n",
        "\n",
        "# Audio data source\n",
        "source = {'audio_time':[],\n",
        "          'sampling_rate':[],\n",
        "          'feature_extraction':[],\n",
        "          'label':[],\n",
        "          'description':[]}"
      ],
      "id": "EHnDUT5YejVV",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XRqKtGBeW_1"
      },
      "source": [
        "def feature_mfcc(y,sr):\n",
        "    return (python_speech_features.mfcc(signal=y, \n",
        "                                    samplerate=sr, \n",
        "                                    winlen= 512/sr, \n",
        "                                    winstep= 160/sr,\n",
        "                                    numcep= 13,\n",
        "                                    nfilt= 40, \n",
        "                                    nfft= 512,\n",
        "                                    lowfreq= 0,\n",
        "                                    highfreq= None,\n",
        "                                    preemph= 0.97, \n",
        "                                    ceplifter= 0,\n",
        "                                    winfunc= hamming))"
      ],
      "id": "_XRqKtGBeW_1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tixApnAUy5tv"
      },
      "source": [
        "for types in fruit_veget:\n",
        "    audio_files = glob('/content/Project_499/Record_audio/' + str(types) + '/*.wav')\n",
        "\n",
        "    for audio in audio_files:\n",
        "        y,sr = librosa.load(audio,duration=5,offset=0)\n",
        "        source['description'].append(str(types))\n",
        "        source['audio_time'].append(y)\n",
        "        source['sampling_rate'].append(sr)\n",
        "        source['feature_extraction'].append(feature_mfcc(y,sr))\n",
        "        source['label'].append(fruit_veget.index(types))"
      ],
      "id": "tixApnAUy5tv",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "8B6cSV4Ckude",
        "outputId": "34e66314-5b79-4a82-bfb7-ecc225982e84"
      },
      "source": [
        "# tranform dictionary to dataframe \n",
        "df = pd.DataFrame.from_dict(source)\n",
        "df"
      ],
      "id": "8B6cSV4Ckude",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>audio_time</th>\n",
              "      <th>sampling_rate</th>\n",
              "      <th>feature_extraction</th>\n",
              "      <th>label</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.004928589, -0.0049591064, -0.0048980713, -...</td>\n",
              "      <td>22050</td>\n",
              "      <td>[[-12.959169471954294, -4.281586674957866, 6.4...</td>\n",
              "      <td>0</td>\n",
              "      <td>กระชาย</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-7.6293945e-05, -0.00015258789, -0.0001068115...</td>\n",
              "      <td>22050</td>\n",
              "      <td>[[-13.963624272836848, -10.834306371234895, 4....</td>\n",
              "      <td>0</td>\n",
              "      <td>กระชาย</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.0004119873, 0.00061035156, 0.0005340576, 0....</td>\n",
              "      <td>22050</td>\n",
              "      <td>[[-14.031458292064183, -9.71991818216831, 3.08...</td>\n",
              "      <td>0</td>\n",
              "      <td>กระชาย</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-0.0009765625, -0.001449585, -0.001373291, -0...</td>\n",
              "      <td>22050</td>\n",
              "      <td>[[-13.841251857084886, -10.542052145166839, 1....</td>\n",
              "      <td>0</td>\n",
              "      <td>กระชาย</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1.5258789e-05, -3.0517578e-05, 4.5776367e-05,...</td>\n",
              "      <td>22050</td>\n",
              "      <td>[[-13.895163210723114, -10.534242113550562, 0....</td>\n",
              "      <td>0</td>\n",
              "      <td>กระชาย</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1115</th>\n",
              "      <td>[-0.0001373291, -9.1552734e-05, -7.6293945e-05...</td>\n",
              "      <td>22050</td>\n",
              "      <td>[[-14.082713493333289, -12.10106746611324, -0....</td>\n",
              "      <td>55</td>\n",
              "      <td>หัวหอม2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>[-0.0001373291, -0.0001373291, -9.1552734e-05,...</td>\n",
              "      <td>22050</td>\n",
              "      <td>[[-13.96050509926212, -9.312697095263102, 1.11...</td>\n",
              "      <td>55</td>\n",
              "      <td>หัวหอม2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1117</th>\n",
              "      <td>[3.0517578e-05, -6.1035156e-05, 6.1035156e-05,...</td>\n",
              "      <td>22050</td>\n",
              "      <td>[[-14.050800633014068, -10.083539910076112, 2....</td>\n",
              "      <td>55</td>\n",
              "      <td>หัวหอม2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1118</th>\n",
              "      <td>[0.0, -1.5258789e-05, 3.0517578e-05, -3.051757...</td>\n",
              "      <td>22050</td>\n",
              "      <td>[[-14.18512183869517, -10.797166196952473, 3.3...</td>\n",
              "      <td>55</td>\n",
              "      <td>หัวหอม2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119</th>\n",
              "      <td>[-0.00021362305, -0.00019836426, -0.0001678466...</td>\n",
              "      <td>22050</td>\n",
              "      <td>[[-13.600642153773194, -15.096089567076714, -0...</td>\n",
              "      <td>55</td>\n",
              "      <td>หัวหอม2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1120 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             audio_time  ...  description\n",
              "0     [-0.004928589, -0.0049591064, -0.0048980713, -...  ...       กระชาย\n",
              "1     [-7.6293945e-05, -0.00015258789, -0.0001068115...  ...       กระชาย\n",
              "2     [0.0004119873, 0.00061035156, 0.0005340576, 0....  ...       กระชาย\n",
              "3     [-0.0009765625, -0.001449585, -0.001373291, -0...  ...       กระชาย\n",
              "4     [1.5258789e-05, -3.0517578e-05, 4.5776367e-05,...  ...       กระชาย\n",
              "...                                                 ...  ...          ...\n",
              "1115  [-0.0001373291, -9.1552734e-05, -7.6293945e-05...  ...      หัวหอม2\n",
              "1116  [-0.0001373291, -0.0001373291, -9.1552734e-05,...  ...      หัวหอม2\n",
              "1117  [3.0517578e-05, -6.1035156e-05, 6.1035156e-05,...  ...      หัวหอม2\n",
              "1118  [0.0, -1.5258789e-05, 3.0517578e-05, -3.051757...  ...      หัวหอม2\n",
              "1119  [-0.00021362305, -0.00019836426, -0.0001678466...  ...      หัวหอม2\n",
              "\n",
              "[1120 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7yzW3hnmuyB"
      },
      "source": [
        "Test = pd.DataFrame(columns=source.keys())\n",
        "\n",
        "# Create test data\n",
        "\n",
        "for description in np.unique(df['description']):\n",
        "    imp_test = df[df['description']==description].sample(5,random_state=1)\n",
        "    Test = pd.concat([Test,imp_test], axis=0)\n",
        "Train = df.drop(Test.index)\n",
        "\n",
        "X_train = np.array(Train['feature_extraction'].to_list())\n",
        "X_test = np.array(Test['feature_extraction'].to_list())\n",
        "y_train = np.array(Train['label'].to_list())\n",
        "y_test = np.array(Test['label'].to_list())"
      ],
      "id": "T7yzW3hnmuyB",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSV3u9RmEU7h",
        "outputId": "7d8197b2-5251-465a-8840-ea7e10e8f3fe"
      },
      "source": [
        "# Proportion of data\n",
        "y_train.shape, y_test.shape"
      ],
      "id": "mSV3u9RmEU7h",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((840,), (280,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7RGaVViSrwg"
      },
      "source": [
        "## Build cnn model"
      ],
      "id": "r7RGaVViSrwg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77wMVqve6W8f"
      },
      "source": [
        "def create_cnn_model():\n",
        "    np.random.seed(1)\n",
        "    tf.random.set_seed(1)\n",
        "    model = None\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Conv2D(128, 5, input_shape=(X_train.shape[1], X_train.shape[2], 1), activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.25))\n",
        "    model.add(keras.layers.MaxPool2D())\n",
        "    model.add(keras.layers.Conv2D(64, 5, activation='relu',padding='same'))\n",
        "    model.add(keras.layers.Dropout(0.25))\n",
        "    model.add(keras.layers.MaxPool2D())\n",
        "    model.add(keras.layers.Conv2D(64, 5, activation='relu',padding='same'))\n",
        "    model.add(keras.layers.Dropout(0.25))\n",
        "    model.add(keras.layers.MaxPool2D())\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.25))\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.25))\n",
        "    model.add(keras.layers.Dense(len(np.unique(df['label'])), activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "id": "77wMVqve6W8f",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb5Hbazg0m69"
      },
      "source": [
        "## Find best parameter"
      ],
      "id": "eb5Hbazg0m69"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tU7GkS21aBX"
      },
      "source": [
        "### find dropout and batch_size"
      ],
      "id": "0tU7GkS21aBX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXVQO4Hd-N8-",
        "outputId": "4373206b-fc5f-4e00-9e88-26b2daa030fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_cnn_model)\n",
        "\n",
        "# define the grid search parameters\n",
        "batch_size = np.arange(10,210,10)\n",
        "epochs = np.arange(10,310,10)\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5).fit(X_train[:,:,:,None],y_train)"
      ],
      "id": "oXVQO4Hd-N8-",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1530 - accuracy: 0.9673\n",
            "Epoch 83/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1278 - accuracy: 0.9732\n",
            "Epoch 84/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1888 - accuracy: 0.9539\n",
            "Epoch 85/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2724 - accuracy: 0.9539\n",
            "Epoch 86/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1522 - accuracy: 0.9643\n",
            "Epoch 87/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0922 - accuracy: 0.9717\n",
            "Epoch 88/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0788 - accuracy: 0.9807\n",
            "Epoch 89/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1650 - accuracy: 0.9628\n",
            "Epoch 90/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1163 - accuracy: 0.9717\n",
            "Epoch 91/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2584 - accuracy: 0.9539\n",
            "Epoch 92/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1054 - accuracy: 0.9732\n",
            "Epoch 93/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1452 - accuracy: 0.9643\n",
            "Epoch 94/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0558 - accuracy: 0.9777\n",
            "Epoch 95/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1110 - accuracy: 0.9732\n",
            "Epoch 96/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0508 - accuracy: 0.9836\n",
            "Epoch 97/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0830 - accuracy: 0.9836\n",
            "Epoch 98/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0641 - accuracy: 0.9851\n",
            "Epoch 99/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0364 - accuracy: 0.9851\n",
            "Epoch 100/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1059 - accuracy: 0.9732\n",
            "Epoch 101/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0858 - accuracy: 0.9717\n",
            "Epoch 102/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1139 - accuracy: 0.9762\n",
            "Epoch 103/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0659 - accuracy: 0.9836\n",
            "Epoch 104/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0245 - accuracy: 0.9926\n",
            "Epoch 105/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0959 - accuracy: 0.9702\n",
            "Epoch 106/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0889 - accuracy: 0.9777\n",
            "Epoch 107/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1695 - accuracy: 0.9688\n",
            "Epoch 108/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0714 - accuracy: 0.9732\n",
            "Epoch 109/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0779 - accuracy: 0.9732\n",
            "Epoch 110/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1794 - accuracy: 0.9613\n",
            "Epoch 111/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0480 - accuracy: 0.9836\n",
            "Epoch 112/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1457 - accuracy: 0.9747\n",
            "Epoch 113/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2161 - accuracy: 0.9628\n",
            "Epoch 114/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1089 - accuracy: 0.9747\n",
            "Epoch 115/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1053 - accuracy: 0.9762\n",
            "Epoch 116/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1139 - accuracy: 0.9688\n",
            "Epoch 117/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0823 - accuracy: 0.9821\n",
            "Epoch 118/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0897 - accuracy: 0.9747\n",
            "Epoch 119/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0705 - accuracy: 0.9821\n",
            "Epoch 120/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0587 - accuracy: 0.9866\n",
            "Epoch 121/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1123 - accuracy: 0.9702\n",
            "Epoch 122/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1085 - accuracy: 0.9777\n",
            "Epoch 123/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1187 - accuracy: 0.9688\n",
            "Epoch 124/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0571 - accuracy: 0.9836\n",
            "Epoch 125/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0788 - accuracy: 0.9777\n",
            "Epoch 126/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0767 - accuracy: 0.9762\n",
            "Epoch 127/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0495 - accuracy: 0.9881\n",
            "Epoch 128/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1036 - accuracy: 0.9688\n",
            "Epoch 129/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1278 - accuracy: 0.9747\n",
            "Epoch 130/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0550 - accuracy: 0.9851\n",
            "Epoch 131/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0418 - accuracy: 0.9851\n",
            "Epoch 132/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0838 - accuracy: 0.9821\n",
            "Epoch 133/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0695 - accuracy: 0.9821\n",
            "Epoch 134/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1090 - accuracy: 0.9777\n",
            "Epoch 135/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0428 - accuracy: 0.9866\n",
            "Epoch 136/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1209 - accuracy: 0.9658\n",
            "Epoch 137/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0888 - accuracy: 0.9717\n",
            "Epoch 138/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0759 - accuracy: 0.9792\n",
            "Epoch 139/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0892 - accuracy: 0.9807\n",
            "Epoch 140/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1461 - accuracy: 0.9762\n",
            "Epoch 141/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1452 - accuracy: 0.9717\n",
            "Epoch 142/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0912 - accuracy: 0.9792\n",
            "Epoch 143/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2148 - accuracy: 0.9613\n",
            "Epoch 144/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1410 - accuracy: 0.9673\n",
            "Epoch 145/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1441 - accuracy: 0.9702\n",
            "Epoch 146/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1633 - accuracy: 0.9643\n",
            "Epoch 147/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1262 - accuracy: 0.9613\n",
            "Epoch 148/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1772 - accuracy: 0.9702\n",
            "Epoch 149/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1469 - accuracy: 0.9688\n",
            "Epoch 150/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1180 - accuracy: 0.9658\n",
            "Epoch 151/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1680 - accuracy: 0.9688\n",
            "Epoch 152/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0494 - accuracy: 0.9821\n",
            "Epoch 153/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0596 - accuracy: 0.9836\n",
            "Epoch 154/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0561 - accuracy: 0.9881\n",
            "Epoch 155/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0494 - accuracy: 0.9851\n",
            "Epoch 156/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0776 - accuracy: 0.9836\n",
            "Epoch 157/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0499 - accuracy: 0.9881\n",
            "Epoch 158/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0632 - accuracy: 0.9851\n",
            "Epoch 159/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0417 - accuracy: 0.9896\n",
            "Epoch 160/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1138 - accuracy: 0.9688\n",
            "Epoch 161/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0744 - accuracy: 0.9836\n",
            "Epoch 162/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1810 - accuracy: 0.9717\n",
            "Epoch 163/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2233 - accuracy: 0.9628\n",
            "Epoch 164/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1802 - accuracy: 0.9583\n",
            "Epoch 165/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1228 - accuracy: 0.9717\n",
            "Epoch 166/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0967 - accuracy: 0.9732\n",
            "Epoch 167/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0687 - accuracy: 0.9807\n",
            "Epoch 168/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1214 - accuracy: 0.9688\n",
            "Epoch 169/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1650 - accuracy: 0.9792\n",
            "Epoch 170/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0574 - accuracy: 0.9896\n",
            "Epoch 171/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0568 - accuracy: 0.9896\n",
            "Epoch 172/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1514 - accuracy: 0.9673\n",
            "Epoch 173/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2164 - accuracy: 0.9643\n",
            "Epoch 174/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0952 - accuracy: 0.9821\n",
            "Epoch 175/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0707 - accuracy: 0.9821\n",
            "Epoch 176/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0569 - accuracy: 0.9851\n",
            "Epoch 177/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1019 - accuracy: 0.9747\n",
            "Epoch 178/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0902 - accuracy: 0.9777\n",
            "Epoch 179/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2244 - accuracy: 0.9807\n",
            "Epoch 180/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0919 - accuracy: 0.9821\n",
            "Epoch 181/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1634 - accuracy: 0.9658\n",
            "Epoch 182/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1035 - accuracy: 0.9807\n",
            "Epoch 183/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1518 - accuracy: 0.9732\n",
            "Epoch 184/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1221 - accuracy: 0.9702\n",
            "Epoch 185/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0718 - accuracy: 0.9836\n",
            "Epoch 186/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0486 - accuracy: 0.9881\n",
            "Epoch 187/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1651 - accuracy: 0.9777\n",
            "Epoch 188/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1478 - accuracy: 0.9747\n",
            "Epoch 189/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0882 - accuracy: 0.9792\n",
            "Epoch 190/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1365 - accuracy: 0.9702\n",
            "Epoch 191/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1355 - accuracy: 0.9717\n",
            "Epoch 192/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0727 - accuracy: 0.9851\n",
            "Epoch 193/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1211 - accuracy: 0.9747\n",
            "Epoch 194/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0421 - accuracy: 0.9911\n",
            "Epoch 195/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0173 - accuracy: 0.9896\n",
            "Epoch 196/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0454 - accuracy: 0.9896\n",
            "Epoch 197/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0580 - accuracy: 0.9881\n",
            "Epoch 198/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0951 - accuracy: 0.9821\n",
            "Epoch 199/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0596 - accuracy: 0.9896\n",
            "Epoch 200/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1100 - accuracy: 0.9807\n",
            "Epoch 201/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0552 - accuracy: 0.9851\n",
            "Epoch 202/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0324 - accuracy: 0.9881\n",
            "Epoch 203/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0247 - accuracy: 0.9926\n",
            "Epoch 204/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0494 - accuracy: 0.9866\n",
            "Epoch 205/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0393 - accuracy: 0.9926\n",
            "Epoch 206/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0398 - accuracy: 0.9926\n",
            "Epoch 207/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0574 - accuracy: 0.9851\n",
            "Epoch 208/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2337 - accuracy: 0.9673\n",
            "Epoch 209/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0541 - accuracy: 0.9851\n",
            "Epoch 210/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1175 - accuracy: 0.9807\n",
            "Epoch 211/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0531 - accuracy: 0.9896\n",
            "Epoch 212/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0387 - accuracy: 0.9881\n",
            "Epoch 213/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1201 - accuracy: 0.9807\n",
            "Epoch 214/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1520 - accuracy: 0.9807\n",
            "Epoch 215/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0578 - accuracy: 0.9926\n",
            "Epoch 216/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0549 - accuracy: 0.9881\n",
            "Epoch 217/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0705 - accuracy: 0.9851\n",
            "Epoch 218/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0433 - accuracy: 0.9866\n",
            "Epoch 219/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1284 - accuracy: 0.9792\n",
            "Epoch 220/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1666 - accuracy: 0.9747\n",
            "Epoch 221/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1471 - accuracy: 0.9807\n",
            "Epoch 222/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2023 - accuracy: 0.9673\n",
            "Epoch 223/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0885 - accuracy: 0.9777\n",
            "Epoch 224/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1008 - accuracy: 0.9777\n",
            "Epoch 225/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1664 - accuracy: 0.9673\n",
            "Epoch 226/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1178 - accuracy: 0.9777\n",
            "Epoch 227/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0940 - accuracy: 0.9762\n",
            "Epoch 228/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0541 - accuracy: 0.9911\n",
            "Epoch 229/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0351 - accuracy: 0.9926\n",
            "Epoch 230/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0500 - accuracy: 0.9866\n",
            "Epoch 231/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0780 - accuracy: 0.9866\n",
            "Epoch 232/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0650 - accuracy: 0.9851\n",
            "Epoch 233/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0684 - accuracy: 0.9866\n",
            "Epoch 234/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1328 - accuracy: 0.9836\n",
            "Epoch 235/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0787 - accuracy: 0.9836\n",
            "Epoch 236/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0484 - accuracy: 0.9836\n",
            "Epoch 237/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1088 - accuracy: 0.9717\n",
            "Epoch 238/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1820 - accuracy: 0.9717\n",
            "Epoch 239/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0785 - accuracy: 0.9777\n",
            "Epoch 240/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1740 - accuracy: 0.9732\n",
            "Epoch 241/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0707 - accuracy: 0.9807\n",
            "Epoch 242/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0806 - accuracy: 0.9792\n",
            "Epoch 243/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0876 - accuracy: 0.9702\n",
            "Epoch 244/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1329 - accuracy: 0.9747\n",
            "Epoch 245/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1415 - accuracy: 0.9821\n",
            "Epoch 246/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1370 - accuracy: 0.9702\n",
            "Epoch 247/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1862 - accuracy: 0.9688\n",
            "Epoch 248/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0855 - accuracy: 0.9836\n",
            "Epoch 249/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0766 - accuracy: 0.9807\n",
            "Epoch 250/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1081 - accuracy: 0.9807\n",
            "Epoch 251/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2248 - accuracy: 0.9792\n",
            "Epoch 252/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1821 - accuracy: 0.9702\n",
            "Epoch 253/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1630 - accuracy: 0.9777\n",
            "Epoch 254/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1407 - accuracy: 0.9792\n",
            "Epoch 255/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2620 - accuracy: 0.9598\n",
            "Epoch 256/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0758 - accuracy: 0.9821\n",
            "Epoch 257/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1327 - accuracy: 0.9792\n",
            "Epoch 258/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0762 - accuracy: 0.9777\n",
            "Epoch 259/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1204 - accuracy: 0.9777\n",
            "Epoch 260/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1020 - accuracy: 0.9777\n",
            "Epoch 261/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1109 - accuracy: 0.9747\n",
            "Epoch 262/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0655 - accuracy: 0.9792\n",
            "Epoch 263/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0927 - accuracy: 0.9702\n",
            "Epoch 264/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1061 - accuracy: 0.9762\n",
            "Epoch 265/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0568 - accuracy: 0.9926\n",
            "Epoch 266/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0266 - accuracy: 0.9940\n",
            "Epoch 267/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0109 - accuracy: 0.9970\n",
            "Epoch 268/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0408 - accuracy: 0.9926\n",
            "Epoch 269/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0765 - accuracy: 0.9866\n",
            "Epoch 270/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0277 - accuracy: 0.9926\n",
            "Epoch 271/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0676 - accuracy: 0.9866\n",
            "Epoch 272/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0207 - accuracy: 0.9926\n",
            "Epoch 273/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0240 - accuracy: 0.9940\n",
            "Epoch 274/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1272 - accuracy: 0.9732\n",
            "Epoch 275/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1393 - accuracy: 0.9777\n",
            "Epoch 276/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1266 - accuracy: 0.9792\n",
            "Epoch 277/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1066 - accuracy: 0.9792\n",
            "Epoch 278/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1105 - accuracy: 0.9732\n",
            "Epoch 279/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1777 - accuracy: 0.9702\n",
            "Epoch 280/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2183 - accuracy: 0.9732\n",
            "Epoch 281/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2131 - accuracy: 0.9717\n",
            "Epoch 282/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0801 - accuracy: 0.9866\n",
            "Epoch 283/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1092 - accuracy: 0.9732\n",
            "Epoch 284/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0991 - accuracy: 0.9866\n",
            "Epoch 285/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1786 - accuracy: 0.9747\n",
            "Epoch 286/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0891 - accuracy: 0.9866\n",
            "Epoch 287/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1400 - accuracy: 0.9807\n",
            "Epoch 288/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0933 - accuracy: 0.9792\n",
            "Epoch 289/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2775 - accuracy: 0.9688\n",
            "Epoch 290/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1043 - accuracy: 0.9851\n",
            "Epoch 291/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1025 - accuracy: 0.9866\n",
            "Epoch 292/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2105 - accuracy: 0.9836\n",
            "Epoch 293/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0735 - accuracy: 0.9881\n",
            "Epoch 294/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0868 - accuracy: 0.9866\n",
            "Epoch 295/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1085 - accuracy: 0.9851\n",
            "Epoch 296/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0909 - accuracy: 0.9792\n",
            "Epoch 297/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0495 - accuracy: 0.9881\n",
            "Epoch 298/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0562 - accuracy: 0.9881\n",
            "Epoch 299/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0405 - accuracy: 0.9940\n",
            "Epoch 300/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1257 - accuracy: 0.9866\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 19.9160 - accuracy: 0.0357\n",
            "Model: \"sequential_151\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_453 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_755 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_453 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_454 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_756 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_454 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_455 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_757 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_455 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_151 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_453 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_758 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_454 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_759 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_455 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 4.0220 - accuracy: 0.0417\n",
            "Epoch 2/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 3.5603 - accuracy: 0.1057\n",
            "Epoch 3/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 2.8322 - accuracy: 0.2545\n",
            "Epoch 4/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 2.0858 - accuracy: 0.4345\n",
            "Epoch 5/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.4736 - accuracy: 0.5640\n",
            "Epoch 6/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.1063 - accuracy: 0.6696\n",
            "Epoch 7/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.9163 - accuracy: 0.7292\n",
            "Epoch 8/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.7732 - accuracy: 0.7693\n",
            "Epoch 9/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.5084 - accuracy: 0.8482\n",
            "Epoch 10/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.5635 - accuracy: 0.8274\n",
            "Epoch 11/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4405 - accuracy: 0.8601\n",
            "Epoch 12/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.5423 - accuracy: 0.8378\n",
            "Epoch 13/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3288 - accuracy: 0.9033\n",
            "Epoch 14/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4040 - accuracy: 0.8839\n",
            "Epoch 15/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3944 - accuracy: 0.8973\n",
            "Epoch 16/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2950 - accuracy: 0.9048\n",
            "Epoch 17/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3009 - accuracy: 0.9167\n",
            "Epoch 18/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3517 - accuracy: 0.9003\n",
            "Epoch 19/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1926 - accuracy: 0.9360\n",
            "Epoch 20/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2734 - accuracy: 0.9256\n",
            "Epoch 21/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2221 - accuracy: 0.9211\n",
            "Epoch 22/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1702 - accuracy: 0.9435\n",
            "Epoch 23/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2758 - accuracy: 0.9152\n",
            "Epoch 24/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2226 - accuracy: 0.9330\n",
            "Epoch 25/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3249 - accuracy: 0.9152\n",
            "Epoch 26/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1997 - accuracy: 0.9390\n",
            "Epoch 27/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1786 - accuracy: 0.9420\n",
            "Epoch 28/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1820 - accuracy: 0.9464\n",
            "Epoch 29/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1390 - accuracy: 0.9554\n",
            "Epoch 30/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2002 - accuracy: 0.9420\n",
            "Epoch 31/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1958 - accuracy: 0.9375\n",
            "Epoch 32/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2337 - accuracy: 0.9375\n",
            "Epoch 33/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1931 - accuracy: 0.9390\n",
            "Epoch 34/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1350 - accuracy: 0.9568\n",
            "Epoch 35/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1384 - accuracy: 0.9583\n",
            "Epoch 36/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1100 - accuracy: 0.9673\n",
            "Epoch 37/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1238 - accuracy: 0.9673\n",
            "Epoch 38/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1457 - accuracy: 0.9568\n",
            "Epoch 39/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1351 - accuracy: 0.9568\n",
            "Epoch 40/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1783 - accuracy: 0.9420\n",
            "Epoch 41/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2443 - accuracy: 0.9375\n",
            "Epoch 42/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1089 - accuracy: 0.9613\n",
            "Epoch 43/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0930 - accuracy: 0.9688\n",
            "Epoch 44/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2364 - accuracy: 0.9405\n",
            "Epoch 45/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2474 - accuracy: 0.9301\n",
            "Epoch 46/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1329 - accuracy: 0.9613\n",
            "Epoch 47/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1495 - accuracy: 0.9568\n",
            "Epoch 48/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1462 - accuracy: 0.9643\n",
            "Epoch 49/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1848 - accuracy: 0.9435\n",
            "Epoch 50/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0968 - accuracy: 0.9688\n",
            "Epoch 51/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0987 - accuracy: 0.9717\n",
            "Epoch 52/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0724 - accuracy: 0.9836\n",
            "Epoch 53/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0799 - accuracy: 0.9777\n",
            "Epoch 54/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1067 - accuracy: 0.9658\n",
            "Epoch 55/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0948 - accuracy: 0.9762\n",
            "Epoch 56/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0790 - accuracy: 0.9732\n",
            "Epoch 57/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1677 - accuracy: 0.9554\n",
            "Epoch 58/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3439 - accuracy: 0.9330\n",
            "Epoch 59/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2537 - accuracy: 0.9420\n",
            "Epoch 60/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2657 - accuracy: 0.9420\n",
            "Epoch 61/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1250 - accuracy: 0.9643\n",
            "Epoch 62/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0718 - accuracy: 0.9836\n",
            "Epoch 63/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0740 - accuracy: 0.9792\n",
            "Epoch 64/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0747 - accuracy: 0.9762\n",
            "Epoch 65/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0876 - accuracy: 0.9688\n",
            "Epoch 66/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0805 - accuracy: 0.9702\n",
            "Epoch 67/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0728 - accuracy: 0.9792\n",
            "Epoch 68/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0890 - accuracy: 0.9702\n",
            "Epoch 69/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2464 - accuracy: 0.9479\n",
            "Epoch 70/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2045 - accuracy: 0.9494\n",
            "Epoch 71/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1048 - accuracy: 0.9673\n",
            "Epoch 72/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2062 - accuracy: 0.9420\n",
            "Epoch 73/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1105 - accuracy: 0.9658\n",
            "Epoch 74/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0856 - accuracy: 0.9673\n",
            "Epoch 75/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1116 - accuracy: 0.9613\n",
            "Epoch 76/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0921 - accuracy: 0.9702\n",
            "Epoch 77/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0822 - accuracy: 0.9807\n",
            "Epoch 78/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1555 - accuracy: 0.9554\n",
            "Epoch 79/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0614 - accuracy: 0.9792\n",
            "Epoch 80/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0829 - accuracy: 0.9702\n",
            "Epoch 81/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1166 - accuracy: 0.9658\n",
            "Epoch 82/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0891 - accuracy: 0.9747\n",
            "Epoch 83/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1060 - accuracy: 0.9717\n",
            "Epoch 84/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0974 - accuracy: 0.9821\n",
            "Epoch 85/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1200 - accuracy: 0.9688\n",
            "Epoch 86/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1105 - accuracy: 0.9673\n",
            "Epoch 87/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0832 - accuracy: 0.9777\n",
            "Epoch 88/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0730 - accuracy: 0.9807\n",
            "Epoch 89/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0392 - accuracy: 0.9851\n",
            "Epoch 90/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0455 - accuracy: 0.9836\n",
            "Epoch 91/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0603 - accuracy: 0.9836\n",
            "Epoch 92/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0426 - accuracy: 0.9881\n",
            "Epoch 93/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2159 - accuracy: 0.9554\n",
            "Epoch 94/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1826 - accuracy: 0.9539\n",
            "Epoch 95/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1827 - accuracy: 0.9628\n",
            "Epoch 96/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1883 - accuracy: 0.9568\n",
            "Epoch 97/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1816 - accuracy: 0.9598\n",
            "Epoch 98/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1058 - accuracy: 0.9717\n",
            "Epoch 99/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1931 - accuracy: 0.9643\n",
            "Epoch 100/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0962 - accuracy: 0.9732\n",
            "Epoch 101/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1018 - accuracy: 0.9732\n",
            "Epoch 102/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1100 - accuracy: 0.9732\n",
            "Epoch 103/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1226 - accuracy: 0.9673\n",
            "Epoch 104/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1001 - accuracy: 0.9702\n",
            "Epoch 105/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0575 - accuracy: 0.9821\n",
            "Epoch 106/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0386 - accuracy: 0.9926\n",
            "Epoch 107/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1026 - accuracy: 0.9643\n",
            "Epoch 108/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0704 - accuracy: 0.9807\n",
            "Epoch 109/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0990 - accuracy: 0.9792\n",
            "Epoch 110/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1517 - accuracy: 0.9732\n",
            "Epoch 111/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0735 - accuracy: 0.9762\n",
            "Epoch 112/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0502 - accuracy: 0.9821\n",
            "Epoch 113/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1158 - accuracy: 0.9777\n",
            "Epoch 114/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0947 - accuracy: 0.9762\n",
            "Epoch 115/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1015 - accuracy: 0.9747\n",
            "Epoch 116/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0666 - accuracy: 0.9821\n",
            "Epoch 117/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0728 - accuracy: 0.9792\n",
            "Epoch 118/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1701 - accuracy: 0.9688\n",
            "Epoch 119/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1550 - accuracy: 0.9613\n",
            "Epoch 120/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0896 - accuracy: 0.9821\n",
            "Epoch 121/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1185 - accuracy: 0.9717\n",
            "Epoch 122/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1556 - accuracy: 0.9732\n",
            "Epoch 123/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1516 - accuracy: 0.9747\n",
            "Epoch 124/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1296 - accuracy: 0.9747\n",
            "Epoch 125/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0787 - accuracy: 0.9673\n",
            "Epoch 126/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1118 - accuracy: 0.9747\n",
            "Epoch 127/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0518 - accuracy: 0.9836\n",
            "Epoch 128/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0843 - accuracy: 0.9807\n",
            "Epoch 129/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1661 - accuracy: 0.9643\n",
            "Epoch 130/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1676 - accuracy: 0.9688\n",
            "Epoch 131/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2923 - accuracy: 0.9509\n",
            "Epoch 132/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0866 - accuracy: 0.9762\n",
            "Epoch 133/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1274 - accuracy: 0.9732\n",
            "Epoch 134/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0611 - accuracy: 0.9851\n",
            "Epoch 135/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0875 - accuracy: 0.9777\n",
            "Epoch 136/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0573 - accuracy: 0.9851\n",
            "Epoch 137/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0391 - accuracy: 0.9881\n",
            "Epoch 138/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0643 - accuracy: 0.9881\n",
            "Epoch 139/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0936 - accuracy: 0.9777\n",
            "Epoch 140/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0289 - accuracy: 0.9881\n",
            "Epoch 141/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0636 - accuracy: 0.9807\n",
            "Epoch 142/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0864 - accuracy: 0.9762\n",
            "Epoch 143/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0989 - accuracy: 0.9732\n",
            "Epoch 144/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1444 - accuracy: 0.9792\n",
            "Epoch 145/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1609 - accuracy: 0.9717\n",
            "Epoch 146/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0798 - accuracy: 0.9807\n",
            "Epoch 147/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0768 - accuracy: 0.9807\n",
            "Epoch 148/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1268 - accuracy: 0.9762\n",
            "Epoch 149/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1347 - accuracy: 0.9821\n",
            "Epoch 150/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1832 - accuracy: 0.9658\n",
            "Epoch 151/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1634 - accuracy: 0.9777\n",
            "Epoch 152/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1378 - accuracy: 0.9792\n",
            "Epoch 153/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0663 - accuracy: 0.9821\n",
            "Epoch 154/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0595 - accuracy: 0.9896\n",
            "Epoch 155/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0763 - accuracy: 0.9807\n",
            "Epoch 156/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0701 - accuracy: 0.9851\n",
            "Epoch 157/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0777 - accuracy: 0.9836\n",
            "Epoch 158/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0974 - accuracy: 0.9777\n",
            "Epoch 159/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0270 - accuracy: 0.9940\n",
            "Epoch 160/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0540 - accuracy: 0.9881\n",
            "Epoch 161/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0845 - accuracy: 0.9673\n",
            "Epoch 162/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0993 - accuracy: 0.9777\n",
            "Epoch 163/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0295 - accuracy: 0.9896\n",
            "Epoch 164/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0794 - accuracy: 0.9807\n",
            "Epoch 165/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0691 - accuracy: 0.9851\n",
            "Epoch 166/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0846 - accuracy: 0.9881\n",
            "Epoch 167/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0836 - accuracy: 0.9807\n",
            "Epoch 168/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0208 - accuracy: 0.9926\n",
            "Epoch 169/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0259 - accuracy: 0.9926\n",
            "Epoch 170/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0436 - accuracy: 0.9866\n",
            "Epoch 171/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1324 - accuracy: 0.9717\n",
            "Epoch 172/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0487 - accuracy: 0.9911\n",
            "Epoch 173/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1288 - accuracy: 0.9732\n",
            "Epoch 174/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0392 - accuracy: 0.9926\n",
            "Epoch 175/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0820 - accuracy: 0.9777\n",
            "Epoch 176/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0361 - accuracy: 0.9851\n",
            "Epoch 177/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0827 - accuracy: 0.9851\n",
            "Epoch 178/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1224 - accuracy: 0.9821\n",
            "Epoch 179/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0879 - accuracy: 0.9747\n",
            "Epoch 180/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0500 - accuracy: 0.9866\n",
            "Epoch 181/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1357 - accuracy: 0.9747\n",
            "Epoch 182/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1794 - accuracy: 0.9643\n",
            "Epoch 183/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1595 - accuracy: 0.9717\n",
            "Epoch 184/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1261 - accuracy: 0.9807\n",
            "Epoch 185/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1225 - accuracy: 0.9762\n",
            "Epoch 186/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1589 - accuracy: 0.9747\n",
            "Epoch 187/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2558 - accuracy: 0.9643\n",
            "Epoch 188/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1134 - accuracy: 0.9762\n",
            "Epoch 189/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1887 - accuracy: 0.9807\n",
            "Epoch 190/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2159 - accuracy: 0.9643\n",
            "Epoch 191/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2328 - accuracy: 0.9628\n",
            "Epoch 192/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1023 - accuracy: 0.9762\n",
            "Epoch 193/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0821 - accuracy: 0.9807\n",
            "Epoch 194/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1256 - accuracy: 0.9762\n",
            "Epoch 195/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0871 - accuracy: 0.9821\n",
            "Epoch 196/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1147 - accuracy: 0.9747\n",
            "Epoch 197/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1322 - accuracy: 0.9807\n",
            "Epoch 198/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1002 - accuracy: 0.9836\n",
            "Epoch 199/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0548 - accuracy: 0.9911\n",
            "Epoch 200/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0536 - accuracy: 0.9896\n",
            "Epoch 201/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1325 - accuracy: 0.9702\n",
            "Epoch 202/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1496 - accuracy: 0.9717\n",
            "Epoch 203/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0654 - accuracy: 0.9866\n",
            "Epoch 204/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0581 - accuracy: 0.9866\n",
            "Epoch 205/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0837 - accuracy: 0.9821\n",
            "Epoch 206/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0711 - accuracy: 0.9807\n",
            "Epoch 207/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0771 - accuracy: 0.9821\n",
            "Epoch 208/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1151 - accuracy: 0.9762\n",
            "Epoch 209/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1308 - accuracy: 0.9717\n",
            "Epoch 210/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0570 - accuracy: 0.9851\n",
            "Epoch 211/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0680 - accuracy: 0.9777\n",
            "Epoch 212/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0813 - accuracy: 0.9821\n",
            "Epoch 213/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0627 - accuracy: 0.9881\n",
            "Epoch 214/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0850 - accuracy: 0.9866\n",
            "Epoch 215/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1267 - accuracy: 0.9747\n",
            "Epoch 216/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1485 - accuracy: 0.9807\n",
            "Epoch 217/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0866 - accuracy: 0.9821\n",
            "Epoch 218/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0913 - accuracy: 0.9807\n",
            "Epoch 219/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0221 - accuracy: 0.9940\n",
            "Epoch 220/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0161 - accuracy: 0.9926\n",
            "Epoch 221/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1958 - accuracy: 0.9717\n",
            "Epoch 222/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1079 - accuracy: 0.9717\n",
            "Epoch 223/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0562 - accuracy: 0.9926\n",
            "Epoch 224/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0769 - accuracy: 0.9836\n",
            "Epoch 225/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0240 - accuracy: 0.9896\n",
            "Epoch 226/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1247 - accuracy: 0.9762\n",
            "Epoch 227/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0268 - accuracy: 0.9911\n",
            "Epoch 228/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0245 - accuracy: 0.9911\n",
            "Epoch 229/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0310 - accuracy: 0.9911\n",
            "Epoch 230/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0167 - accuracy: 0.9970\n",
            "Epoch 231/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0340 - accuracy: 0.9881\n",
            "Epoch 232/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0564 - accuracy: 0.9836\n",
            "Epoch 233/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0179 - accuracy: 0.9970\n",
            "Epoch 234/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0428 - accuracy: 0.9866\n",
            "Epoch 235/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0297 - accuracy: 0.9926\n",
            "Epoch 236/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0655 - accuracy: 0.9926\n",
            "Epoch 237/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1083 - accuracy: 0.9777\n",
            "Epoch 238/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0754 - accuracy: 0.9836\n",
            "Epoch 239/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0909 - accuracy: 0.9821\n",
            "Epoch 240/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0605 - accuracy: 0.9836\n",
            "Epoch 241/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1134 - accuracy: 0.9807\n",
            "Epoch 242/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1386 - accuracy: 0.9747\n",
            "Epoch 243/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1461 - accuracy: 0.9747\n",
            "Epoch 244/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1547 - accuracy: 0.9747\n",
            "Epoch 245/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1306 - accuracy: 0.9762\n",
            "Epoch 246/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0853 - accuracy: 0.9821\n",
            "Epoch 247/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0495 - accuracy: 0.9866\n",
            "Epoch 248/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0763 - accuracy: 0.9911\n",
            "Epoch 249/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0867 - accuracy: 0.9851\n",
            "Epoch 250/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1015 - accuracy: 0.9821\n",
            "Epoch 251/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0936 - accuracy: 0.9747\n",
            "Epoch 252/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1311 - accuracy: 0.9732\n",
            "Epoch 253/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1303 - accuracy: 0.9702\n",
            "Epoch 254/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1965 - accuracy: 0.9747\n",
            "Epoch 255/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0929 - accuracy: 0.9717\n",
            "Epoch 256/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1604 - accuracy: 0.9807\n",
            "Epoch 257/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0573 - accuracy: 0.9911\n",
            "Epoch 258/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0520 - accuracy: 0.9851\n",
            "Epoch 259/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1036 - accuracy: 0.9836\n",
            "Epoch 260/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0685 - accuracy: 0.9851\n",
            "Epoch 261/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0808 - accuracy: 0.9836\n",
            "Epoch 262/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1510 - accuracy: 0.9792\n",
            "Epoch 263/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1368 - accuracy: 0.9777\n",
            "Epoch 264/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0814 - accuracy: 0.9881\n",
            "Epoch 265/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1828 - accuracy: 0.9762\n",
            "Epoch 266/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1940 - accuracy: 0.9702\n",
            "Epoch 267/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1910 - accuracy: 0.9762\n",
            "Epoch 268/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2622 - accuracy: 0.9702\n",
            "Epoch 269/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0656 - accuracy: 0.9881\n",
            "Epoch 270/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1287 - accuracy: 0.9792\n",
            "Epoch 271/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0377 - accuracy: 0.9896\n",
            "Epoch 272/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1226 - accuracy: 0.9851\n",
            "Epoch 273/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0495 - accuracy: 0.9926\n",
            "Epoch 274/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2212 - accuracy: 0.9702\n",
            "Epoch 275/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1851 - accuracy: 0.9792\n",
            "Epoch 276/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0958 - accuracy: 0.9836\n",
            "Epoch 277/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1171 - accuracy: 0.9777\n",
            "Epoch 278/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1221 - accuracy: 0.9747\n",
            "Epoch 279/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0596 - accuracy: 0.9896\n",
            "Epoch 280/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0794 - accuracy: 0.9896\n",
            "Epoch 281/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0676 - accuracy: 0.9851\n",
            "Epoch 282/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1127 - accuracy: 0.9821\n",
            "Epoch 283/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1890 - accuracy: 0.9807\n",
            "Epoch 284/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1529 - accuracy: 0.9792\n",
            "Epoch 285/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0843 - accuracy: 0.9851\n",
            "Epoch 286/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1330 - accuracy: 0.9851\n",
            "Epoch 287/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0818 - accuracy: 0.9836\n",
            "Epoch 288/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1164 - accuracy: 0.9792\n",
            "Epoch 289/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0350 - accuracy: 0.9955\n",
            "Epoch 290/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1027 - accuracy: 0.9836\n",
            "Epoch 291/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2781 - accuracy: 0.9762\n",
            "Epoch 292/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0927 - accuracy: 0.9911\n",
            "Epoch 293/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1315 - accuracy: 0.9792\n",
            "Epoch 294/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1847 - accuracy: 0.9836\n",
            "Epoch 295/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0632 - accuracy: 0.9911\n",
            "Epoch 296/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1219 - accuracy: 0.9851\n",
            "Epoch 297/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1684 - accuracy: 0.9881\n",
            "Epoch 298/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1195 - accuracy: 0.9881\n",
            "Epoch 299/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0950 - accuracy: 0.9807\n",
            "Epoch 300/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0881 - accuracy: 0.9911\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 18.5654 - accuracy: 0.1607\n",
            "Model: \"sequential_152\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_456 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_760 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_456 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_457 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_761 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_457 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_458 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_762 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_458 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_152 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_456 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_763 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_457 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_764 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_458 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 4.0378 - accuracy: 0.0253\n",
            "Epoch 2/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 3.4830 - accuracy: 0.1220\n",
            "Epoch 3/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 2.7135 - accuracy: 0.2574\n",
            "Epoch 4/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 2.1745 - accuracy: 0.3943\n",
            "Epoch 5/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.6709 - accuracy: 0.5060\n",
            "Epoch 6/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 1.3774 - accuracy: 0.6012\n",
            "Epoch 7/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.1344 - accuracy: 0.6667\n",
            "Epoch 8/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.8959 - accuracy: 0.7277\n",
            "Epoch 9/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.8563 - accuracy: 0.7411\n",
            "Epoch 10/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.7768 - accuracy: 0.7530\n",
            "Epoch 11/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.6617 - accuracy: 0.7991\n",
            "Epoch 12/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.5438 - accuracy: 0.8304\n",
            "Epoch 13/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4861 - accuracy: 0.8438\n",
            "Epoch 14/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4525 - accuracy: 0.8557\n",
            "Epoch 15/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.4119 - accuracy: 0.8750\n",
            "Epoch 16/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3507 - accuracy: 0.8914\n",
            "Epoch 17/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3589 - accuracy: 0.8914\n",
            "Epoch 18/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4139 - accuracy: 0.8750\n",
            "Epoch 19/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3095 - accuracy: 0.9107\n",
            "Epoch 20/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3124 - accuracy: 0.9092\n",
            "Epoch 21/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2700 - accuracy: 0.9182\n",
            "Epoch 22/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2326 - accuracy: 0.9226\n",
            "Epoch 23/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3940 - accuracy: 0.8869\n",
            "Epoch 24/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2465 - accuracy: 0.9271\n",
            "Epoch 25/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2492 - accuracy: 0.9182\n",
            "Epoch 26/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1919 - accuracy: 0.9360\n",
            "Epoch 27/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2338 - accuracy: 0.9301\n",
            "Epoch 28/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3129 - accuracy: 0.9077\n",
            "Epoch 29/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2112 - accuracy: 0.9360\n",
            "Epoch 30/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2026 - accuracy: 0.9420\n",
            "Epoch 31/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2995 - accuracy: 0.9122\n",
            "Epoch 32/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1972 - accuracy: 0.9420\n",
            "Epoch 33/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1951 - accuracy: 0.9360\n",
            "Epoch 34/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1956 - accuracy: 0.9315\n",
            "Epoch 35/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2225 - accuracy: 0.9435\n",
            "Epoch 36/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2524 - accuracy: 0.9256\n",
            "Epoch 37/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2068 - accuracy: 0.9390\n",
            "Epoch 38/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2685 - accuracy: 0.9315\n",
            "Epoch 39/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3577 - accuracy: 0.8958\n",
            "Epoch 40/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2273 - accuracy: 0.9330\n",
            "Epoch 41/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1988 - accuracy: 0.9464\n",
            "Epoch 42/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1106 - accuracy: 0.9658\n",
            "Epoch 43/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1723 - accuracy: 0.9479\n",
            "Epoch 44/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1755 - accuracy: 0.9420\n",
            "Epoch 45/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2215 - accuracy: 0.9345\n",
            "Epoch 46/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1108 - accuracy: 0.9658\n",
            "Epoch 47/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1126 - accuracy: 0.9732\n",
            "Epoch 48/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1878 - accuracy: 0.9449\n",
            "Epoch 49/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1785 - accuracy: 0.9464\n",
            "Epoch 50/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1168 - accuracy: 0.9658\n",
            "Epoch 51/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1677 - accuracy: 0.9598\n",
            "Epoch 52/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1506 - accuracy: 0.9479\n",
            "Epoch 53/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1514 - accuracy: 0.9554\n",
            "Epoch 54/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2205 - accuracy: 0.9375\n",
            "Epoch 55/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1111 - accuracy: 0.9598\n",
            "Epoch 56/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0913 - accuracy: 0.9747\n",
            "Epoch 57/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1906 - accuracy: 0.9539\n",
            "Epoch 58/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1048 - accuracy: 0.9628\n",
            "Epoch 59/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1354 - accuracy: 0.9643\n",
            "Epoch 60/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1308 - accuracy: 0.9628\n",
            "Epoch 61/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1075 - accuracy: 0.9688\n",
            "Epoch 62/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1574 - accuracy: 0.9494\n",
            "Epoch 63/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1272 - accuracy: 0.9628\n",
            "Epoch 64/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2275 - accuracy: 0.9479\n",
            "Epoch 65/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1766 - accuracy: 0.9479\n",
            "Epoch 66/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1311 - accuracy: 0.9643\n",
            "Epoch 67/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1257 - accuracy: 0.9494\n",
            "Epoch 68/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0881 - accuracy: 0.9747\n",
            "Epoch 69/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1757 - accuracy: 0.9494\n",
            "Epoch 70/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1412 - accuracy: 0.9658\n",
            "Epoch 71/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1141 - accuracy: 0.9673\n",
            "Epoch 72/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2689 - accuracy: 0.9271\n",
            "Epoch 73/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2222 - accuracy: 0.9464\n",
            "Epoch 74/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2066 - accuracy: 0.9524\n",
            "Epoch 75/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1840 - accuracy: 0.9479\n",
            "Epoch 76/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1314 - accuracy: 0.9613\n",
            "Epoch 77/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1848 - accuracy: 0.9420\n",
            "Epoch 78/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1773 - accuracy: 0.9613\n",
            "Epoch 79/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1887 - accuracy: 0.9464\n",
            "Epoch 80/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0842 - accuracy: 0.9732\n",
            "Epoch 81/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2395 - accuracy: 0.9390\n",
            "Epoch 82/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1381 - accuracy: 0.9628\n",
            "Epoch 83/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1438 - accuracy: 0.9658\n",
            "Epoch 84/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2226 - accuracy: 0.9598\n",
            "Epoch 85/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0891 - accuracy: 0.9762\n",
            "Epoch 86/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0753 - accuracy: 0.9807\n",
            "Epoch 87/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0842 - accuracy: 0.9732\n",
            "Epoch 88/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1212 - accuracy: 0.9688\n",
            "Epoch 89/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1670 - accuracy: 0.9598\n",
            "Epoch 90/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0901 - accuracy: 0.9762\n",
            "Epoch 91/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0750 - accuracy: 0.9732\n",
            "Epoch 92/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1501 - accuracy: 0.9673\n",
            "Epoch 93/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1162 - accuracy: 0.9643\n",
            "Epoch 94/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0707 - accuracy: 0.9777\n",
            "Epoch 95/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0783 - accuracy: 0.9747\n",
            "Epoch 96/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0526 - accuracy: 0.9851\n",
            "Epoch 97/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1761 - accuracy: 0.9658\n",
            "Epoch 98/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1326 - accuracy: 0.9673\n",
            "Epoch 99/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2801 - accuracy: 0.9479\n",
            "Epoch 100/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1219 - accuracy: 0.9658\n",
            "Epoch 101/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0708 - accuracy: 0.9732\n",
            "Epoch 102/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1716 - accuracy: 0.9568\n",
            "Epoch 103/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0712 - accuracy: 0.9777\n",
            "Epoch 104/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0797 - accuracy: 0.9717\n",
            "Epoch 105/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0716 - accuracy: 0.9777\n",
            "Epoch 106/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0424 - accuracy: 0.9881\n",
            "Epoch 107/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0844 - accuracy: 0.9762\n",
            "Epoch 108/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0614 - accuracy: 0.9747\n",
            "Epoch 109/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0962 - accuracy: 0.9732\n",
            "Epoch 110/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1111 - accuracy: 0.9717\n",
            "Epoch 111/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1289 - accuracy: 0.9598\n",
            "Epoch 112/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1137 - accuracy: 0.9747\n",
            "Epoch 113/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0880 - accuracy: 0.9747\n",
            "Epoch 114/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0808 - accuracy: 0.9777\n",
            "Epoch 115/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2284 - accuracy: 0.9658\n",
            "Epoch 116/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1396 - accuracy: 0.9658\n",
            "Epoch 117/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0927 - accuracy: 0.9747\n",
            "Epoch 118/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0685 - accuracy: 0.9807\n",
            "Epoch 119/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0435 - accuracy: 0.9881\n",
            "Epoch 120/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0941 - accuracy: 0.9732\n",
            "Epoch 121/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0325 - accuracy: 0.9851\n",
            "Epoch 122/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0375 - accuracy: 0.9851\n",
            "Epoch 123/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0451 - accuracy: 0.9896\n",
            "Epoch 124/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1755 - accuracy: 0.9673\n",
            "Epoch 125/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1459 - accuracy: 0.9658\n",
            "Epoch 126/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2485 - accuracy: 0.9479\n",
            "Epoch 127/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2267 - accuracy: 0.9539\n",
            "Epoch 128/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1504 - accuracy: 0.9717\n",
            "Epoch 129/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1362 - accuracy: 0.9717\n",
            "Epoch 130/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0635 - accuracy: 0.9807\n",
            "Epoch 131/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1308 - accuracy: 0.9762\n",
            "Epoch 132/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1267 - accuracy: 0.9658\n",
            "Epoch 133/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0978 - accuracy: 0.9747\n",
            "Epoch 134/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0872 - accuracy: 0.9717\n",
            "Epoch 135/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0977 - accuracy: 0.9777\n",
            "Epoch 136/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0839 - accuracy: 0.9821\n",
            "Epoch 137/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1904 - accuracy: 0.9673\n",
            "Epoch 138/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1205 - accuracy: 0.9762\n",
            "Epoch 139/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0682 - accuracy: 0.9807\n",
            "Epoch 140/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1234 - accuracy: 0.9688\n",
            "Epoch 141/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0784 - accuracy: 0.9821\n",
            "Epoch 142/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0552 - accuracy: 0.9851\n",
            "Epoch 143/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1046 - accuracy: 0.9777\n",
            "Epoch 144/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0585 - accuracy: 0.9792\n",
            "Epoch 145/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1899 - accuracy: 0.9628\n",
            "Epoch 146/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0981 - accuracy: 0.9747\n",
            "Epoch 147/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1312 - accuracy: 0.9688\n",
            "Epoch 148/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1152 - accuracy: 0.9807\n",
            "Epoch 149/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2057 - accuracy: 0.9628\n",
            "Epoch 150/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1573 - accuracy: 0.9762\n",
            "Epoch 151/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1404 - accuracy: 0.9702\n",
            "Epoch 152/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1346 - accuracy: 0.9717\n",
            "Epoch 153/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1366 - accuracy: 0.9688\n",
            "Epoch 154/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0855 - accuracy: 0.9762\n",
            "Epoch 155/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1286 - accuracy: 0.9658\n",
            "Epoch 156/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1537 - accuracy: 0.9702\n",
            "Epoch 157/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0458 - accuracy: 0.9836\n",
            "Epoch 158/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1444 - accuracy: 0.9792\n",
            "Epoch 159/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0830 - accuracy: 0.9732\n",
            "Epoch 160/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0537 - accuracy: 0.9851\n",
            "Epoch 161/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0773 - accuracy: 0.9807\n",
            "Epoch 162/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0421 - accuracy: 0.9881\n",
            "Epoch 163/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0826 - accuracy: 0.9777\n",
            "Epoch 164/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0584 - accuracy: 0.9777\n",
            "Epoch 165/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0376 - accuracy: 0.9821\n",
            "Epoch 166/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1285 - accuracy: 0.9717\n",
            "Epoch 167/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0540 - accuracy: 0.9836\n",
            "Epoch 168/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0548 - accuracy: 0.9836\n",
            "Epoch 169/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0584 - accuracy: 0.9851\n",
            "Epoch 170/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0735 - accuracy: 0.9747\n",
            "Epoch 171/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0180 - accuracy: 0.9926\n",
            "Epoch 172/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0908 - accuracy: 0.9821\n",
            "Epoch 173/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0714 - accuracy: 0.9807\n",
            "Epoch 174/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1294 - accuracy: 0.9762\n",
            "Epoch 175/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1337 - accuracy: 0.9673\n",
            "Epoch 176/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1770 - accuracy: 0.9568\n",
            "Epoch 177/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4255 - accuracy: 0.9479\n",
            "Epoch 178/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3147 - accuracy: 0.9405\n",
            "Epoch 179/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1266 - accuracy: 0.9762\n",
            "Epoch 180/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1014 - accuracy: 0.9807\n",
            "Epoch 181/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3080 - accuracy: 0.9583\n",
            "Epoch 182/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0765 - accuracy: 0.9836\n",
            "Epoch 183/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0770 - accuracy: 0.9836\n",
            "Epoch 184/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0915 - accuracy: 0.9777\n",
            "Epoch 185/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0677 - accuracy: 0.9881\n",
            "Epoch 186/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1225 - accuracy: 0.9777\n",
            "Epoch 187/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1357 - accuracy: 0.9717\n",
            "Epoch 188/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0649 - accuracy: 0.9866\n",
            "Epoch 189/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0682 - accuracy: 0.9821\n",
            "Epoch 190/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0483 - accuracy: 0.9896\n",
            "Epoch 191/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0624 - accuracy: 0.9881\n",
            "Epoch 192/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0398 - accuracy: 0.9896\n",
            "Epoch 193/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0345 - accuracy: 0.9896\n",
            "Epoch 194/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0357 - accuracy: 0.9881\n",
            "Epoch 195/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0983 - accuracy: 0.9866\n",
            "Epoch 196/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1980 - accuracy: 0.9613\n",
            "Epoch 197/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1095 - accuracy: 0.9747\n",
            "Epoch 198/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0658 - accuracy: 0.9881\n",
            "Epoch 199/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1211 - accuracy: 0.9717\n",
            "Epoch 200/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1049 - accuracy: 0.9836\n",
            "Epoch 201/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0673 - accuracy: 0.9836\n",
            "Epoch 202/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2016 - accuracy: 0.9747\n",
            "Epoch 203/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1636 - accuracy: 0.9613\n",
            "Epoch 204/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0646 - accuracy: 0.9836\n",
            "Epoch 205/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1096 - accuracy: 0.9777\n",
            "Epoch 206/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1117 - accuracy: 0.9762\n",
            "Epoch 207/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0367 - accuracy: 0.9911\n",
            "Epoch 208/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0819 - accuracy: 0.9866\n",
            "Epoch 209/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0158 - accuracy: 0.9940\n",
            "Epoch 210/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0844 - accuracy: 0.9807\n",
            "Epoch 211/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0554 - accuracy: 0.9836\n",
            "Epoch 212/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0382 - accuracy: 0.9881\n",
            "Epoch 213/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0318 - accuracy: 0.9896\n",
            "Epoch 214/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1110 - accuracy: 0.9747\n",
            "Epoch 215/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1817 - accuracy: 0.9643\n",
            "Epoch 216/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2882 - accuracy: 0.9524\n",
            "Epoch 217/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1155 - accuracy: 0.9702\n",
            "Epoch 218/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1124 - accuracy: 0.9836\n",
            "Epoch 219/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0853 - accuracy: 0.9777\n",
            "Epoch 220/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1020 - accuracy: 0.9747\n",
            "Epoch 221/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0304 - accuracy: 0.9911\n",
            "Epoch 222/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0349 - accuracy: 0.9881\n",
            "Epoch 223/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0391 - accuracy: 0.9896\n",
            "Epoch 224/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0375 - accuracy: 0.9926\n",
            "Epoch 225/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0938 - accuracy: 0.9777\n",
            "Epoch 226/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0610 - accuracy: 0.9911\n",
            "Epoch 227/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0453 - accuracy: 0.9911\n",
            "Epoch 228/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0483 - accuracy: 0.9851\n",
            "Epoch 229/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0794 - accuracy: 0.9821\n",
            "Epoch 230/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0235 - accuracy: 0.9896\n",
            "Epoch 231/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0478 - accuracy: 0.9896\n",
            "Epoch 232/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0581 - accuracy: 0.9836\n",
            "Epoch 233/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1027 - accuracy: 0.9807\n",
            "Epoch 234/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1261 - accuracy: 0.9777\n",
            "Epoch 235/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0352 - accuracy: 0.9851\n",
            "Epoch 236/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2183 - accuracy: 0.9658\n",
            "Epoch 237/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1881 - accuracy: 0.9673\n",
            "Epoch 238/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2010 - accuracy: 0.9702\n",
            "Epoch 239/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4046 - accuracy: 0.9554\n",
            "Epoch 240/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0490 - accuracy: 0.9821\n",
            "Epoch 241/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0661 - accuracy: 0.9866\n",
            "Epoch 242/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1206 - accuracy: 0.9732\n",
            "Epoch 243/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1172 - accuracy: 0.9807\n",
            "Epoch 244/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0668 - accuracy: 0.9851\n",
            "Epoch 245/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0789 - accuracy: 0.9851\n",
            "Epoch 246/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1019 - accuracy: 0.9821\n",
            "Epoch 247/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0483 - accuracy: 0.9896\n",
            "Epoch 248/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0924 - accuracy: 0.9821\n",
            "Epoch 249/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0765 - accuracy: 0.9807\n",
            "Epoch 250/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0273 - accuracy: 0.9955\n",
            "Epoch 251/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1022 - accuracy: 0.9851\n",
            "Epoch 252/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0967 - accuracy: 0.9851\n",
            "Epoch 253/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1086 - accuracy: 0.9807\n",
            "Epoch 254/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0913 - accuracy: 0.9807\n",
            "Epoch 255/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0618 - accuracy: 0.9851\n",
            "Epoch 256/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0285 - accuracy: 0.9896\n",
            "Epoch 257/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1135 - accuracy: 0.9821\n",
            "Epoch 258/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1651 - accuracy: 0.9747\n",
            "Epoch 259/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0641 - accuracy: 0.9866\n",
            "Epoch 260/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0349 - accuracy: 0.9881\n",
            "Epoch 261/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0626 - accuracy: 0.9926\n",
            "Epoch 262/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0503 - accuracy: 0.9911\n",
            "Epoch 263/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0414 - accuracy: 0.9940\n",
            "Epoch 264/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0183 - accuracy: 0.9940\n",
            "Epoch 265/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1081 - accuracy: 0.9866\n",
            "Epoch 266/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0316 - accuracy: 0.9926\n",
            "Epoch 267/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0513 - accuracy: 0.9836\n",
            "Epoch 268/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0566 - accuracy: 0.9896\n",
            "Epoch 269/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0804 - accuracy: 0.9807\n",
            "Epoch 270/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0841 - accuracy: 0.9851\n",
            "Epoch 271/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0523 - accuracy: 0.9836\n",
            "Epoch 272/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1077 - accuracy: 0.9792\n",
            "Epoch 273/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1300 - accuracy: 0.9688\n",
            "Epoch 274/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1698 - accuracy: 0.9702\n",
            "Epoch 275/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1312 - accuracy: 0.9702\n",
            "Epoch 276/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1564 - accuracy: 0.9792\n",
            "Epoch 277/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0401 - accuracy: 0.9911\n",
            "Epoch 278/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1345 - accuracy: 0.9807\n",
            "Epoch 279/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1035 - accuracy: 0.9792\n",
            "Epoch 280/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1328 - accuracy: 0.9762\n",
            "Epoch 281/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1081 - accuracy: 0.9807\n",
            "Epoch 282/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0692 - accuracy: 0.9881\n",
            "Epoch 283/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2137 - accuracy: 0.9762\n",
            "Epoch 284/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0917 - accuracy: 0.9851\n",
            "Epoch 285/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0950 - accuracy: 0.9866\n",
            "Epoch 286/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0918 - accuracy: 0.9836\n",
            "Epoch 287/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0851 - accuracy: 0.9792\n",
            "Epoch 288/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0681 - accuracy: 0.9866\n",
            "Epoch 289/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0775 - accuracy: 0.9881\n",
            "Epoch 290/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0931 - accuracy: 0.9836\n",
            "Epoch 291/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0689 - accuracy: 0.9911\n",
            "Epoch 292/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1114 - accuracy: 0.9851\n",
            "Epoch 293/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1732 - accuracy: 0.9613\n",
            "Epoch 294/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1564 - accuracy: 0.9762\n",
            "Epoch 295/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0870 - accuracy: 0.9821\n",
            "Epoch 296/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1103 - accuracy: 0.9911\n",
            "Epoch 297/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1464 - accuracy: 0.9866\n",
            "Epoch 298/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1897 - accuracy: 0.9807\n",
            "Epoch 299/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1074 - accuracy: 0.9792\n",
            "Epoch 300/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0629 - accuracy: 0.9851\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 14.2900 - accuracy: 0.1012\n",
            "Model: \"sequential_153\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_459 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_765 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_459 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_460 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_766 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_460 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_461 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_767 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_461 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_153 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_459 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_768 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_460 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_769 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_461 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 4.0308 - accuracy: 0.0253\n",
            "Epoch 2/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 3.6892 - accuracy: 0.0685\n",
            "Epoch 3/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 3.1092 - accuracy: 0.1622\n",
            "Epoch 4/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 2.4478 - accuracy: 0.3125\n",
            "Epoch 5/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.8443 - accuracy: 0.4613\n",
            "Epoch 6/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.3925 - accuracy: 0.5938\n",
            "Epoch 7/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.0334 - accuracy: 0.6890\n",
            "Epoch 8/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.0077 - accuracy: 0.6964\n",
            "Epoch 9/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.6838 - accuracy: 0.7961\n",
            "Epoch 10/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.7023 - accuracy: 0.7872\n",
            "Epoch 11/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.5537 - accuracy: 0.8318\n",
            "Epoch 12/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.4036 - accuracy: 0.8869\n",
            "Epoch 13/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3324 - accuracy: 0.8988\n",
            "Epoch 14/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3459 - accuracy: 0.8810\n",
            "Epoch 15/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3041 - accuracy: 0.9003\n",
            "Epoch 16/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2233 - accuracy: 0.9330\n",
            "Epoch 17/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2380 - accuracy: 0.9196\n",
            "Epoch 18/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2800 - accuracy: 0.9122\n",
            "Epoch 19/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4084 - accuracy: 0.8899\n",
            "Epoch 20/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2293 - accuracy: 0.9226\n",
            "Epoch 21/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2210 - accuracy: 0.9241\n",
            "Epoch 22/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1445 - accuracy: 0.9568\n",
            "Epoch 23/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2108 - accuracy: 0.9390\n",
            "Epoch 24/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1999 - accuracy: 0.9435\n",
            "Epoch 25/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3116 - accuracy: 0.9048\n",
            "Epoch 26/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1516 - accuracy: 0.9479\n",
            "Epoch 27/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1455 - accuracy: 0.9598\n",
            "Epoch 28/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1362 - accuracy: 0.9494\n",
            "Epoch 29/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1699 - accuracy: 0.9405\n",
            "Epoch 30/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1720 - accuracy: 0.9509\n",
            "Epoch 31/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1439 - accuracy: 0.9449\n",
            "Epoch 32/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1672 - accuracy: 0.9509\n",
            "Epoch 33/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2221 - accuracy: 0.9494\n",
            "Epoch 34/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0911 - accuracy: 0.9702\n",
            "Epoch 35/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1598 - accuracy: 0.9554\n",
            "Epoch 36/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2287 - accuracy: 0.9390\n",
            "Epoch 37/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1118 - accuracy: 0.9673\n",
            "Epoch 38/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0933 - accuracy: 0.9702\n",
            "Epoch 39/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0923 - accuracy: 0.9688\n",
            "Epoch 40/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2430 - accuracy: 0.9449\n",
            "Epoch 41/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1760 - accuracy: 0.9405\n",
            "Epoch 42/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0888 - accuracy: 0.9688\n",
            "Epoch 43/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1646 - accuracy: 0.9554\n",
            "Epoch 44/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1621 - accuracy: 0.9613\n",
            "Epoch 45/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3079 - accuracy: 0.9241\n",
            "Epoch 46/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2281 - accuracy: 0.9435\n",
            "Epoch 47/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1650 - accuracy: 0.9568\n",
            "Epoch 48/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1328 - accuracy: 0.9568\n",
            "Epoch 49/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0671 - accuracy: 0.9836\n",
            "Epoch 50/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1304 - accuracy: 0.9643\n",
            "Epoch 51/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1633 - accuracy: 0.9583\n",
            "Epoch 52/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1105 - accuracy: 0.9598\n",
            "Epoch 53/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1489 - accuracy: 0.9494\n",
            "Epoch 54/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1590 - accuracy: 0.9539\n",
            "Epoch 55/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0934 - accuracy: 0.9792\n",
            "Epoch 56/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0963 - accuracy: 0.9732\n",
            "Epoch 57/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0805 - accuracy: 0.9747\n",
            "Epoch 58/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1085 - accuracy: 0.9688\n",
            "Epoch 59/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1216 - accuracy: 0.9688\n",
            "Epoch 60/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0603 - accuracy: 0.9821\n",
            "Epoch 61/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0775 - accuracy: 0.9777\n",
            "Epoch 62/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1273 - accuracy: 0.9732\n",
            "Epoch 63/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1410 - accuracy: 0.9673\n",
            "Epoch 64/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1741 - accuracy: 0.9464\n",
            "Epoch 65/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1224 - accuracy: 0.9598\n",
            "Epoch 66/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0968 - accuracy: 0.9762\n",
            "Epoch 67/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1311 - accuracy: 0.9762\n",
            "Epoch 68/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0523 - accuracy: 0.9851\n",
            "Epoch 69/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1022 - accuracy: 0.9762\n",
            "Epoch 70/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0835 - accuracy: 0.9762\n",
            "Epoch 71/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1079 - accuracy: 0.9628\n",
            "Epoch 72/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1359 - accuracy: 0.9643\n",
            "Epoch 73/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1856 - accuracy: 0.9479\n",
            "Epoch 74/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1709 - accuracy: 0.9539\n",
            "Epoch 75/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1591 - accuracy: 0.9598\n",
            "Epoch 76/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0971 - accuracy: 0.9762\n",
            "Epoch 77/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1076 - accuracy: 0.9747\n",
            "Epoch 78/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2286 - accuracy: 0.9568\n",
            "Epoch 79/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1301 - accuracy: 0.9673\n",
            "Epoch 80/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0903 - accuracy: 0.9762\n",
            "Epoch 81/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1210 - accuracy: 0.9702\n",
            "Epoch 82/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0890 - accuracy: 0.9747\n",
            "Epoch 83/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1101 - accuracy: 0.9717\n",
            "Epoch 84/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1235 - accuracy: 0.9688\n",
            "Epoch 85/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1273 - accuracy: 0.9717\n",
            "Epoch 86/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1617 - accuracy: 0.9717\n",
            "Epoch 87/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1768 - accuracy: 0.9583\n",
            "Epoch 88/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0890 - accuracy: 0.9732\n",
            "Epoch 89/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1504 - accuracy: 0.9717\n",
            "Epoch 90/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0705 - accuracy: 0.9732\n",
            "Epoch 91/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1829 - accuracy: 0.9628\n",
            "Epoch 92/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1181 - accuracy: 0.9717\n",
            "Epoch 93/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0873 - accuracy: 0.9762\n",
            "Epoch 94/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1143 - accuracy: 0.9732\n",
            "Epoch 95/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0518 - accuracy: 0.9866\n",
            "Epoch 96/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1273 - accuracy: 0.9807\n",
            "Epoch 97/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0839 - accuracy: 0.9821\n",
            "Epoch 98/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0481 - accuracy: 0.9821\n",
            "Epoch 99/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0683 - accuracy: 0.9807\n",
            "Epoch 100/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1527 - accuracy: 0.9658\n",
            "Epoch 101/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1260 - accuracy: 0.9643\n",
            "Epoch 102/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1150 - accuracy: 0.9598\n",
            "Epoch 103/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0931 - accuracy: 0.9717\n",
            "Epoch 104/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4018 - accuracy: 0.9315\n",
            "Epoch 105/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.5413 - accuracy: 0.9360\n",
            "Epoch 106/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3275 - accuracy: 0.9464\n",
            "Epoch 107/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2896 - accuracy: 0.9613\n",
            "Epoch 108/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3634 - accuracy: 0.9375\n",
            "Epoch 109/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2492 - accuracy: 0.9464\n",
            "Epoch 110/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0938 - accuracy: 0.9792\n",
            "Epoch 111/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1000 - accuracy: 0.9747\n",
            "Epoch 112/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1376 - accuracy: 0.9717\n",
            "Epoch 113/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1685 - accuracy: 0.9673\n",
            "Epoch 114/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1201 - accuracy: 0.9643\n",
            "Epoch 115/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1734 - accuracy: 0.9717\n",
            "Epoch 116/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0703 - accuracy: 0.9762\n",
            "Epoch 117/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0778 - accuracy: 0.9762\n",
            "Epoch 118/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1320 - accuracy: 0.9717\n",
            "Epoch 119/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0809 - accuracy: 0.9747\n",
            "Epoch 120/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0466 - accuracy: 0.9836\n",
            "Epoch 121/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0394 - accuracy: 0.9896\n",
            "Epoch 122/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0445 - accuracy: 0.9881\n",
            "Epoch 123/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0343 - accuracy: 0.9866\n",
            "Epoch 124/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0869 - accuracy: 0.9866\n",
            "Epoch 125/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0708 - accuracy: 0.9866\n",
            "Epoch 126/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0794 - accuracy: 0.9777\n",
            "Epoch 127/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0680 - accuracy: 0.9777\n",
            "Epoch 128/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0526 - accuracy: 0.9881\n",
            "Epoch 129/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0597 - accuracy: 0.9836\n",
            "Epoch 130/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0836 - accuracy: 0.9807\n",
            "Epoch 131/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0736 - accuracy: 0.9807\n",
            "Epoch 132/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1003 - accuracy: 0.9807\n",
            "Epoch 133/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0892 - accuracy: 0.9881\n",
            "Epoch 134/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0898 - accuracy: 0.9792\n",
            "Epoch 135/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0801 - accuracy: 0.9777\n",
            "Epoch 136/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1069 - accuracy: 0.9792\n",
            "Epoch 137/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0903 - accuracy: 0.9896\n",
            "Epoch 138/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0793 - accuracy: 0.9807\n",
            "Epoch 139/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0397 - accuracy: 0.9866\n",
            "Epoch 140/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0257 - accuracy: 0.9926\n",
            "Epoch 141/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0355 - accuracy: 0.9881\n",
            "Epoch 142/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0169 - accuracy: 0.9911\n",
            "Epoch 143/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0252 - accuracy: 0.9926\n",
            "Epoch 144/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1837 - accuracy: 0.9732\n",
            "Epoch 145/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1056 - accuracy: 0.9762\n",
            "Epoch 146/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0574 - accuracy: 0.9851\n",
            "Epoch 147/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2604 - accuracy: 0.9643\n",
            "Epoch 148/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1124 - accuracy: 0.9688\n",
            "Epoch 149/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1997 - accuracy: 0.9673\n",
            "Epoch 150/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3455 - accuracy: 0.9360\n",
            "Epoch 151/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1257 - accuracy: 0.9747\n",
            "Epoch 152/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1036 - accuracy: 0.9747\n",
            "Epoch 153/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2762 - accuracy: 0.9598\n",
            "Epoch 154/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2261 - accuracy: 0.9717\n",
            "Epoch 155/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1513 - accuracy: 0.9732\n",
            "Epoch 156/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1533 - accuracy: 0.9762\n",
            "Epoch 157/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1418 - accuracy: 0.9821\n",
            "Epoch 158/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1030 - accuracy: 0.9747\n",
            "Epoch 159/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0979 - accuracy: 0.9792\n",
            "Epoch 160/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0294 - accuracy: 0.9881\n",
            "Epoch 161/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0371 - accuracy: 0.9881\n",
            "Epoch 162/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0562 - accuracy: 0.9866\n",
            "Epoch 163/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0641 - accuracy: 0.9896\n",
            "Epoch 164/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1191 - accuracy: 0.9836\n",
            "Epoch 165/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0739 - accuracy: 0.9821\n",
            "Epoch 166/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0916 - accuracy: 0.9747\n",
            "Epoch 167/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0877 - accuracy: 0.9851\n",
            "Epoch 168/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0532 - accuracy: 0.9881\n",
            "Epoch 169/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0887 - accuracy: 0.9792\n",
            "Epoch 170/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1051 - accuracy: 0.9836\n",
            "Epoch 171/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1265 - accuracy: 0.9747\n",
            "Epoch 172/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0835 - accuracy: 0.9747\n",
            "Epoch 173/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0483 - accuracy: 0.9911\n",
            "Epoch 174/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0732 - accuracy: 0.9911\n",
            "Epoch 175/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0703 - accuracy: 0.9732\n",
            "Epoch 176/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0754 - accuracy: 0.9732\n",
            "Epoch 177/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1077 - accuracy: 0.9702\n",
            "Epoch 178/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2041 - accuracy: 0.9732\n",
            "Epoch 179/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1085 - accuracy: 0.9762\n",
            "Epoch 180/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0986 - accuracy: 0.9821\n",
            "Epoch 181/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0727 - accuracy: 0.9762\n",
            "Epoch 182/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1051 - accuracy: 0.9762\n",
            "Epoch 183/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1177 - accuracy: 0.9792\n",
            "Epoch 184/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0686 - accuracy: 0.9807\n",
            "Epoch 185/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0661 - accuracy: 0.9836\n",
            "Epoch 186/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1070 - accuracy: 0.9792\n",
            "Epoch 187/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0592 - accuracy: 0.9807\n",
            "Epoch 188/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0697 - accuracy: 0.9807\n",
            "Epoch 189/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0604 - accuracy: 0.9851\n",
            "Epoch 190/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1017 - accuracy: 0.9821\n",
            "Epoch 191/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0782 - accuracy: 0.9851\n",
            "Epoch 192/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0621 - accuracy: 0.9866\n",
            "Epoch 193/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0575 - accuracy: 0.9836\n",
            "Epoch 194/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1038 - accuracy: 0.9792\n",
            "Epoch 195/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0804 - accuracy: 0.9821\n",
            "Epoch 196/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0898 - accuracy: 0.9792\n",
            "Epoch 197/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0790 - accuracy: 0.9821\n",
            "Epoch 198/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0912 - accuracy: 0.9836\n",
            "Epoch 199/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0618 - accuracy: 0.9851\n",
            "Epoch 200/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0789 - accuracy: 0.9866\n",
            "Epoch 201/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1023 - accuracy: 0.9777\n",
            "Epoch 202/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1126 - accuracy: 0.9777\n",
            "Epoch 203/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2185 - accuracy: 0.9688\n",
            "Epoch 204/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2418 - accuracy: 0.9673\n",
            "Epoch 205/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1991 - accuracy: 0.9821\n",
            "Epoch 206/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1316 - accuracy: 0.9777\n",
            "Epoch 207/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0548 - accuracy: 0.9836\n",
            "Epoch 208/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1795 - accuracy: 0.9821\n",
            "Epoch 209/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1456 - accuracy: 0.9792\n",
            "Epoch 210/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1201 - accuracy: 0.9851\n",
            "Epoch 211/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1966 - accuracy: 0.9777\n",
            "Epoch 212/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0595 - accuracy: 0.9807\n",
            "Epoch 213/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1416 - accuracy: 0.9851\n",
            "Epoch 214/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1499 - accuracy: 0.9628\n",
            "Epoch 215/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1183 - accuracy: 0.9762\n",
            "Epoch 216/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1750 - accuracy: 0.9702\n",
            "Epoch 217/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1013 - accuracy: 0.9851\n",
            "Epoch 218/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0899 - accuracy: 0.9836\n",
            "Epoch 219/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1835 - accuracy: 0.9732\n",
            "Epoch 220/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1344 - accuracy: 0.9747\n",
            "Epoch 221/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0790 - accuracy: 0.9866\n",
            "Epoch 222/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1247 - accuracy: 0.9762\n",
            "Epoch 223/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3314 - accuracy: 0.9643\n",
            "Epoch 224/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1345 - accuracy: 0.9747\n",
            "Epoch 225/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1191 - accuracy: 0.9777\n",
            "Epoch 226/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0595 - accuracy: 0.9851\n",
            "Epoch 227/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0954 - accuracy: 0.9821\n",
            "Epoch 228/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0840 - accuracy: 0.9821\n",
            "Epoch 229/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0854 - accuracy: 0.9866\n",
            "Epoch 230/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0740 - accuracy: 0.9866\n",
            "Epoch 231/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0313 - accuracy: 0.9911\n",
            "Epoch 232/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0585 - accuracy: 0.9896\n",
            "Epoch 233/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0615 - accuracy: 0.9896\n",
            "Epoch 234/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0283 - accuracy: 0.9911\n",
            "Epoch 235/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0315 - accuracy: 0.9896\n",
            "Epoch 236/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0965 - accuracy: 0.9792\n",
            "Epoch 237/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1072 - accuracy: 0.9792\n",
            "Epoch 238/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0357 - accuracy: 0.9911\n",
            "Epoch 239/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1460 - accuracy: 0.9747\n",
            "Epoch 240/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0658 - accuracy: 0.9866\n",
            "Epoch 241/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0708 - accuracy: 0.9851\n",
            "Epoch 242/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1123 - accuracy: 0.9851\n",
            "Epoch 243/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2916 - accuracy: 0.9717\n",
            "Epoch 244/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1578 - accuracy: 0.9717\n",
            "Epoch 245/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1802 - accuracy: 0.9821\n",
            "Epoch 246/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1074 - accuracy: 0.9777\n",
            "Epoch 247/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0718 - accuracy: 0.9836\n",
            "Epoch 248/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0837 - accuracy: 0.9777\n",
            "Epoch 249/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1769 - accuracy: 0.9762\n",
            "Epoch 250/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2853 - accuracy: 0.9628\n",
            "Epoch 251/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1294 - accuracy: 0.9807\n",
            "Epoch 252/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0492 - accuracy: 0.9896\n",
            "Epoch 253/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0789 - accuracy: 0.9836\n",
            "Epoch 254/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1217 - accuracy: 0.9792\n",
            "Epoch 255/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0593 - accuracy: 0.9821\n",
            "Epoch 256/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0529 - accuracy: 0.9896\n",
            "Epoch 257/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0870 - accuracy: 0.9807\n",
            "Epoch 258/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1655 - accuracy: 0.9807\n",
            "Epoch 259/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1701 - accuracy: 0.9851\n",
            "Epoch 260/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1443 - accuracy: 0.9717\n",
            "Epoch 261/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0825 - accuracy: 0.9821\n",
            "Epoch 262/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1995 - accuracy: 0.9836\n",
            "Epoch 263/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0832 - accuracy: 0.9836\n",
            "Epoch 264/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0464 - accuracy: 0.9911\n",
            "Epoch 265/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0655 - accuracy: 0.9821\n",
            "Epoch 266/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1192 - accuracy: 0.9881\n",
            "Epoch 267/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1774 - accuracy: 0.9747\n",
            "Epoch 268/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2034 - accuracy: 0.9732\n",
            "Epoch 269/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0912 - accuracy: 0.9836\n",
            "Epoch 270/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0685 - accuracy: 0.9881\n",
            "Epoch 271/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1153 - accuracy: 0.9851\n",
            "Epoch 272/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1352 - accuracy: 0.9836\n",
            "Epoch 273/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1434 - accuracy: 0.9807\n",
            "Epoch 274/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1455 - accuracy: 0.9807\n",
            "Epoch 275/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0707 - accuracy: 0.9896\n",
            "Epoch 276/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0736 - accuracy: 0.9896\n",
            "Epoch 277/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0863 - accuracy: 0.9821\n",
            "Epoch 278/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1252 - accuracy: 0.9807\n",
            "Epoch 279/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1517 - accuracy: 0.9851\n",
            "Epoch 280/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1768 - accuracy: 0.9732\n",
            "Epoch 281/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1346 - accuracy: 0.9866\n",
            "Epoch 282/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2038 - accuracy: 0.9836\n",
            "Epoch 283/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0671 - accuracy: 0.9881\n",
            "Epoch 284/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1465 - accuracy: 0.9792\n",
            "Epoch 285/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1840 - accuracy: 0.9732\n",
            "Epoch 286/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2171 - accuracy: 0.9762\n",
            "Epoch 287/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1204 - accuracy: 0.9747\n",
            "Epoch 288/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1340 - accuracy: 0.9792\n",
            "Epoch 289/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0653 - accuracy: 0.9866\n",
            "Epoch 290/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1063 - accuracy: 0.9807\n",
            "Epoch 291/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1516 - accuracy: 0.9821\n",
            "Epoch 292/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1233 - accuracy: 0.9807\n",
            "Epoch 293/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0758 - accuracy: 0.9896\n",
            "Epoch 294/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1378 - accuracy: 0.9747\n",
            "Epoch 295/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0367 - accuracy: 0.9881\n",
            "Epoch 296/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1859 - accuracy: 0.9851\n",
            "Epoch 297/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1243 - accuracy: 0.9792\n",
            "Epoch 298/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0747 - accuracy: 0.9836\n",
            "Epoch 299/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0697 - accuracy: 0.9866\n",
            "Epoch 300/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0461 - accuracy: 0.9896\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 25.5812 - accuracy: 0.0893\n",
            "Model: \"sequential_154\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_462 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_770 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_462 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_463 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_771 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_463 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_464 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_772 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_464 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_154 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_462 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_773 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_463 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_774 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_464 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 4.0272 - accuracy: 0.0268\n",
            "Epoch 2/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 3.5940 - accuracy: 0.0848\n",
            "Epoch 3/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 2.9280 - accuracy: 0.2188\n",
            "Epoch 4/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 2.2747 - accuracy: 0.3423\n",
            "Epoch 5/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 1.8593 - accuracy: 0.4539\n",
            "Epoch 6/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.6541 - accuracy: 0.5149\n",
            "Epoch 7/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.3688 - accuracy: 0.5729\n",
            "Epoch 8/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.2158 - accuracy: 0.6116\n",
            "Epoch 9/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.0714 - accuracy: 0.6473\n",
            "Epoch 10/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.8910 - accuracy: 0.6994\n",
            "Epoch 11/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.7682 - accuracy: 0.7634\n",
            "Epoch 12/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.7440 - accuracy: 0.7768\n",
            "Epoch 13/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.5842 - accuracy: 0.8214\n",
            "Epoch 14/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.6644 - accuracy: 0.7812\n",
            "Epoch 15/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.5123 - accuracy: 0.8318\n",
            "Epoch 16/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.5388 - accuracy: 0.8021\n",
            "Epoch 17/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4484 - accuracy: 0.8452\n",
            "Epoch 18/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3360 - accuracy: 0.8854\n",
            "Epoch 19/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4485 - accuracy: 0.8601\n",
            "Epoch 20/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3535 - accuracy: 0.8973\n",
            "Epoch 21/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4007 - accuracy: 0.8646\n",
            "Epoch 22/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3606 - accuracy: 0.8824\n",
            "Epoch 23/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3399 - accuracy: 0.8929\n",
            "Epoch 24/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2975 - accuracy: 0.8929\n",
            "Epoch 25/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3454 - accuracy: 0.8899\n",
            "Epoch 26/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2797 - accuracy: 0.9182\n",
            "Epoch 27/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2839 - accuracy: 0.9167\n",
            "Epoch 28/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3109 - accuracy: 0.9003\n",
            "Epoch 29/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4011 - accuracy: 0.8810\n",
            "Epoch 30/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3165 - accuracy: 0.9122\n",
            "Epoch 31/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2573 - accuracy: 0.9152\n",
            "Epoch 32/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2218 - accuracy: 0.9435\n",
            "Epoch 33/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2758 - accuracy: 0.9107\n",
            "Epoch 34/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2570 - accuracy: 0.9122\n",
            "Epoch 35/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1979 - accuracy: 0.9420\n",
            "Epoch 36/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2091 - accuracy: 0.9360\n",
            "Epoch 37/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2388 - accuracy: 0.9390\n",
            "Epoch 38/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2090 - accuracy: 0.9449\n",
            "Epoch 39/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2022 - accuracy: 0.9390\n",
            "Epoch 40/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2176 - accuracy: 0.9301\n",
            "Epoch 41/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2210 - accuracy: 0.9271\n",
            "Epoch 42/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2356 - accuracy: 0.9271\n",
            "Epoch 43/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1926 - accuracy: 0.9405\n",
            "Epoch 44/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1978 - accuracy: 0.9360\n",
            "Epoch 45/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1986 - accuracy: 0.9390\n",
            "Epoch 46/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2571 - accuracy: 0.9256\n",
            "Epoch 47/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1942 - accuracy: 0.9315\n",
            "Epoch 48/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2418 - accuracy: 0.9315\n",
            "Epoch 49/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2206 - accuracy: 0.9464\n",
            "Epoch 50/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1919 - accuracy: 0.9464\n",
            "Epoch 51/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1672 - accuracy: 0.9449\n",
            "Epoch 52/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1836 - accuracy: 0.9420\n",
            "Epoch 53/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3699 - accuracy: 0.8943\n",
            "Epoch 54/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2733 - accuracy: 0.9152\n",
            "Epoch 55/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2267 - accuracy: 0.9449\n",
            "Epoch 56/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1723 - accuracy: 0.9420\n",
            "Epoch 57/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1135 - accuracy: 0.9598\n",
            "Epoch 58/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1552 - accuracy: 0.9464\n",
            "Epoch 59/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1815 - accuracy: 0.9375\n",
            "Epoch 60/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1775 - accuracy: 0.9420\n",
            "Epoch 61/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2216 - accuracy: 0.9330\n",
            "Epoch 62/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1903 - accuracy: 0.9330\n",
            "Epoch 63/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1133 - accuracy: 0.9688\n",
            "Epoch 64/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1634 - accuracy: 0.9568\n",
            "Epoch 65/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1280 - accuracy: 0.9598\n",
            "Epoch 66/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1439 - accuracy: 0.9643\n",
            "Epoch 67/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1529 - accuracy: 0.9524\n",
            "Epoch 68/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1305 - accuracy: 0.9673\n",
            "Epoch 69/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1229 - accuracy: 0.9658\n",
            "Epoch 70/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1487 - accuracy: 0.9598\n",
            "Epoch 71/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1812 - accuracy: 0.9405\n",
            "Epoch 72/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2312 - accuracy: 0.9390\n",
            "Epoch 73/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2351 - accuracy: 0.9315\n",
            "Epoch 74/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0817 - accuracy: 0.9717\n",
            "Epoch 75/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1581 - accuracy: 0.9509\n",
            "Epoch 76/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1605 - accuracy: 0.9509\n",
            "Epoch 77/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1265 - accuracy: 0.9509\n",
            "Epoch 78/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1677 - accuracy: 0.9598\n",
            "Epoch 79/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1146 - accuracy: 0.9688\n",
            "Epoch 80/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1157 - accuracy: 0.9628\n",
            "Epoch 81/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1809 - accuracy: 0.9613\n",
            "Epoch 82/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1343 - accuracy: 0.9628\n",
            "Epoch 83/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1574 - accuracy: 0.9583\n",
            "Epoch 84/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0803 - accuracy: 0.9688\n",
            "Epoch 85/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0968 - accuracy: 0.9702\n",
            "Epoch 86/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1218 - accuracy: 0.9673\n",
            "Epoch 87/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1084 - accuracy: 0.9702\n",
            "Epoch 88/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0721 - accuracy: 0.9777\n",
            "Epoch 89/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1140 - accuracy: 0.9628\n",
            "Epoch 90/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1892 - accuracy: 0.9435\n",
            "Epoch 91/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1180 - accuracy: 0.9658\n",
            "Epoch 92/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1479 - accuracy: 0.9583\n",
            "Epoch 93/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1591 - accuracy: 0.9509\n",
            "Epoch 94/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1058 - accuracy: 0.9717\n",
            "Epoch 95/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1154 - accuracy: 0.9628\n",
            "Epoch 96/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0924 - accuracy: 0.9702\n",
            "Epoch 97/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0782 - accuracy: 0.9777\n",
            "Epoch 98/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1234 - accuracy: 0.9643\n",
            "Epoch 99/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1335 - accuracy: 0.9673\n",
            "Epoch 100/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.4532 - accuracy: 0.8988\n",
            "Epoch 101/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2729 - accuracy: 0.9405\n",
            "Epoch 102/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2109 - accuracy: 0.9435\n",
            "Epoch 103/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1583 - accuracy: 0.9613\n",
            "Epoch 104/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0529 - accuracy: 0.9807\n",
            "Epoch 105/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0926 - accuracy: 0.9792\n",
            "Epoch 106/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1499 - accuracy: 0.9509\n",
            "Epoch 107/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1253 - accuracy: 0.9732\n",
            "Epoch 108/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1535 - accuracy: 0.9628\n",
            "Epoch 109/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1277 - accuracy: 0.9688\n",
            "Epoch 110/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1775 - accuracy: 0.9524\n",
            "Epoch 111/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0887 - accuracy: 0.9673\n",
            "Epoch 112/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0605 - accuracy: 0.9792\n",
            "Epoch 113/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0792 - accuracy: 0.9688\n",
            "Epoch 114/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1013 - accuracy: 0.9747\n",
            "Epoch 115/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1121 - accuracy: 0.9747\n",
            "Epoch 116/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0761 - accuracy: 0.9807\n",
            "Epoch 117/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1196 - accuracy: 0.9673\n",
            "Epoch 118/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0870 - accuracy: 0.9777\n",
            "Epoch 119/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0968 - accuracy: 0.9658\n",
            "Epoch 120/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1145 - accuracy: 0.9747\n",
            "Epoch 121/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1957 - accuracy: 0.9449\n",
            "Epoch 122/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1676 - accuracy: 0.9539\n",
            "Epoch 123/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1749 - accuracy: 0.9613\n",
            "Epoch 124/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1375 - accuracy: 0.9673\n",
            "Epoch 125/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1751 - accuracy: 0.9583\n",
            "Epoch 126/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1047 - accuracy: 0.9628\n",
            "Epoch 127/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0623 - accuracy: 0.9807\n",
            "Epoch 128/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1127 - accuracy: 0.9688\n",
            "Epoch 129/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0398 - accuracy: 0.9866\n",
            "Epoch 130/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0815 - accuracy: 0.9747\n",
            "Epoch 131/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0783 - accuracy: 0.9717\n",
            "Epoch 132/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1297 - accuracy: 0.9673\n",
            "Epoch 133/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0841 - accuracy: 0.9821\n",
            "Epoch 134/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0537 - accuracy: 0.9777\n",
            "Epoch 135/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1619 - accuracy: 0.9568\n",
            "Epoch 136/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1586 - accuracy: 0.9613\n",
            "Epoch 137/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1339 - accuracy: 0.9613\n",
            "Epoch 138/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1314 - accuracy: 0.9732\n",
            "Epoch 139/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1092 - accuracy: 0.9717\n",
            "Epoch 140/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2369 - accuracy: 0.9568\n",
            "Epoch 141/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0610 - accuracy: 0.9747\n",
            "Epoch 142/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0368 - accuracy: 0.9851\n",
            "Epoch 143/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0859 - accuracy: 0.9732\n",
            "Epoch 144/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1973 - accuracy: 0.9509\n",
            "Epoch 145/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3121 - accuracy: 0.9330\n",
            "Epoch 146/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1224 - accuracy: 0.9613\n",
            "Epoch 147/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0805 - accuracy: 0.9732\n",
            "Epoch 148/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0747 - accuracy: 0.9792\n",
            "Epoch 149/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0696 - accuracy: 0.9866\n",
            "Epoch 150/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1066 - accuracy: 0.9732\n",
            "Epoch 151/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1511 - accuracy: 0.9613\n",
            "Epoch 152/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0695 - accuracy: 0.9747\n",
            "Epoch 153/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1506 - accuracy: 0.9643\n",
            "Epoch 154/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0446 - accuracy: 0.9896\n",
            "Epoch 155/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0755 - accuracy: 0.9762\n",
            "Epoch 156/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0690 - accuracy: 0.9732\n",
            "Epoch 157/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0842 - accuracy: 0.9688\n",
            "Epoch 158/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1911 - accuracy: 0.9613\n",
            "Epoch 159/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0943 - accuracy: 0.9762\n",
            "Epoch 160/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1358 - accuracy: 0.9717\n",
            "Epoch 161/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0603 - accuracy: 0.9851\n",
            "Epoch 162/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1171 - accuracy: 0.9643\n",
            "Epoch 163/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0800 - accuracy: 0.9851\n",
            "Epoch 164/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1296 - accuracy: 0.9747\n",
            "Epoch 165/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1041 - accuracy: 0.9732\n",
            "Epoch 166/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2347 - accuracy: 0.9539\n",
            "Epoch 167/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0881 - accuracy: 0.9717\n",
            "Epoch 168/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0576 - accuracy: 0.9836\n",
            "Epoch 169/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0827 - accuracy: 0.9747\n",
            "Epoch 170/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1589 - accuracy: 0.9658\n",
            "Epoch 171/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1271 - accuracy: 0.9673\n",
            "Epoch 172/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1007 - accuracy: 0.9702\n",
            "Epoch 173/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1438 - accuracy: 0.9732\n",
            "Epoch 174/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0989 - accuracy: 0.9732\n",
            "Epoch 175/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1325 - accuracy: 0.9762\n",
            "Epoch 176/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1321 - accuracy: 0.9598\n",
            "Epoch 177/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0956 - accuracy: 0.9688\n",
            "Epoch 178/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0878 - accuracy: 0.9762\n",
            "Epoch 179/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1603 - accuracy: 0.9732\n",
            "Epoch 180/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0491 - accuracy: 0.9851\n",
            "Epoch 181/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0584 - accuracy: 0.9792\n",
            "Epoch 182/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1344 - accuracy: 0.9688\n",
            "Epoch 183/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1143 - accuracy: 0.9732\n",
            "Epoch 184/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1672 - accuracy: 0.9598\n",
            "Epoch 185/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0921 - accuracy: 0.9777\n",
            "Epoch 186/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1288 - accuracy: 0.9658\n",
            "Epoch 188/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1138 - accuracy: 0.9628\n",
            "Epoch 189/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0512 - accuracy: 0.9836\n",
            "Epoch 190/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0908 - accuracy: 0.9688\n",
            "Epoch 191/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1557 - accuracy: 0.9717\n",
            "Epoch 192/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1048 - accuracy: 0.9732\n",
            "Epoch 193/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0576 - accuracy: 0.9807\n",
            "Epoch 194/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0634 - accuracy: 0.9836\n",
            "Epoch 195/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0647 - accuracy: 0.9792\n",
            "Epoch 196/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1184 - accuracy: 0.9732\n",
            "Epoch 197/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0579 - accuracy: 0.9836\n",
            "Epoch 198/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1535 - accuracy: 0.9673\n",
            "Epoch 199/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1418 - accuracy: 0.9628\n",
            "Epoch 200/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1526 - accuracy: 0.9658\n",
            "Epoch 201/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0845 - accuracy: 0.9777\n",
            "Epoch 202/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0803 - accuracy: 0.9807\n",
            "Epoch 203/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1095 - accuracy: 0.9792\n",
            "Epoch 204/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1691 - accuracy: 0.9643\n",
            "Epoch 205/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0395 - accuracy: 0.9896\n",
            "Epoch 206/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1225 - accuracy: 0.9732\n",
            "Epoch 207/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0918 - accuracy: 0.9762\n",
            "Epoch 208/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0733 - accuracy: 0.9762\n",
            "Epoch 209/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0515 - accuracy: 0.9821\n",
            "Epoch 210/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0664 - accuracy: 0.9836\n",
            "Epoch 211/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0763 - accuracy: 0.9792\n",
            "Epoch 212/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1056 - accuracy: 0.9807\n",
            "Epoch 213/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0625 - accuracy: 0.9777\n",
            "Epoch 214/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1643 - accuracy: 0.9628\n",
            "Epoch 215/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1737 - accuracy: 0.9583\n",
            "Epoch 216/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2522 - accuracy: 0.9479\n",
            "Epoch 217/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1748 - accuracy: 0.9628\n",
            "Epoch 218/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1339 - accuracy: 0.9673\n",
            "Epoch 219/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1103 - accuracy: 0.9688\n",
            "Epoch 220/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1254 - accuracy: 0.9747\n",
            "Epoch 221/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0832 - accuracy: 0.9821\n",
            "Epoch 222/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1058 - accuracy: 0.9732\n",
            "Epoch 223/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1030 - accuracy: 0.9747\n",
            "Epoch 224/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0619 - accuracy: 0.9792\n",
            "Epoch 225/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0442 - accuracy: 0.9896\n",
            "Epoch 226/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0503 - accuracy: 0.9851\n",
            "Epoch 227/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1423 - accuracy: 0.9658\n",
            "Epoch 228/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0732 - accuracy: 0.9792\n",
            "Epoch 229/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0900 - accuracy: 0.9732\n",
            "Epoch 230/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0906 - accuracy: 0.9777\n",
            "Epoch 231/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0530 - accuracy: 0.9866\n",
            "Epoch 232/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0521 - accuracy: 0.9836\n",
            "Epoch 233/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0680 - accuracy: 0.9836\n",
            "Epoch 234/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1389 - accuracy: 0.9732\n",
            "Epoch 235/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1186 - accuracy: 0.9732\n",
            "Epoch 236/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1101 - accuracy: 0.9747\n",
            "Epoch 237/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1171 - accuracy: 0.9702\n",
            "Epoch 238/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1029 - accuracy: 0.9777\n",
            "Epoch 239/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1043 - accuracy: 0.9717\n",
            "Epoch 240/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0979 - accuracy: 0.9717\n",
            "Epoch 241/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1989 - accuracy: 0.9673\n",
            "Epoch 242/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2765 - accuracy: 0.9435\n",
            "Epoch 243/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0798 - accuracy: 0.9762\n",
            "Epoch 244/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0456 - accuracy: 0.9911\n",
            "Epoch 245/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1113 - accuracy: 0.9762\n",
            "Epoch 246/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0347 - accuracy: 0.9881\n",
            "Epoch 247/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1353 - accuracy: 0.9777\n",
            "Epoch 248/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0670 - accuracy: 0.9792\n",
            "Epoch 249/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0438 - accuracy: 0.9866\n",
            "Epoch 250/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0659 - accuracy: 0.9807\n",
            "Epoch 251/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0670 - accuracy: 0.9821\n",
            "Epoch 252/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0597 - accuracy: 0.9792\n",
            "Epoch 253/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0615 - accuracy: 0.9792\n",
            "Epoch 254/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0573 - accuracy: 0.9866\n",
            "Epoch 255/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0536 - accuracy: 0.9792\n",
            "Epoch 256/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0720 - accuracy: 0.9881\n",
            "Epoch 257/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0609 - accuracy: 0.9821\n",
            "Epoch 258/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0560 - accuracy: 0.9821\n",
            "Epoch 259/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0290 - accuracy: 0.9911\n",
            "Epoch 260/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0672 - accuracy: 0.9821\n",
            "Epoch 261/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0814 - accuracy: 0.9747\n",
            "Epoch 262/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1049 - accuracy: 0.9717\n",
            "Epoch 263/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1257 - accuracy: 0.9762\n",
            "Epoch 264/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.5241 - accuracy: 0.9122\n",
            "Epoch 265/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.3005 - accuracy: 0.9539\n",
            "Epoch 266/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2336 - accuracy: 0.9539\n",
            "Epoch 267/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1225 - accuracy: 0.9747\n",
            "Epoch 268/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.2375 - accuracy: 0.9613\n",
            "Epoch 269/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1118 - accuracy: 0.9762\n",
            "Epoch 270/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1218 - accuracy: 0.9747\n",
            "Epoch 271/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1185 - accuracy: 0.9762\n",
            "Epoch 272/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1109 - accuracy: 0.9762\n",
            "Epoch 273/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0835 - accuracy: 0.9821\n",
            "Epoch 274/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1276 - accuracy: 0.9762\n",
            "Epoch 275/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1422 - accuracy: 0.9792\n",
            "Epoch 276/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0615 - accuracy: 0.9821\n",
            "Epoch 277/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0643 - accuracy: 0.9821\n",
            "Epoch 278/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1012 - accuracy: 0.9807\n",
            "Epoch 279/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0766 - accuracy: 0.9762\n",
            "Epoch 280/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1143 - accuracy: 0.9777\n",
            "Epoch 281/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0980 - accuracy: 0.9762\n",
            "Epoch 282/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1250 - accuracy: 0.9643\n",
            "Epoch 283/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0922 - accuracy: 0.9792\n",
            "Epoch 284/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0793 - accuracy: 0.9777\n",
            "Epoch 285/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0824 - accuracy: 0.9732\n",
            "Epoch 286/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0460 - accuracy: 0.9866\n",
            "Epoch 287/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0873 - accuracy: 0.9777\n",
            "Epoch 288/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1148 - accuracy: 0.9821\n",
            "Epoch 289/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0741 - accuracy: 0.9821\n",
            "Epoch 290/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0817 - accuracy: 0.9807\n",
            "Epoch 291/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1032 - accuracy: 0.9777\n",
            "Epoch 292/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0355 - accuracy: 0.9926\n",
            "Epoch 293/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0513 - accuracy: 0.9866\n",
            "Epoch 294/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0779 - accuracy: 0.9807\n",
            "Epoch 295/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0562 - accuracy: 0.9851\n",
            "Epoch 296/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0739 - accuracy: 0.9896\n",
            "Epoch 297/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0331 - accuracy: 0.9866\n",
            "Epoch 298/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0260 - accuracy: 0.9940\n",
            "Epoch 299/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0718 - accuracy: 0.9807\n",
            "Epoch 300/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.0971 - accuracy: 0.9792\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 19.3335 - accuracy: 0.0179\n",
            "Model: \"sequential_155\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_465 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_775 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_465 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_466 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_776 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_466 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_467 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_777 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_467 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_155 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_465 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_778 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_466 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_779 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_467 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 4.0541 - accuracy: 0.0208\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.7100 - accuracy: 0.0729\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.0601 - accuracy: 0.2054\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.4173 - accuracy: 0.3318\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.9553 - accuracy: 0.4479\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.5905 - accuracy: 0.5432\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.2839 - accuracy: 0.6190\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.0150 - accuracy: 0.6949\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.8637 - accuracy: 0.7426\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.6956 - accuracy: 0.7872\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 7.2540 - accuracy: 0.0238\n",
            "Model: \"sequential_156\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_468 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_780 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_468 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_469 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_781 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_469 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_470 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_782 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_470 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_156 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_468 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_783 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_469 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_784 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_470 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0845 - accuracy: 0.0253\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 3.8521 - accuracy: 0.0521\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 3.2691 - accuracy: 0.1503\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 2.5731 - accuracy: 0.2753\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.8710 - accuracy: 0.4554\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.3612 - accuracy: 0.5967\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 1.0978 - accuracy: 0.6756\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.8359 - accuracy: 0.7574\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6755 - accuracy: 0.7768\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5284 - accuracy: 0.8348\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6.9489 - accuracy: 0.0714\n",
            "Model: \"sequential_157\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_471 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_785 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_471 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_472 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_786 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_472 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_473 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_787 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_473 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_157 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_471 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_788 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_472 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_789 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_473 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "34/34 [==============================] - 1s 14ms/step - loss: 4.0647 - accuracy: 0.0223\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.6121 - accuracy: 0.0952\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.8751 - accuracy: 0.2158\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 2.0977 - accuracy: 0.4330\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.5865 - accuracy: 0.5312\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.1547 - accuracy: 0.6637\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.9405 - accuracy: 0.7277\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7246 - accuracy: 0.7917\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6692 - accuracy: 0.7872\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.5285 - accuracy: 0.8452\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 5.5017 - accuracy: 0.0536\n",
            "Model: \"sequential_158\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_474 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_790 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_474 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_475 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_791 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_475 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_476 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_792 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_476 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_158 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_474 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_793 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_475 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_794 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_476 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0827 - accuracy: 0.0164\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 3.8228 - accuracy: 0.0595\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 3.2841 - accuracy: 0.1384\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 2.5855 - accuracy: 0.2932\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.8240 - accuracy: 0.4420\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 1.4046 - accuracy: 0.5878\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 1.1137 - accuracy: 0.6562\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.9948 - accuracy: 0.6905\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7649 - accuracy: 0.7440\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.7142 - accuracy: 0.7693\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6.7501 - accuracy: 0.0833\n",
            "Model: \"sequential_159\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_477 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_795 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_477 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_478 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_796 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_478 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_479 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_797 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_479 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_159 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_477 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_798 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_478 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_799 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_479 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0326 - accuracy: 0.0327\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.5719 - accuracy: 0.0997\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 2.9425 - accuracy: 0.2143\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.2815 - accuracy: 0.3765\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.7434 - accuracy: 0.4970\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.3634 - accuracy: 0.5789\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.1029 - accuracy: 0.6607\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.9083 - accuracy: 0.7083\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.7993 - accuracy: 0.7560\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7408 - accuracy: 0.7723\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6.8571 - accuracy: 0.0179\n",
            "Model: \"sequential_160\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_480 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_800 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_480 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_481 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_801 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_481 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_482 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_802 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_482 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_160 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_480 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_803 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_481 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_804 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_482 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0626 - accuracy: 0.0208\n",
            "Epoch 2/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.8818 - accuracy: 0.0491\n",
            "Epoch 3/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.3448 - accuracy: 0.1369\n",
            "Epoch 4/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.6617 - accuracy: 0.2932\n",
            "Epoch 5/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.9639 - accuracy: 0.4420\n",
            "Epoch 6/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.6349 - accuracy: 0.5208\n",
            "Epoch 7/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.2167 - accuracy: 0.6265\n",
            "Epoch 8/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.0277 - accuracy: 0.6979\n",
            "Epoch 9/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7348 - accuracy: 0.7842\n",
            "Epoch 10/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.6938 - accuracy: 0.7872\n",
            "Epoch 11/20\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5785 - accuracy: 0.8304\n",
            "Epoch 12/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5578 - accuracy: 0.8378\n",
            "Epoch 13/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4795 - accuracy: 0.8497\n",
            "Epoch 14/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5042 - accuracy: 0.8616\n",
            "Epoch 15/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4330 - accuracy: 0.8765\n",
            "Epoch 16/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3681 - accuracy: 0.8601\n",
            "Epoch 17/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.3258 - accuracy: 0.8988\n",
            "Epoch 18/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3864 - accuracy: 0.8943\n",
            "Epoch 19/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2652 - accuracy: 0.9196\n",
            "Epoch 20/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2691 - accuracy: 0.9122\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 8.6209 - accuracy: 0.0238\n",
            "Model: \"sequential_161\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_483 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_805 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_483 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_484 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_806 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_484 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_485 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_807 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_485 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_161 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_483 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_808 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_484 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_809 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_485 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "34/34 [==============================] - 1s 14ms/step - loss: 4.0845 - accuracy: 0.0253\n",
            "Epoch 2/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 3.8580 - accuracy: 0.0461\n",
            "Epoch 3/20\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 3.3003 - accuracy: 0.1429\n",
            "Epoch 4/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.7278 - accuracy: 0.2470\n",
            "Epoch 5/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.1212 - accuracy: 0.4033\n",
            "Epoch 6/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.5964 - accuracy: 0.5387\n",
            "Epoch 7/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.2297 - accuracy: 0.6562\n",
            "Epoch 8/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.9261 - accuracy: 0.7262\n",
            "Epoch 9/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7998 - accuracy: 0.7500\n",
            "Epoch 10/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6389 - accuracy: 0.8006\n",
            "Epoch 11/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4975 - accuracy: 0.8185\n",
            "Epoch 12/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4616 - accuracy: 0.8616\n",
            "Epoch 13/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.4638 - accuracy: 0.8557\n",
            "Epoch 14/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3770 - accuracy: 0.8884\n",
            "Epoch 15/20\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.2992 - accuracy: 0.9167\n",
            "Epoch 16/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2631 - accuracy: 0.9196\n",
            "Epoch 17/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2840 - accuracy: 0.9167\n",
            "Epoch 18/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2984 - accuracy: 0.9137\n",
            "Epoch 19/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2438 - accuracy: 0.9152\n",
            "Epoch 20/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2823 - accuracy: 0.9196\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6.3534 - accuracy: 0.1012\n",
            "Model: \"sequential_162\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_486 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_810 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_486 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_487 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_811 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_487 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_488 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_812 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_488 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_162 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_486 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_813 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_487 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_814 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_488 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0634 - accuracy: 0.0238\n",
            "Epoch 2/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.6516 - accuracy: 0.0759\n",
            "Epoch 3/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.9542 - accuracy: 0.2068\n",
            "Epoch 4/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.2340 - accuracy: 0.3780\n",
            "Epoch 5/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.5727 - accuracy: 0.5476\n",
            "Epoch 6/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.2349 - accuracy: 0.6310\n",
            "Epoch 7/20\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.9558 - accuracy: 0.7307\n",
            "Epoch 8/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.7274 - accuracy: 0.7842\n",
            "Epoch 9/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6402 - accuracy: 0.8036\n",
            "Epoch 10/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.5317 - accuracy: 0.8393\n",
            "Epoch 11/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5753 - accuracy: 0.8244\n",
            "Epoch 12/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4504 - accuracy: 0.8497\n",
            "Epoch 13/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4517 - accuracy: 0.8690\n",
            "Epoch 14/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3097 - accuracy: 0.9003\n",
            "Epoch 15/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3562 - accuracy: 0.8795\n",
            "Epoch 16/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3389 - accuracy: 0.8914\n",
            "Epoch 17/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2751 - accuracy: 0.9152\n",
            "Epoch 18/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3129 - accuracy: 0.9152\n",
            "Epoch 19/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2530 - accuracy: 0.9122\n",
            "Epoch 20/20\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.2750 - accuracy: 0.9226\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6.3811 - accuracy: 0.0893\n",
            "Model: \"sequential_163\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_489 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_815 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_489 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_490 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_816 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_490 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_491 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_817 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_491 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_163 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_489 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_818 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_490 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_819 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_491 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0778 - accuracy: 0.0179\n",
            "Epoch 2/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.8454 - accuracy: 0.0551\n",
            "Epoch 3/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 3.2499 - accuracy: 0.1473\n",
            "Epoch 4/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 2.3819 - accuracy: 0.3423\n",
            "Epoch 5/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.6294 - accuracy: 0.5372\n",
            "Epoch 6/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.2757 - accuracy: 0.6250\n",
            "Epoch 7/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.0356 - accuracy: 0.6905\n",
            "Epoch 8/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7265 - accuracy: 0.7693\n",
            "Epoch 9/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6366 - accuracy: 0.8021\n",
            "Epoch 10/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5676 - accuracy: 0.8199\n",
            "Epoch 11/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.4700 - accuracy: 0.8631\n",
            "Epoch 12/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.3628 - accuracy: 0.8720\n",
            "Epoch 13/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3856 - accuracy: 0.8646\n",
            "Epoch 14/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.4390 - accuracy: 0.8676\n",
            "Epoch 15/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2948 - accuracy: 0.9033\n",
            "Epoch 16/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3372 - accuracy: 0.9048\n",
            "Epoch 17/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2992 - accuracy: 0.9077\n",
            "Epoch 18/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.3135 - accuracy: 0.9152\n",
            "Epoch 19/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1926 - accuracy: 0.9435\n",
            "Epoch 20/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2380 - accuracy: 0.9241\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 8.3016 - accuracy: 0.0774\n",
            "Model: \"sequential_164\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_492 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_820 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_492 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_493 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_821 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_493 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_494 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_822 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_494 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_164 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_492 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_823 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_493 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_824 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_494 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "34/34 [==============================] - 1s 14ms/step - loss: 4.0359 - accuracy: 0.0298\n",
            "Epoch 2/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.5675 - accuracy: 0.0997\n",
            "Epoch 3/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.8428 - accuracy: 0.2336\n",
            "Epoch 4/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 2.2747 - accuracy: 0.3750\n",
            "Epoch 5/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.7476 - accuracy: 0.4940\n",
            "Epoch 6/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.4038 - accuracy: 0.5982\n",
            "Epoch 7/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.1357 - accuracy: 0.6607\n",
            "Epoch 8/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.9150 - accuracy: 0.7307\n",
            "Epoch 9/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7500 - accuracy: 0.7902\n",
            "Epoch 10/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.6787 - accuracy: 0.7872\n",
            "Epoch 11/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6248 - accuracy: 0.8051\n",
            "Epoch 12/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5614 - accuracy: 0.8155\n",
            "Epoch 13/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4543 - accuracy: 0.8661\n",
            "Epoch 14/20\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3612 - accuracy: 0.8795\n",
            "Epoch 15/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4301 - accuracy: 0.8705\n",
            "Epoch 16/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.3831 - accuracy: 0.8914\n",
            "Epoch 17/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.3258 - accuracy: 0.9077\n",
            "Epoch 18/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2811 - accuracy: 0.9033\n",
            "Epoch 19/20\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2527 - accuracy: 0.9256\n",
            "Epoch 20/20\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3073 - accuracy: 0.9033\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 8.6860 - accuracy: 0.0179\n",
            "Model: \"sequential_165\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_495 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_825 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_495 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_496 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_826 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_496 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_497 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_827 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_497 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_165 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_495 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_828 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_496 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_829 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_497 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0471 - accuracy: 0.0268\n",
            "Epoch 2/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 3.7224 - accuracy: 0.0714\n",
            "Epoch 3/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 3.1556 - accuracy: 0.1845\n",
            "Epoch 4/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.4776 - accuracy: 0.3348\n",
            "Epoch 5/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.9355 - accuracy: 0.4256\n",
            "Epoch 6/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.5175 - accuracy: 0.5402\n",
            "Epoch 7/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.2657 - accuracy: 0.6280\n",
            "Epoch 8/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.9857 - accuracy: 0.7143\n",
            "Epoch 9/30\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.8322 - accuracy: 0.7381\n",
            "Epoch 10/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6945 - accuracy: 0.7857\n",
            "Epoch 11/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.6889 - accuracy: 0.7827\n",
            "Epoch 12/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.5299 - accuracy: 0.8333\n",
            "Epoch 13/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4454 - accuracy: 0.8720\n",
            "Epoch 14/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.3785 - accuracy: 0.8810\n",
            "Epoch 15/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4190 - accuracy: 0.8735\n",
            "Epoch 16/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3804 - accuracy: 0.8705\n",
            "Epoch 17/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3097 - accuracy: 0.9048\n",
            "Epoch 18/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3118 - accuracy: 0.8914\n",
            "Epoch 19/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2985 - accuracy: 0.9122\n",
            "Epoch 20/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2645 - accuracy: 0.9286\n",
            "Epoch 21/30\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.2148 - accuracy: 0.9301\n",
            "Epoch 22/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1990 - accuracy: 0.9330\n",
            "Epoch 23/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2798 - accuracy: 0.9092\n",
            "Epoch 24/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.3118 - accuracy: 0.9048\n",
            "Epoch 25/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2360 - accuracy: 0.9226\n",
            "Epoch 26/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1758 - accuracy: 0.9494\n",
            "Epoch 27/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1278 - accuracy: 0.9509\n",
            "Epoch 28/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2261 - accuracy: 0.9286\n",
            "Epoch 29/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2221 - accuracy: 0.9360\n",
            "Epoch 30/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1225 - accuracy: 0.9613\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 7.0098 - accuracy: 0.0238\n",
            "Model: \"sequential_166\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_498 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_830 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_498 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_499 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_831 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_499 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_500 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_832 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_500 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_166 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_498 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_833 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_499 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_834 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_500 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0845 - accuracy: 0.0253\n",
            "Epoch 2/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.8545 - accuracy: 0.0521\n",
            "Epoch 3/30\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 3.2621 - accuracy: 0.1607\n",
            "Epoch 4/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.6276 - accuracy: 0.2500\n",
            "Epoch 5/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.8537 - accuracy: 0.4613\n",
            "Epoch 6/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.4454 - accuracy: 0.5625\n",
            "Epoch 7/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.0149 - accuracy: 0.7128\n",
            "Epoch 8/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.9259 - accuracy: 0.7381\n",
            "Epoch 9/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7020 - accuracy: 0.7515\n",
            "Epoch 10/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5534 - accuracy: 0.8244\n",
            "Epoch 11/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4869 - accuracy: 0.8646\n",
            "Epoch 12/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4261 - accuracy: 0.8512\n",
            "Epoch 13/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4750 - accuracy: 0.8616\n",
            "Epoch 14/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3794 - accuracy: 0.8810\n",
            "Epoch 15/30\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.3018 - accuracy: 0.8958\n",
            "Epoch 16/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2816 - accuracy: 0.9003\n",
            "Epoch 17/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3409 - accuracy: 0.9062\n",
            "Epoch 18/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2271 - accuracy: 0.9226\n",
            "Epoch 19/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2303 - accuracy: 0.9390\n",
            "Epoch 20/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2022 - accuracy: 0.9330\n",
            "Epoch 21/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2110 - accuracy: 0.9226\n",
            "Epoch 22/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2137 - accuracy: 0.9345\n",
            "Epoch 23/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1478 - accuracy: 0.9568\n",
            "Epoch 24/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1707 - accuracy: 0.9420\n",
            "Epoch 25/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1719 - accuracy: 0.9464\n",
            "Epoch 26/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1685 - accuracy: 0.9568\n",
            "Epoch 27/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2402 - accuracy: 0.9420\n",
            "Epoch 28/30\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.2088 - accuracy: 0.9479\n",
            "Epoch 29/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1683 - accuracy: 0.9464\n",
            "Epoch 30/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1262 - accuracy: 0.9554\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 8.1284 - accuracy: 0.1548\n",
            "Model: \"sequential_167\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_501 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_835 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_501 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_502 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_836 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_502 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_503 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_837 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_503 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_167 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_501 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_838 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_502 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_839 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_503 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0639 - accuracy: 0.0238\n",
            "Epoch 2/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.5961 - accuracy: 0.0818\n",
            "Epoch 3/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.7660 - accuracy: 0.2247\n",
            "Epoch 4/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 2.1075 - accuracy: 0.4211\n",
            "Epoch 5/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.5067 - accuracy: 0.5655\n",
            "Epoch 6/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.1695 - accuracy: 0.6548\n",
            "Epoch 7/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.9877 - accuracy: 0.7262\n",
            "Epoch 8/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7737 - accuracy: 0.7798\n",
            "Epoch 9/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6879 - accuracy: 0.8036\n",
            "Epoch 10/30\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5448 - accuracy: 0.8155\n",
            "Epoch 11/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4673 - accuracy: 0.8661\n",
            "Epoch 12/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4589 - accuracy: 0.8631\n",
            "Epoch 13/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4540 - accuracy: 0.8542\n",
            "Epoch 14/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3391 - accuracy: 0.8929\n",
            "Epoch 15/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3043 - accuracy: 0.8988\n",
            "Epoch 16/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3044 - accuracy: 0.9077\n",
            "Epoch 17/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2934 - accuracy: 0.9018\n",
            "Epoch 18/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2943 - accuracy: 0.9062\n",
            "Epoch 19/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2519 - accuracy: 0.9137\n",
            "Epoch 20/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2549 - accuracy: 0.9315\n",
            "Epoch 21/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2033 - accuracy: 0.9390\n",
            "Epoch 22/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1640 - accuracy: 0.9464\n",
            "Epoch 23/30\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.1873 - accuracy: 0.9435\n",
            "Epoch 24/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1255 - accuracy: 0.9643\n",
            "Epoch 25/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1823 - accuracy: 0.9509\n",
            "Epoch 26/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1988 - accuracy: 0.9375\n",
            "Epoch 27/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1469 - accuracy: 0.9568\n",
            "Epoch 28/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1652 - accuracy: 0.9613\n",
            "Epoch 29/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1867 - accuracy: 0.9554\n",
            "Epoch 30/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1768 - accuracy: 0.9449\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6.6961 - accuracy: 0.0893\n",
            "Model: \"sequential_168\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_504 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_840 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_504 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_505 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_841 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_505 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_506 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_842 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_506 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_168 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_504 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_843 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_505 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_844 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_506 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0755 - accuracy: 0.0253\n",
            "Epoch 2/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 3.7598 - accuracy: 0.0685\n",
            "Epoch 3/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 3.0605 - accuracy: 0.1964\n",
            "Epoch 4/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 2.2910 - accuracy: 0.3720\n",
            "Epoch 5/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.6151 - accuracy: 0.5342\n",
            "Epoch 6/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.1745 - accuracy: 0.6652\n",
            "Epoch 7/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.0524 - accuracy: 0.6711\n",
            "Epoch 8/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.7721 - accuracy: 0.7664\n",
            "Epoch 9/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6870 - accuracy: 0.7783\n",
            "Epoch 10/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.5656 - accuracy: 0.8259\n",
            "Epoch 11/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.4453 - accuracy: 0.8690\n",
            "Epoch 12/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4262 - accuracy: 0.8854\n",
            "Epoch 13/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4010 - accuracy: 0.8661\n",
            "Epoch 14/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3176 - accuracy: 0.8988\n",
            "Epoch 15/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2926 - accuracy: 0.9092\n",
            "Epoch 16/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3181 - accuracy: 0.9048\n",
            "Epoch 17/30\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.2237 - accuracy: 0.9241\n",
            "Epoch 18/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2868 - accuracy: 0.9211\n",
            "Epoch 19/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2078 - accuracy: 0.9345\n",
            "Epoch 20/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2476 - accuracy: 0.9241\n",
            "Epoch 21/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2870 - accuracy: 0.9018\n",
            "Epoch 22/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1837 - accuracy: 0.9420\n",
            "Epoch 23/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1761 - accuracy: 0.9405\n",
            "Epoch 24/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2503 - accuracy: 0.9256\n",
            "Epoch 25/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1476 - accuracy: 0.9539\n",
            "Epoch 26/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1918 - accuracy: 0.9405\n",
            "Epoch 27/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1835 - accuracy: 0.9449\n",
            "Epoch 28/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1650 - accuracy: 0.9449\n",
            "Epoch 29/30\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.1859 - accuracy: 0.9435\n",
            "Epoch 30/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1758 - accuracy: 0.9449\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 8.5742 - accuracy: 0.0774\n",
            "Model: \"sequential_169\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_507 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_845 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_507 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_508 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_846 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_508 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_509 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_847 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_509 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_169 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_507 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_848 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_508 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_849 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_509 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0340 - accuracy: 0.0327\n",
            "Epoch 2/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.7033 - accuracy: 0.0923\n",
            "Epoch 3/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 2.9892 - accuracy: 0.2098\n",
            "Epoch 4/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.2609 - accuracy: 0.3601\n",
            "Epoch 5/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.8274 - accuracy: 0.4777\n",
            "Epoch 6/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.3940 - accuracy: 0.5863\n",
            "Epoch 7/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.1457 - accuracy: 0.6577\n",
            "Epoch 8/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.8680 - accuracy: 0.7411\n",
            "Epoch 9/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.8131 - accuracy: 0.7560\n",
            "Epoch 10/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7062 - accuracy: 0.7798\n",
            "Epoch 11/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.5971 - accuracy: 0.8110\n",
            "Epoch 12/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.4905 - accuracy: 0.8586\n",
            "Epoch 13/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.5013 - accuracy: 0.8482\n",
            "Epoch 14/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3322 - accuracy: 0.9003\n",
            "Epoch 15/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3306 - accuracy: 0.8869\n",
            "Epoch 16/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3523 - accuracy: 0.8780\n",
            "Epoch 17/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2644 - accuracy: 0.9315\n",
            "Epoch 18/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2216 - accuracy: 0.9226\n",
            "Epoch 19/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2539 - accuracy: 0.9092\n",
            "Epoch 20/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3448 - accuracy: 0.8973\n",
            "Epoch 21/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2806 - accuracy: 0.9152\n",
            "Epoch 22/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3299 - accuracy: 0.9122\n",
            "Epoch 23/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2400 - accuracy: 0.9271\n",
            "Epoch 24/30\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.1887 - accuracy: 0.9345\n",
            "Epoch 25/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2012 - accuracy: 0.9479\n",
            "Epoch 26/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2326 - accuracy: 0.9256\n",
            "Epoch 27/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1873 - accuracy: 0.9479\n",
            "Epoch 28/30\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1976 - accuracy: 0.9464\n",
            "Epoch 29/30\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1768 - accuracy: 0.9479\n",
            "Epoch 30/30\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2016 - accuracy: 0.9286\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 10.1686 - accuracy: 0.0179\n",
            "Model: \"sequential_170\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_510 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_850 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_510 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_511 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_851 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_511 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_512 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_852 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_512 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_170 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_510 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_853 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_511 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_854 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_512 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0572 - accuracy: 0.0223\n",
            "Epoch 2/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.8474 - accuracy: 0.0506\n",
            "Epoch 3/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.3569 - accuracy: 0.1429\n",
            "Epoch 4/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.8258 - accuracy: 0.2381\n",
            "Epoch 5/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.1596 - accuracy: 0.3839\n",
            "Epoch 6/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 1.7224 - accuracy: 0.4881\n",
            "Epoch 7/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.3464 - accuracy: 0.6027\n",
            "Epoch 8/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 1.0599 - accuracy: 0.6771\n",
            "Epoch 9/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.9141 - accuracy: 0.7158\n",
            "Epoch 10/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.8099 - accuracy: 0.7247\n",
            "Epoch 11/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.6467 - accuracy: 0.7932\n",
            "Epoch 12/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5653 - accuracy: 0.8304\n",
            "Epoch 13/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4860 - accuracy: 0.8676\n",
            "Epoch 14/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4309 - accuracy: 0.8601\n",
            "Epoch 15/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.5082 - accuracy: 0.8378\n",
            "Epoch 16/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4155 - accuracy: 0.8676\n",
            "Epoch 17/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.3721 - accuracy: 0.8705\n",
            "Epoch 18/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2730 - accuracy: 0.9033\n",
            "Epoch 19/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2985 - accuracy: 0.9137\n",
            "Epoch 20/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2284 - accuracy: 0.9390\n",
            "Epoch 21/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2505 - accuracy: 0.9226\n",
            "Epoch 22/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2416 - accuracy: 0.9286\n",
            "Epoch 23/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2485 - accuracy: 0.9256\n",
            "Epoch 24/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2205 - accuracy: 0.9345\n",
            "Epoch 25/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2164 - accuracy: 0.9330\n",
            "Epoch 26/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1547 - accuracy: 0.9494\n",
            "Epoch 27/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1578 - accuracy: 0.9405\n",
            "Epoch 28/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2125 - accuracy: 0.9420\n",
            "Epoch 29/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1681 - accuracy: 0.9435\n",
            "Epoch 30/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.1691 - accuracy: 0.9554\n",
            "Epoch 31/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1417 - accuracy: 0.9583\n",
            "Epoch 32/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1410 - accuracy: 0.9539\n",
            "Epoch 33/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2426 - accuracy: 0.9345\n",
            "Epoch 34/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1903 - accuracy: 0.9479\n",
            "Epoch 35/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1622 - accuracy: 0.9494\n",
            "Epoch 36/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1658 - accuracy: 0.9509\n",
            "Epoch 37/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1442 - accuracy: 0.9598\n",
            "Epoch 38/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1588 - accuracy: 0.9554\n",
            "Epoch 39/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1232 - accuracy: 0.9539\n",
            "Epoch 40/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1329 - accuracy: 0.9554\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 8.4609 - accuracy: 0.0179\n",
            "Model: \"sequential_171\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_513 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_855 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_513 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_514 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_856 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_514 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_515 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_857 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_515 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_171 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_513 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_858 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_514 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_859 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_515 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "34/34 [==============================] - 1s 14ms/step - loss: 4.0826 - accuracy: 0.0164\n",
            "Epoch 2/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 3.8310 - accuracy: 0.0580\n",
            "Epoch 3/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 3.2084 - accuracy: 0.1473\n",
            "Epoch 4/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.6792 - accuracy: 0.2589\n",
            "Epoch 5/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 2.2162 - accuracy: 0.3631\n",
            "Epoch 6/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.7638 - accuracy: 0.4940\n",
            "Epoch 7/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.2950 - accuracy: 0.6250\n",
            "Epoch 8/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.8991 - accuracy: 0.7292\n",
            "Epoch 9/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.7630 - accuracy: 0.7619\n",
            "Epoch 10/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6228 - accuracy: 0.7917\n",
            "Epoch 11/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4592 - accuracy: 0.8542\n",
            "Epoch 12/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.5701 - accuracy: 0.8140\n",
            "Epoch 13/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.4745 - accuracy: 0.8423\n",
            "Epoch 14/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3062 - accuracy: 0.8958\n",
            "Epoch 15/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.2347 - accuracy: 0.9226\n",
            "Epoch 16/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2880 - accuracy: 0.9092\n",
            "Epoch 17/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3371 - accuracy: 0.8914\n",
            "Epoch 18/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2874 - accuracy: 0.9033\n",
            "Epoch 19/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2567 - accuracy: 0.9167\n",
            "Epoch 20/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2102 - accuracy: 0.9345\n",
            "Epoch 21/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1913 - accuracy: 0.9345\n",
            "Epoch 22/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1915 - accuracy: 0.9345\n",
            "Epoch 23/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1593 - accuracy: 0.9464\n",
            "Epoch 24/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2358 - accuracy: 0.9271\n",
            "Epoch 25/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1447 - accuracy: 0.9598\n",
            "Epoch 26/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1382 - accuracy: 0.9598\n",
            "Epoch 27/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2568 - accuracy: 0.9241\n",
            "Epoch 28/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.1767 - accuracy: 0.9464\n",
            "Epoch 29/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1746 - accuracy: 0.9405\n",
            "Epoch 30/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1540 - accuracy: 0.9509\n",
            "Epoch 31/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1407 - accuracy: 0.9524\n",
            "Epoch 32/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1561 - accuracy: 0.9539\n",
            "Epoch 33/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1507 - accuracy: 0.9628\n",
            "Epoch 34/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1374 - accuracy: 0.9494\n",
            "Epoch 35/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2207 - accuracy: 0.9405\n",
            "Epoch 36/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1603 - accuracy: 0.9539\n",
            "Epoch 37/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1370 - accuracy: 0.9554\n",
            "Epoch 38/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1232 - accuracy: 0.9568\n",
            "Epoch 39/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1116 - accuracy: 0.9628\n",
            "Epoch 40/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.0751 - accuracy: 0.9702\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 7.8428 - accuracy: 0.1310\n",
            "Model: \"sequential_172\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_516 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_860 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_516 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_517 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_861 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_517 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_518 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_862 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_518 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_172 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_516 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_863 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_517 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_864 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_518 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "34/34 [==============================] - 1s 14ms/step - loss: 4.0667 - accuracy: 0.0208\n",
            "Epoch 2/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.6050 - accuracy: 0.0923\n",
            "Epoch 3/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.8433 - accuracy: 0.2455\n",
            "Epoch 4/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.1774 - accuracy: 0.4033\n",
            "Epoch 5/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.5073 - accuracy: 0.5699\n",
            "Epoch 6/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.2315 - accuracy: 0.6369\n",
            "Epoch 7/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.9296 - accuracy: 0.7098\n",
            "Epoch 8/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.6783 - accuracy: 0.7946\n",
            "Epoch 9/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6654 - accuracy: 0.8185\n",
            "Epoch 10/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6170 - accuracy: 0.7976\n",
            "Epoch 11/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5456 - accuracy: 0.8289\n",
            "Epoch 12/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.4219 - accuracy: 0.8750\n",
            "Epoch 13/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3903 - accuracy: 0.8735\n",
            "Epoch 14/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3117 - accuracy: 0.9018\n",
            "Epoch 15/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3715 - accuracy: 0.8854\n",
            "Epoch 16/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3179 - accuracy: 0.9092\n",
            "Epoch 17/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2602 - accuracy: 0.9301\n",
            "Epoch 18/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2547 - accuracy: 0.9152\n",
            "Epoch 19/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2493 - accuracy: 0.9256\n",
            "Epoch 20/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2540 - accuracy: 0.9241\n",
            "Epoch 21/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2239 - accuracy: 0.9301\n",
            "Epoch 22/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2428 - accuracy: 0.9286\n",
            "Epoch 23/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1767 - accuracy: 0.9420\n",
            "Epoch 24/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1833 - accuracy: 0.9330\n",
            "Epoch 25/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.1546 - accuracy: 0.9554\n",
            "Epoch 26/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1861 - accuracy: 0.9449\n",
            "Epoch 27/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.0830 - accuracy: 0.9732\n",
            "Epoch 28/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.0691 - accuracy: 0.9732\n",
            "Epoch 29/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1689 - accuracy: 0.9494\n",
            "Epoch 30/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1683 - accuracy: 0.9494\n",
            "Epoch 31/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1488 - accuracy: 0.9524\n",
            "Epoch 32/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.0933 - accuracy: 0.9583\n",
            "Epoch 33/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1191 - accuracy: 0.9658\n",
            "Epoch 34/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1464 - accuracy: 0.9509\n",
            "Epoch 35/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1290 - accuracy: 0.9554\n",
            "Epoch 36/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1037 - accuracy: 0.9628\n",
            "Epoch 37/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.0906 - accuracy: 0.9688\n",
            "Epoch 38/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.0851 - accuracy: 0.9807\n",
            "Epoch 39/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.0852 - accuracy: 0.9821\n",
            "Epoch 40/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.0892 - accuracy: 0.9732\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 7.4110 - accuracy: 0.1071\n",
            "Model: \"sequential_173\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_519 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_865 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_519 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_520 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_866 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_520 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_521 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_867 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_521 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_173 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_519 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_868 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_520 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_869 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_521 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "34/34 [==============================] - 1s 14ms/step - loss: 4.0753 - accuracy: 0.0253\n",
            "Epoch 2/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.7136 - accuracy: 0.0818\n",
            "Epoch 3/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.9930 - accuracy: 0.2143\n",
            "Epoch 4/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 2.2120 - accuracy: 0.3527\n",
            "Epoch 5/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.5544 - accuracy: 0.5268\n",
            "Epoch 6/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.2208 - accuracy: 0.6458\n",
            "Epoch 7/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.9635 - accuracy: 0.6890\n",
            "Epoch 8/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7570 - accuracy: 0.7753\n",
            "Epoch 9/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6274 - accuracy: 0.7976\n",
            "Epoch 10/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5285 - accuracy: 0.8185\n",
            "Epoch 11/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3974 - accuracy: 0.8750\n",
            "Epoch 12/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4462 - accuracy: 0.8780\n",
            "Epoch 13/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3677 - accuracy: 0.8884\n",
            "Epoch 14/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2822 - accuracy: 0.9107\n",
            "Epoch 15/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3036 - accuracy: 0.8914\n",
            "Epoch 16/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2878 - accuracy: 0.9122\n",
            "Epoch 17/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2749 - accuracy: 0.9182\n",
            "Epoch 18/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2388 - accuracy: 0.9271\n",
            "Epoch 19/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2043 - accuracy: 0.9256\n",
            "Epoch 20/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2481 - accuracy: 0.9286\n",
            "Epoch 21/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2802 - accuracy: 0.9152\n",
            "Epoch 22/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.1757 - accuracy: 0.9405\n",
            "Epoch 23/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1387 - accuracy: 0.9524\n",
            "Epoch 24/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2150 - accuracy: 0.9286\n",
            "Epoch 25/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1544 - accuracy: 0.9628\n",
            "Epoch 26/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1526 - accuracy: 0.9494\n",
            "Epoch 27/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1829 - accuracy: 0.9420\n",
            "Epoch 28/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1872 - accuracy: 0.9435\n",
            "Epoch 29/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2533 - accuracy: 0.9196\n",
            "Epoch 30/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2426 - accuracy: 0.9271\n",
            "Epoch 31/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1570 - accuracy: 0.9509\n",
            "Epoch 32/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2003 - accuracy: 0.9420\n",
            "Epoch 33/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2367 - accuracy: 0.9449\n",
            "Epoch 34/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.1381 - accuracy: 0.9583\n",
            "Epoch 35/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1450 - accuracy: 0.9598\n",
            "Epoch 36/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1147 - accuracy: 0.9717\n",
            "Epoch 37/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1314 - accuracy: 0.9613\n",
            "Epoch 38/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.0910 - accuracy: 0.9717\n",
            "Epoch 39/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1124 - accuracy: 0.9643\n",
            "Epoch 40/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1004 - accuracy: 0.9643\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 8.3246 - accuracy: 0.1190\n",
            "Model: \"sequential_174\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_522 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_870 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_522 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_523 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_871 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_523 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_524 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_872 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_524 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_174 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_522 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_873 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_523 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_874 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_524 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0303 - accuracy: 0.0283\n",
            "Epoch 2/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.5666 - accuracy: 0.1131\n",
            "Epoch 3/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 2.9214 - accuracy: 0.2143\n",
            "Epoch 4/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 2.1991 - accuracy: 0.3943\n",
            "Epoch 5/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.8085 - accuracy: 0.4583\n",
            "Epoch 6/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 1.4915 - accuracy: 0.5685\n",
            "Epoch 7/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.2100 - accuracy: 0.6518\n",
            "Epoch 8/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.9634 - accuracy: 0.7083\n",
            "Epoch 9/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.7917 - accuracy: 0.7574\n",
            "Epoch 10/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.7548 - accuracy: 0.7589\n",
            "Epoch 11/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.6456 - accuracy: 0.8185\n",
            "Epoch 12/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4939 - accuracy: 0.8542\n",
            "Epoch 13/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4968 - accuracy: 0.8482\n",
            "Epoch 14/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4689 - accuracy: 0.8661\n",
            "Epoch 15/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4997 - accuracy: 0.8393\n",
            "Epoch 16/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4333 - accuracy: 0.8631\n",
            "Epoch 17/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3464 - accuracy: 0.8824\n",
            "Epoch 18/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3001 - accuracy: 0.9003\n",
            "Epoch 19/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.2804 - accuracy: 0.9033\n",
            "Epoch 20/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2696 - accuracy: 0.9077\n",
            "Epoch 21/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3098 - accuracy: 0.8973\n",
            "Epoch 22/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2665 - accuracy: 0.9211\n",
            "Epoch 23/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2354 - accuracy: 0.9271\n",
            "Epoch 24/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2138 - accuracy: 0.9345\n",
            "Epoch 25/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2244 - accuracy: 0.9196\n",
            "Epoch 26/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2269 - accuracy: 0.9196\n",
            "Epoch 27/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1662 - accuracy: 0.9405\n",
            "Epoch 28/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1855 - accuracy: 0.9405\n",
            "Epoch 29/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2559 - accuracy: 0.9226\n",
            "Epoch 30/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1966 - accuracy: 0.9345\n",
            "Epoch 31/40\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.1420 - accuracy: 0.9509\n",
            "Epoch 32/40\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2385 - accuracy: 0.9271\n",
            "Epoch 33/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1991 - accuracy: 0.9464\n",
            "Epoch 34/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2054 - accuracy: 0.9435\n",
            "Epoch 35/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1671 - accuracy: 0.9494\n",
            "Epoch 36/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1588 - accuracy: 0.9479\n",
            "Epoch 37/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1704 - accuracy: 0.9360\n",
            "Epoch 38/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2406 - accuracy: 0.9345\n",
            "Epoch 39/40\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1724 - accuracy: 0.9509\n",
            "Epoch 40/40\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1531 - accuracy: 0.9568\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 9.1024 - accuracy: 0.0179\n",
            "Model: \"sequential_175\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_525 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_875 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_525 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_526 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_876 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_526 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_527 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_877 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_527 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_175 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_525 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_878 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_526 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_879 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_527 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0642 - accuracy: 0.0193\n",
            "Epoch 2/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 3.9278 - accuracy: 0.0446\n",
            "Epoch 3/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 3.4995 - accuracy: 0.1116\n",
            "Epoch 4/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 2.8728 - accuracy: 0.2054\n",
            "Epoch 5/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.2813 - accuracy: 0.3616\n",
            "Epoch 6/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 1.6495 - accuracy: 0.5119\n",
            "Epoch 7/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.1617 - accuracy: 0.6339\n",
            "Epoch 8/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.0292 - accuracy: 0.6771\n",
            "Epoch 9/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.8542 - accuracy: 0.7396\n",
            "Epoch 10/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.6337 - accuracy: 0.7946\n",
            "Epoch 11/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.5504 - accuracy: 0.8125\n",
            "Epoch 12/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4153 - accuracy: 0.8631\n",
            "Epoch 13/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.4457 - accuracy: 0.8557\n",
            "Epoch 14/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3560 - accuracy: 0.8884\n",
            "Epoch 15/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3521 - accuracy: 0.8810\n",
            "Epoch 16/50\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.3294 - accuracy: 0.8943\n",
            "Epoch 17/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2956 - accuracy: 0.9048\n",
            "Epoch 18/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3282 - accuracy: 0.9003\n",
            "Epoch 19/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3321 - accuracy: 0.8914\n",
            "Epoch 20/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.3163 - accuracy: 0.9107\n",
            "Epoch 21/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2604 - accuracy: 0.9226\n",
            "Epoch 22/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1692 - accuracy: 0.9405\n",
            "Epoch 23/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.2006 - accuracy: 0.9271\n",
            "Epoch 24/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2170 - accuracy: 0.9390\n",
            "Epoch 25/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1376 - accuracy: 0.9494\n",
            "Epoch 26/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1511 - accuracy: 0.9554\n",
            "Epoch 27/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1563 - accuracy: 0.9509\n",
            "Epoch 28/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.0918 - accuracy: 0.9747\n",
            "Epoch 29/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1190 - accuracy: 0.9628\n",
            "Epoch 30/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1272 - accuracy: 0.9613\n",
            "Epoch 31/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1369 - accuracy: 0.9464\n",
            "Epoch 32/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.1153 - accuracy: 0.9613\n",
            "Epoch 33/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1380 - accuracy: 0.9613\n",
            "Epoch 34/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2339 - accuracy: 0.9345\n",
            "Epoch 35/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2250 - accuracy: 0.9390\n",
            "Epoch 36/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2243 - accuracy: 0.9226\n",
            "Epoch 37/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1143 - accuracy: 0.9628\n",
            "Epoch 38/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1564 - accuracy: 0.9554\n",
            "Epoch 39/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1544 - accuracy: 0.9509\n",
            "Epoch 40/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1486 - accuracy: 0.9673\n",
            "Epoch 41/50\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.1136 - accuracy: 0.9643\n",
            "Epoch 42/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1504 - accuracy: 0.9449\n",
            "Epoch 43/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1149 - accuracy: 0.9568\n",
            "Epoch 44/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1403 - accuracy: 0.9554\n",
            "Epoch 45/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1005 - accuracy: 0.9762\n",
            "Epoch 46/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1163 - accuracy: 0.9643\n",
            "Epoch 47/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.0703 - accuracy: 0.9747\n",
            "Epoch 48/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.0923 - accuracy: 0.9702\n",
            "Epoch 49/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1369 - accuracy: 0.9658\n",
            "Epoch 50/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1069 - accuracy: 0.9658\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 9.2605 - accuracy: 0.0238\n",
            "Model: \"sequential_176\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_528 (Conv2D)          (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout_880 (Dropout)        (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_528 (MaxPoolin (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_529 (Conv2D)          (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_881 (Dropout)        (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_529 (MaxPoolin (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_530 (Conv2D)          (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_882 (Dropout)        (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_530 (MaxPoolin (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_176 (Flatten)        (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_528 (Dense)            (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_883 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_529 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_884 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_530 (Dense)            (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 4.0836 - accuracy: 0.0193\n",
            "Epoch 2/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 3.8478 - accuracy: 0.0521\n",
            "Epoch 3/50\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 3.2544 - accuracy: 0.1622\n",
            "Epoch 4/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 2.5220 - accuracy: 0.3036\n",
            "Epoch 5/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 1.7902 - accuracy: 0.4747\n",
            "Epoch 6/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.3504 - accuracy: 0.5982\n",
            "Epoch 7/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 1.0307 - accuracy: 0.6920\n",
            "Epoch 8/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.8991 - accuracy: 0.7277\n",
            "Epoch 9/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.6413 - accuracy: 0.7932\n",
            "Epoch 10/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.5633 - accuracy: 0.8452\n",
            "Epoch 11/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.5113 - accuracy: 0.8408\n",
            "Epoch 12/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4837 - accuracy: 0.8571\n",
            "Epoch 13/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.4787 - accuracy: 0.8438\n",
            "Epoch 14/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.4127 - accuracy: 0.8631\n",
            "Epoch 15/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.3653 - accuracy: 0.8795\n",
            "Epoch 16/50\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.2845 - accuracy: 0.9003\n",
            "Epoch 17/50\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.3896 - accuracy: 0.8914\n",
            "Epoch 18/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2384 - accuracy: 0.9211\n",
            "Epoch 19/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2024 - accuracy: 0.9301\n",
            "Epoch 20/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.2209 - accuracy: 0.9286\n",
            "Epoch 21/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2837 - accuracy: 0.9182\n",
            "Epoch 22/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.1696 - accuracy: 0.9479\n",
            "Epoch 23/50\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1500 - accuracy: 0.9494\n",
            "Epoch 24/50\n",
            "34/34 [==============================] - 0s 15ms/step - loss: 0.2204 - accuracy: 0.9256\n",
            "Epoch 25/50\n",
            " 5/34 [===>..........................] - ETA: 0s - loss: 0.0732 - accuracy: 0.9800"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-fcfc2cbb3e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m310\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0d9nvdE48Zi",
        "outputId": "ade4b356-a4c6-43c4-c115-c4c4cad7aa31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "id": "N0d9nvdE48Zi",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c9cd1e2aba91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'grid_result' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvSJeo3EThvi"
      },
      "source": [
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_cnn_model)\n",
        "\n",
        "# define the grid search parameters\n",
        "batch_size = np.arange(10,310,10)\n",
        "epochs = np.arange(10,210,10)\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5).fit(X_train[:,:,:,None],y_train)"
      ],
      "id": "WvSJeo3EThvi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viRlsm4bV-lE"
      },
      "source": [
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "id": "viRlsm4bV-lE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_EPqJGnS7ng"
      },
      "source": [
        "## Train model"
      ],
      "id": "N_EPqJGnS7ng"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngmfDNcnGMC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2daac0a2-c88e-467c-a236-dc4a55c3be81"
      },
      "source": [
        "# Train model\n",
        "start = time()\n",
        "\n",
        "model_cnn = create_cnn_model(X_train)\n",
        "history = model_cnn.fit(X_train[:,:,:,None], y_train, epochs=100, validation_data=(X_test[:,:,:,None],y_test),batch_size=80,verbose=0)\n",
        "\n",
        "end = time()\n",
        "print(f'time: {end-start} sec')"
      ],
      "id": "ngmfDNcnGMC7",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 683, 9, 128)       3328      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 683, 9, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 341, 4, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 341, 4, 64)        204864    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 341, 4, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 170, 2, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 170, 2, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 85, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 56)                3640      \n",
            "=================================================================\n",
            "Total params: 1,019,000\n",
            "Trainable params: 1,019,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 75.50107884407043 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_i70WoqTAxI"
      },
      "source": [
        "## Evaluate and predict test set"
      ],
      "id": "y_i70WoqTAxI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ4n8wAmB9ZX",
        "outputId": "43d00ab2-a3bf-43ea-d43a-eec734a3fb2a"
      },
      "source": [
        "# Evaluate model\n",
        "print(f'Test Accuracy: {model_cnn.evaluate(X_test[:,:,:,None],y_test,verbose=0)[1]}')"
      ],
      "id": "ZQ4n8wAmB9ZX",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9642857313156128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F62fEQaGFMqs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "720173f8-2dd4-4b4b-aaea-4584e6a376f7"
      },
      "source": [
        "# Learning curve\n",
        "# plot the accuracy and loss plots between training and validation data\n",
        "# verify overfitting or underfit \n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "x = range(1,len(acc)+1)\n",
        "\n",
        "plt.plot(x,acc,'b',label='Training accuracy')\n",
        "plt.plot(x,val_acc,'r',label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(x,loss,'b',label='Training loss')\n",
        "plt.plot(x,val_loss,'r',label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "F62fEQaGFMqs",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUVfrA8e9LQgsgHaUKKtJEWrAAVlDBAqKIYANZG4qKjXWt2HYt7FrWtvhTqkqxsKiAKygoIiU0pUoxSmhGkE7qvL8/zk0yCZNkUiZl5v08T56ZufXcuZP73lPuOaKqGGOMiVwVSjsBxhhjSpcFAmOMiXAWCIwxJsJZIDDGmAhngcAYYyKcBQJjjIlwFgjMMURktogMKe5lS5OIxItIrxBsV0XkFO/92yLyeDDLFmI/14vI/wqbTmPyIvYcQXgQkUN+H2OAZCDd+3y7qr5f8qkqO0QkHrhFVecW83YVaKmqm4trWRFpDvwCVFTVtOJIpzF5iS7tBJjioarVM97nddETkWi7uJiywn6PZYMVDYU5ETlfRBJE5K8isgsYJyK1ReRzEUkUkT+990381pkvIrd474eKyEIRGeMt+4uI9Cnksi1E5FsROSgic0XkDRGZnEu6g0njMyLyvbe9/4lIPb/5N4rIryKyR0QezeP7OVNEdolIlN+0/iLyo/f+DBH5QUT2ichOEXldRCrlsq3xIvKs3+eHvHV2iMiwHMteJiIrReSAiGwTkdF+s7/1XveJyCEROTvju/Vbv5uILBOR/d5rt2C/mwJ+z3VEZJx3DH+KyAy/ef1EZJV3DFtEpLc3PVsxnIiMzjjPItLcKyL7i4j8BnztTZ/unYf93m+knd/6VUXkn9753O/9xqqKyBcicneO4/lRRPoHOlaTOwsEkeEEoA5wInAb7ryP8z43A44Cr+ex/pnARqAe8CLwrohIIZb9AFgK1AVGAzfmsc9g0ngdcDPQAKgEPAggIm2Bt7ztN/L214QAVHUJcBi4MMd2P/DepwP3ecdzNtATuDOPdOOlobeXnouAlkDO+onDwE1ALeAyYLiIXOnNO9d7raWq1VX1hxzbrgN8AbzmHdu/gC9EpG6OYzjmuwkgv+95Eq6osZ23rZe9NJwBTAQe8o7hXCA+t+8jgPOANsAl3ufZuO+pAbAC8C/KHAN0AbrhfsejAB8wAbghYyER6QA0xn03piBU1f7C7A/3D9nLe38+kAJUyWP5jsCffp/n44qWAIYCm/3mxQAKnFCQZXEXmTQgxm/+ZGBykMcUKI2P+X2+E5jjvX8CmOI3r5r3HfTKZdvPAu9572vgLtIn5rLsSOBTv88KnOK9Hw88671/D3jeb7lT/ZcNsN1XgJe99829ZaP95g8FFnrvbwSW5lj/B2Boft9NQb5noCHugls7wHL/yUhvXr8/7/PojPPsd2wn5ZGGWt4yNXGB6ijQIcByVYA/cfUu4ALGmyX9/xYOf5YjiAyJqpqU8UFEYkTkP15W+wCuKKKWf/FIDrsy3qjqEe9t9QIu2wjY6zcNYFtuCQ4yjbv83h/xS1Mj/22r6mFgT277wt39XyUilYGrgBWq+quXjlO94pJdXjr+jssd5CdbGoBfcxzfmSLyjVcksx+4I8jtZmz71xzTfsXdDWfI7bvJJp/vuSnunP0ZYNWmwJYg0xtI5ncjIlEi8rxXvHSArJxFPe+vSqB9eb/pqcANIlIBGIzLwZgCskAQGXI2DXsAaAWcqarHkVUUkVtxT3HYCdQRkRi/aU3zWL4oadzpv21vn3VzW1hV1+EupH3IXiwErohpA+6u8zjgkcKkAZcj8vcBMBNoqqo1gbf9tptfU74duKIcf82A7UGkK6e8vudtuHNWK8B624CTc9nmYVxuMMMJAZbxP8brgH644rOauFxDRhr+AJLy2NcE4Hpckd0RzVGMZoJjgSAy1cBlt/d55c1PhnqH3h12HDBaRCqJyNnAFSFK40fA5SLSw6vYfZr8f+sfAPfiLoTTc6TjAHBIRFoDw4NMwzRgqIi09QJRzvTXwN1tJ3nl7df5zUvEFcmclMu2ZwGnish1IhItItcCbYHPg0xbznQE/J5VdSeu7P5Nr1K5oohkBIp3gZtFpKeIVBCRxt73A7AKGOQtHwsMCCINybhcWwwu15WRBh+umO1fItLIyz2c7eXe8C78PuCfWG6g0CwQRKZXgKq4u63FwJwS2u/1uArXPbhy+am4C0AghU6jqq4F7sJd3HfiypET8lntQ1wF5teq+off9AdxF+mDwDtemoNJw2zvGL4GNnuv/u4EnhaRg7g6jWl+6x4BngO+F9da6awc294DXI67m9+Dqzy9PEe6g5Xf93wjkIrLFf2OqyNBVZfiKqNfBvYDC8jKpTyOu4P/E3iK7DmsQCbicmTbgXVeOvw9CPwELAP2Ai+Q/do1EWiPq3MyhWAPlJlSIyJTgQ2qGvIciQlfInITcJuq9ijttJRXliMwJUZEuorIyV5RQm9cufCM/NYzJjdesdudwNjSTkt5ZoHAlKQTcE0bD+HawA9X1ZWlmiJTbonIJbj6lN3kX/xk8mBFQ8YYE+EsR2CMMRGu3HU6V69ePW3evHlpJ8MYY8qV5cuX/6Gq9QPNK3eBoHnz5sTFxZV2MowxplwRkZxPo2eyoiFjjIlwIQsEIvKeiPwuImtymS8i8pqIbPa6ju0cqrQYY4zJXShzBOOB3nnM74PrdrYlrmvkt0KYFmOMMbkIWSBQ1W9xj4Pnph8wUZ3FuB4PG4YqPcYYYwIrzTqCxmTvpjeB7N3oZhKR20QkTkTiEhMTSyRxxhgTKcpFZbGqjlXVWFWNrV8/YOsnY4wxhVSagWA72ftrb0Lh+lM3xhhTBKUZCGYCN3mth84C9nv9nxtjwkBaGrzzDiTk1wG4IZiefnbvhvT00Ow/ZA+UiciHuPFy64lIAm7Ai4oAqvo2bnCNS3F9tR/B9W1uTOEsXgyrV8Nf/gLR5eg5yYQE+PRTuOUWqFo1tPtKToYPPoD167OmdesGV16ZfblVq+Dbb+Gmm6BWoMHJ8peeDkOGuN01bAiffw6dc2sg/v777txl6NQJrrmG7bujSU2FYDsSmDbN/V1+OQwcCDEx+a+TYdcumDwZGjSAc0/by4nff4C0bgW9eoFkDUiXmgorVsDChfD993DiifDkk4X+mgAYOxYefBAGD4ZRo+DkHGOxHT4M//wnvPgivPqq+4kXu9IeNLmgf126dFFjMqWkqD72mGqFCqqg2q2b6tatuS/v8xVs+0ePqv7+u/tLTCz4+nnx+VQvuMClu1071VWrAi62f7/qmDGqe/cWcj8HDrgNNGrk9lW5smrVqqqVKrnPL72kaWmqqamq+s03qtWru+k1aqiOGqW6Y0eBdpeelKJDhrhNjByp2qyZarVqqp99lmNBn0/14Yezp6lyZVXQpEYt9IGYN7VG9BF94gl3GnKTkqJ6331uM8cdl/U6YoTqnj15p3XrVtXhw91uG5GgY7hfD1LNbQR0d9MuuuWF6TptSpoOHpy1fVBt0cL97E44QfWna59R38CBqosX5/gy0lWTkzUlRfWTT1RffFH1jz+yZk9844A+wEu6sNrFekf0O1pFknTAANWnnlJ94w3Vf/5TtWFDt7+rr1b9+eeCnInsgDjN5bpa6hf2gv5ZIChjkpJUFy4s3gtksNauVT3jDPczHjJE9b333H9qjRqqb72lumuXW87nU507V7VnT/cf//TT3lUvH9u2qTZokPWfD+7C/dtvQSXPuwbk7oMP3DZvu81dTSpVUn355WyLpKaq9unjFrv88iC/5j17VGfOVH3oIdWzzlKtWNFt4MILVb/6KnMjR/Yl6/YeA1VBX6r6uA6I+UJToqtoaqu2qv/7n+q116pWqKA+Ed1/Ynud1/pOfaLdR7rzt5Rcd3107ERNjqqij/G0PjXa7WfHDtXOnd1F8/rrVT/6SPXQgXTdd/2dqqD/bXS73jwkXdeudV/amr//V5dUOEsVdNtxbbURCdqqlep33+XYWWqq7vlymd7WeZmC6j33uO97wQLVG25QjY5WbdJE9euvs1bxpaVrwtgv9J0HN+jZZ/lURLVN9M+6oNUtml6xkvqionRj1+v1b5cs14dqv6M/c4oq6Hpa6d3V39NbhyTrtGlZsTEuTvXZpm+rgiaLC6xHz75AfU8/o0d7XqYp1WtpcsUYfTtmpDZmm4JqrePSddx9q3XFFY/rHmqrgvoaNVYF3Ve9sT5Z7SVtyUYFn4Lq2WerLpuRoPrhh6qbNwfxAwjMAoEJnaFD3c9ozJiS2+eyZapXXaUqolqrluq0aVnz4uNVzzkn68LdsqVqhw7u/QknqPbu7d537676yy9572fAAHeX+sorqq+/7m7TqlU7dp857d+v6bPm6AenPKbv1n5AD038WHX37mOW0YYNVWNjVdPSXG6jb1+XtrlzMxe7+2436bLL8via4+NVJ09Wvf12l7PIOPZKldxx/vWvqkuWaHKy6pdfupv8s8928aECaTqh4l9UQdMRXUYXbVIlUfv2dXHjslab9OmKT+uXXKQHcDmFnZWbadrLr6kePpztlEzu/oabz/Hu4vbAg5lB5+BB1TvuUK1TR7UZ8TpNXAB6gYf0tHY+jYlxSe7Tx33lrVv5NHHCF6o1aujhE1roOU22anS06ri3k1THjlXt1UvTq2bduccNeingz+TUU93P5NZbXVx7vfqozHX2RNfX+BPPUV+FCu4GYfhw1S1bsm3j161p+v29U/XAKR3dek2auGB96JBb4Kuv1BcVpb+06aNnnPqn3s8Y3Y67hV9Ha/0Pt+pkuV7TJErToyvqwS7n6sHomplp+K7elXp0wRL3PX35per552fOS6vXQA+f21t9zZtnndMi/J9ZIDB5O3rU3dkX1PTpWf8cIqozZmTN8/ncnXMwd97B8PlU581T7dXL7bNmTdVHH3VFNjmlp7ss+ksvqfbrp9q1q+rbb2eVL0ye7HIOlSu74HDCCS6f//HHWdv48ku3n2efzb7tTZuy50IOHMiat3On259XTJVKlB6lctY/cadObt+pqa4sQ0R16dKs9ZOSXHDo1UtVXewBt6jP52JfdLTqDz+ouwi/+qrqSSdlbf+441T79NGNQ5/TO9p9q2eefkQHDVJ98knNVqxRsaILBKNGqc6erZp01Oe+yyuv1I1L9+nNN6u2bu1K2fr1c0Usc+aoJh9O1fkPfqbf0d1tqHZtTe17lX545sv6FI+rgq5oeoX+MO+wWykjt/Pjj+7v++81/YabND0qWlMrVNTvLv27bt3iAkViourjj7sY26mTX9xculS1dm1Nb9hIJ5z8VOZF9tCJbfU/lUbobTWn6J5eLqjo448fk2U6dMglAVQfqP2uu0CfPUwT//GOO38dO7pAuXNn/r+/2bNVzz3XbaxuXVesVbOm6mmnucCuqhs2qL74bLKOunWvvv66i+m//67upuOuu9zB3Xqr/jRqov5j+K+Z8SSbDRtcsLvxRrftq692NyNxcUX6f7JAYHLn86n26OHy7ik5svw//+zy8Tmnq6omJLjbu65d3T9B166qMTGqy5er/ve/rkgC3B10z56qo0e7/4qMX35amurq1e4H/9lneReAz5uXdfE94QTVF17I/McrtF9+cQXYt93m/jp2dBfwCRPcBbllS/cXKECmpLiLToUK7kK8aJE7hvr1VatU0d3DHtbe0V/ptZcd1DHPJelZLNLvr3xBtW1bdwzNm6tGRanedpumpKjOn+9Ksu65R/Xd1i+qgvZrvEwrVHDFQWlpbrd//qnaodlefanmM5pet57bVo8eqv/+t+rKlbp6RZpedJGbfNJJ7u66RQsXb+rVUx02zJUY+d3IF8p996n24Ftd2f5G/S066241ecDgrN+Kz6f6yCNZQSrjLybGfe+5FK8lJQX4uf34o+rxLpextlFP7clXCj5t08bL1KWluYMD1ZtucnfNY8a4SLpkiWpKih6d/Y36oqNVL7oo8O+5IL7/3p0YcEWH8fFF214JsUBgcjd3btY/qX/59IEDWXebJ57oLjZHjrh56enurjUmRnXjRjdtxw7Vpk2zKm2bN1f9xz/cnWGHDu5qBO4CePrp7k4q50XitNPcFdH/rm7yZLdOixZuXl61hgV06JCLMaNHqw4beFAPnNFTM8vSweUKPL/95mJF/fqqbdq40qeRXRfqzirNNQ13zAdO6qCHlq7V005z8er3391X1aePK6VZEZfuck1nnqm+Jk30s/F/aMuWWYdfrZpqt9P264HoWrqk6dX66KN+GY4dO1QfekjTYlzxzIIal+ofM7IKzSdMcHf6deq40+gfv44ezQomxSE1NasEo1kz1R+mb3P1RIF28u237mbio49cbWliYuF2un276qpV6vO5CtTBg11gzJSernr//cf+pjKCT7Vq7sRlW6mI1q8vN0FA1QKByUvPnq444uKLXSXr9u1u+s03u4v6P//pypkD/YONHZt9W6tXq15xheqkScdmYfftU501y90lXnSRK7SdMMHlOubPV33mGd3bystFXHaZq+h9+20XQC64IHsRjOeuu1wZ8IcfFqyu2udTfe21zAYqKuIayjQ47qju6eGV0w8YkLl8QoLqySe72HXbbS6nft55LtPTM3afTm8wXF+IfkQrkZRZLzt7dtb+EhNVGzd2Zd9durgK025nu4rAtm1ddcNvv/kdw6OPukStX+8ucC+/7BJboYLqoEG6+D+rtFo1l2GJj3dFP+BOZX6tZIrLH3+4ZBXndbVYHD7sfisHDrjK/mnTXEVLnz7HlP9HGgsEJrClS91P4MUXXWuEypXdrdZHH7npjzyStey337orzhNPuL9x44q1pdDmzapRkq6PVH9VfZUqu8JiLyjErz+iCQnZl8+onqhf37127eoauuR353v4sGtRkhFvZs1yMerXX92FtWZMiq594N3MNn47d7pgU6PGsS0D/R054krEhg1zVRM5rV3rin4uvthlnJo2dXE0YJHv7t2qVaq4SoGLL3aJveIKVz/hWbTIBaaqVd3soUPzaaFkIp4Fgkizd29W88EePVxRTMbfo49mXTGuvtpdcDPK2594wv0kqld3rVmKWpZaACNGuKKNmjVV+7f8SdO6nKE6ZIh+MCFFq1ZVrV3bZRxU3R167dqu2iApSXX8eFdfDVll4e+95+rXHnvMbTvjr317d7P9zDPuZtvfzp1ufqVKWV9X/fquVOGYposl8YWAu9K//XbAoLtypavUffbZ0mm9a8oXCwSRwudz5egZt4kZzUOuvNL9ZZR9d+7sbl9FXGDIcOSIK4uPiXEtF0rIH3+4Xd58s2vmHhWleumlqvfeq5n1oW3auMOZONEVgcTEZH+45sgRVwpw3XXZH/qpUMHFujp13F+LFqpffJF7WvbsccU/GV/ZgAGu+LvE7dzpyr7Wry+FnZtwZIEgHO3d68oaXn7ZNZjescMVH4DqJZe4p2oyKnf9ffqpa/oGrvghZ/v2+Hh3qxkiO3a4isbJk7OmPfusS86aNe7zW29lXcjvvddlTPbuzdbE+pjqCX/Jya4UZc+eY+/6jYlUFgjCTUpK1t29/1/lyq5teX5Xv+3bVa+5xtUN5CI93ZUsXXKJexiooCZPdsUs/nffyclZ9c4iqu+/71q0HH+8q8vz9/bb2Zv1Z6x/113uuR8rCjGmYCwQhBOfz7W4AdfqZts212zmkUdce+tikJrqmmNnxJd+/YJvfvjnn654JqPVXnS06tSpbt5dd7np773n7u6jotzTnuCacRpjQscCQXmzdq17Hj/QQ1NjxugxLXqKUVKSa6wCrkueV1917x96KO/1UlJc+X2zZu4C/8wzrmimRw9XTp/RUueBB9zyBw9m5Q46dbI7fGNCzQJBeXPlle7U9OmTvX3hhAmuTGXAgJAUfu/bl9WDQ8azZT6f6p2ubzB98UX3qMD27e7Of8sW9+Dmv//tnjkDVxzk38zy8OGsFpAXXpj9cPbvdy18Fiwo9kMxxuRggaA8iY93t9CdO7vTc889bvobb2RdTYvaR0AAv/7q+iuLjnaPCPhLTXV1BYGeKcv469bN9bIQKD4lJan+3/+V3MNOxphj5RUIytEIHhHirbfc66efwiuvwMsvw9atbmSPvn1h6lSoUqVYd7lyJVx6KRw5AnPmQM+e2edHR8Nnn8EPP0Biovs7fBjq1oX69aFpU2jfPtv4HdlUrhyiwTSMMcXCAkFZcvSoG9vvyiuhWTN46SXYtMkFgeuug/HjoWLFYt2lzweDBrmL/fffw2mnBV6uYkU499xi3bUxpoywQFDSFixwQwXeccex86ZMgb17YcQI9zkqyuUAvv4a+vRxn4vZ7Nnw889uSMHcgoAxJryJKzoqP2JjYzUuLq60k1E4qtC6tbvyvvYa3H139nldukBKCvz0U+7lLMXsootcXPrll2LPbBhjyhARWa6qsYHmVSjpxES0RYtcEGjaFEaOdLfjGebMcYX1I0YUOQjs2uWKfPylp7t6gEceyZq2Zg3Mnet2aUHAmMhlgaAkvfceVK8Oy5bB6afDtdfCpEmu2OfSS6FRI7jhhiLt4quvoEkTuO++7NPHj3dx5x//gH//20179VWoWhVuvbVIuzTGlHMWCErKwYOuvP/aa+H4410znOrV4aabYPlyeO45WLvWTSukdetgwABX8fvvf8PixW764cPw+ONw9tmuHnrkSJg40cWgm25yrX+MMZHLKotLyvTp7oqc0Y6ySRNXCbx4MQwcCDExRdp8YiJcfrm7w1+0yGUybr0VVqyAf/4Tdu6Ejz6CDh3gnHNgyBC33j33FPG4jDHlngWCkvLuu66i+Kyzsqa1bu3+iigtDfr3dxf7+fOhXTt44w332MFDD8H//R9cfTV06+aWnznTJaNLF2jbtsi7N8aUcxYISsKGDe42/aWXQtIaaNIk9wzAxIlw5plu2hVXwDXXuHqA6GhXN5ChSRNXZ13BCgaNMVgdQcl47z13Nb7xxmLfdGoqPP00xMYeW8/86quuOuKBB6Bly+zzYmKK/QFlY0w5ZTmCUDt40D0t3K+fuyoXs/HjIT4e3nzz2MxGw4ZuXuXKxb5bY0wYsUAQamPHwr598Ne/Fvumk5Ph2WddeX/v3oGXsbt+Y0x+LBCEUnIy/Otfrhe3rl2LffPvvQe//eYqg0voQWRjTBiyQBBKkybBjh0wYUKxb/roUffoQY8e0KtXsW/eGBNBLBCESno6vPiia6OZs1/nYnD//bB9O7z/vuUGjDFFY4EgVD75xHUhPX16sV+pp02Dt992zwicd16xbtoYE4FC2nxURHqLyEYR2SwiDweY30xEvhGRlSLyo4hcGsr0lBhVlxto2dI96VWMNm+GW25x3UU891yxbtoYE6FCFghEJAp4A+gDtAUGi0jO51gfA6apaidgEPBmqNJTopYsgbg416lPMY4hkJzsuiqKjnZDF1iPocaY4hDKHMEZwGZV3aqqKcAUoF+OZRQ4zntfE9gRwvSUnNdfhxo1iv0BsjffdH0HjR/vBjAzxpjiEMpA0BjY5vc5wZvmbzRwg4gkALOAuwlARG4TkTgRiUtMTAxFWovP7t2uEP/mm10wKCaHD8Pzz7sWQn37FttmjTGm1LuYGAyMV9UmwKXAJBE5Jk2qOlZVY1U1tn79+iWeyAIZO9b1+3DnncW62TffhN9/h6eeKtbNGmNMSAPBdqCp3+cm3jR/fwGmAajqD0AVoF4I0xRaqamuOc/FF0OrVoXezJo1MHgwbN3qPh88CC+8AJdcktWDqDHGFJdQBoJlQEsRaSEilXCVwTNzLPMb0BNARNrgAkEZL/vJw4wZ7gGyjMHnC2ncOFcZ3KULzJrlqhz27LHcgDEmNEL2HIGqponICOBLIAp4T1XXisjTQJyqzgQeAN4RkftwFcdDVVVDlaaQe+MNaNHCDTtZBIsXu3ECKlVyg81UqQKXXZbVxbQxxhSnkNYRqOosVT1VVU9W1ee8aU94QQBVXaeq3VW1g6p2VNX/hTI9IRUfDwsWuEb+RWgympLiRq7s08eNMXDDDe4hZcsNGGNCpbQri8PHlCnudfDgIm1m9Wr3vMBZZ7kxAyZOdMVCXboUQxqNMSYACwTF5f33XU1uixZF2kzGgPP+I1oWYTx7Y4zJlwWC4vDTT66pz3XXFXlTixdD48ZuOEljjCkJFgiKwwcfuHqBa64p8qYWL86eGzDGmFCzQFBUPh98+KF7dqBBgyJt6vff3bMDFgiMMSXJAkFRLVoEv/5aLMVCS5a4VwsExpiSZIGgqD74AKpWdYPTF9Hixa5n0c6diyFdxhgTJAsERaEKH38MV1xRLB3MLV4MHTq4ZqPGGFNSLBAURXy8K9i/8MIibyo9HZYutWIhY0zJs6EqiyIuzr0W8mmvGTPg889djxSNGsGhQxYIjDElzwJBUSxf7oYJa9++wKt+8gkMHOjev/tu1nQLBMaYkmaBoCji4lwQqFy5QKt99pkbcvLMM+GLL2DlSvj0Uzh6FE4+OURpNcaYXFggKCxVlyPIuK0P0rx5MGAAdOrkupiuWRMuuMD9GWNMabDK4sLauhX27Stw/cDf/+7qA7780gUBY4wpbRYICmv5cvcaGxv0Kikp8MMPbszh2rVDlC5jjCkgCwSFFRfnRo457bSgV1m+3NUDnHtuCNNljDEFZIGgsOLi4PTTXTAI0oIF7tUCgTGmLLFAUBg+H6xYUaBiIYBvv4U2baB+/RClyxhjCsECQWFs2QL79xeoojgtDRYuhPPOC2G6jDGmECwQFEYhKopXr4aDB61YyBhT9lggKIy4OPcQWbt2Qa9i9QPGmLLKAkFhLF/uugmtWDHoVb791j013LhxCNNljDGFYIGgoHw+FwgKUD/g88F331n9gDGmbLJAUFCbNrnC/gIEgrVrYe9eKxYyxpRNFggKKqOiuGvXoFfJqB+wHIExpiyyQFBQcXFQpQq0bRvU4j6f63K6aVM48cQQp80YYwrBeh8tqOXLoWNHN7hwEP7xD/jmG3j1VRAJcdqMMaYQLEdQEBlPFAdZPzB7Njz+OFx3Hdx9d4jTZowxhWSBoCB+/tmNJxnEg2RbtrgAcPrp8GN1PrYAAB/WSURBVM47lhswxpRdFggKImOM4iACwfDh7uL/yScQExPidBljTBFYICiIuDioWhVat85zsdRU99zA0KFw0kklkzRjjCmskAYCEektIhtFZLOIPJzLMgNFZJ2IrBWRD0KZniJbvtyNMZlPRfGPP0JSkg1Eb4wpH0IWCEQkCngD6AO0BQaLSNscy7QE/gZ0V9V2wMhQpafI0tODrihessS9nnlmiNNkjDHFIJQ5gjOAzaq6VVVTgClAvxzL3Aq8oap/Aqjq7yFMT9Fs3AhHjgRVP7BkCRx/PDRrVgLpMsaYIgplIGgMbPP7nOBN83cqcKqIfC8ii0WkdwjTUzQZFcVB5gjOPNNaChljyofSriyOBloC5wODgXdEpFbOhUTkNhGJE5G4xMTEEk6iJy7ONf/Jp6L4zz9d5sGKhYwx5UVQgUBEPhGRy0SkIIFjO9DU73MTb5q/BGCmqqaq6i/Az7jAkI2qjlXVWFWNrV9a4zwuXw6dO0NUVJ6LLVvmXi0QGGPKi2Av7G8C1wGbROR5EWkVxDrLgJYi0kJEKgGDgJk5lpmByw0gIvVwRUVbg0xTyUlLg5Urgy4WEilQn3TGGFOqggoEqjpXVa8HOgPxwFwRWSQiN4tIwNFZVDUNGAF8CawHpqnqWhF5WkT6eot9CewRkXXAN8BDqrqnaIcUAjt2wNGjQY1ItnixG6D+uONKIF3GGFMMgu50TkTqAjcANwIrgfeBHsAQvLv6nFR1FjArx7Qn/N4rcL/3V3Zl1Es0aJDnYqouR9C3b56LGWNMmRJUIBCRT4FWwCTgClXd6c2aKiJxoUpcmZERCPKpn9i6FfbssfoBY0z5EmyO4DVV/SbQDFXNv2F9eRdkIMh4kMyeKDbGlCfBVha39W/WKSK1ReTOEKWp7ClAIIiJCaoqwRhjyoxgA8Gtqrov44P3JPCtoUlSGZSY6PoXqlkzz8WWLHEPHgc5Zo0xxpQJwQaCKJGs52S9foQqhSZJZdAff0C9enk+KpySAqtWwRlnlGC6jDGmGAR77zoHVzH8H+/z7d60yJCYmG+x0E8/QXKyBQJjTPkTbCD4K+7iP9z7/BXwfyFJUVkURCDIeKLYHiQzxpQ3QQUCVfUBb3l/kScx0XUvkYdly1zp0YknllCajDGmmAT7HEFL4B+4cQWqZExX1cgYfyuIHMHSpS43YD2OGmPKm2Ari8fhcgNpwAXARGByqBJVpqSmwr597nY/F4cPw7p1Vj9gjCmfgg0EVVV1HiCq+quqjgYuC12yypA9XtdHeeQIVqwAn8/qB4wx5VOwlcXJXhfUm0RkBK476eqhS1YZEsTDZFZRbIwpz4LNEdwLxAD3AF1wnc8NCVWiypQgAsHSpW5Yynz6pDPGmDIp3xyB9/DYtar6IHAIuDnkqSpLgswRWP2AMaa8yjdHoKrpuO6mI1M+gWDPHtfrqBULGWPKq2DrCFaKyExgOnA4Y6KqfhKSVJUlf/zh2oTWqRNwdsaY9hYIjDHlVbCBoAqwB7jQb5oC4R8IEhOhdu1ce5JbutTFiSBGsTTGmDIp2CeLI6tewF8+D5MtWwatWtnQlMaY8ivYJ4vH4XIA2ajqsGJPUVmTTyCIi4NevUowPcYYU8yCLRr63O99FaA/sKP4k1MGJSbCqafmOmvnTujUqYTTZIwxxSjYoqGP/T+LyIfAwpCkqKxJTITu3QPOWrvWvdqIZMaY8izYB8pyagmE/+NTPp9rH5pL0dCaNe71tNNKME3GGFPMgq0jOEj2OoJduDEKwtu+fZCenmuHc2vWuAZFDRuWcLqMMaYYBVs0VCPUCSmT8nmYbM0alxuwrqeNMeVZUEVDItJfRGr6fa4lIleGLlllRB6BQNXVEVixkDGmvAu2juBJVd2f8UFV9wFPhiZJZUgegWDHDldyZIHAGFPeBRsIAi0XbNPT8iuPQGAVxcaYcBFsIIgTkX+JyMne37+A5aFMWJnwxx/uNY9AYE1HjTHlXbCB4G4gBZgKTAGSgLtClagyIzERatSAypWPmbVmDZxwAtStWwrpMsaYYhRsq6HDwMMhTkvZk5iYZ9NRKxYyxoSDYFsNfSUitfw+1xaRL0OXrDIil36GfD5rMWSMCR/BFg3V81oKAaCqfxIJTxbnEgh++QWOHrVAYIwJD8EGAp+INMv4ICLNCdAbaU4i0ltENorIZhHJtWhJRK4WERWR2CDTUzJyCQTWYsgYE06CbQL6KLBQRBYAApwD3JbXCt5Yx28AFwEJwDIRmamq63IsVwO4F1hSwLSHlqprNZRHIGjbtoTTZIwxIRBUjkBV5wCxwEbgQ+AB4Gg+q50BbFbVraqagmtt1C/Acs8AL+BaIpUdhw9DUlKugaB5c9egyBhjyrtgK4tvAebhAsCDwCRgdD6rNQa2+X1O8Kb5b7cz0FRVv8hn/7eJSJyIxCVmPOQVahn7CdBqyFoMGWPCSbB1BPcCXYFfVfUCoBOwL+9V8iYiFYB/4YJLnlR1rKrGqmps/TxGCytWv//uXhtkrxNPTYWNG+1BMmNM+Ag2ECSpahKAiFRW1Q1Aq3zW2Q409fvcxJuWoQZwGjBfROKBs4CZZabCeOdO95qjj+ktW1wwsPoBY0y4CLayOMF7jmAG8JWI/An8ms86y4CWItICFwAGAddlzPQ6scssdxGR+cCDqhoXfPJDaNcu93rCCdkmb9jgXtu0KeH0GGNMiAT7ZHF/7+1oEfkGqAnMyWedNBEZAXwJRAHvqepaEXkaiFPVmUVId+jt2uUGGshRNLR+vXttlV9+yBhjyokC9yCqqgsKsOwsYFaOaU/ksuz5BU1LSO3a5VoMRWf/ijZsgMaN4bjjSildxhhTzAo7ZnH427XrmGIhcDmC1q1LIT3GGBMiFghyEyAQqLocgdUPGGPCiQWC3AQIBDt2wMGDFgiMMeHFAkEgqgEDQUZFsRUNGWPCiQWCQPbvh+TkXAOB5QiMMeHEAkEgeTxDULNmwDpkY4wptywQBJJLIMhoMSRSCmkyxpgQsUAQSB45AisWMsaEGwsEgQQIBPv3u+6HLBAYY8KNBYJAdu6ESpWgVuYwzdZiyBgTtiwQBJLRdNSvMsA6mzPGhCsLBIHk8gxBpUrQokUppckYY0LEAkEguQSCli2P6YPOGGPKPQsEgQQIBNZiyBgTriwQ5JSW5sYr9gsEycmwdatVFBtjwpMFgpwSE11fQ36BYOtWSE+3wWiMMeHJAkFOGc8Q+I1VvGWLez355FJIjzHGhJgFgpwCPEyWEQhOOaUU0mOMMSFmgSCnXAJBjRpQr14ppckYY0LIAkFOGYHg+OMzJ23e7IqFrLM5Y0w4skCQ065drq/pqlUzJ23ZYsVCxpjwZYEgpxzPEKSnwy+/WEWxMSZ8WSDIaefObIFg2zZITbVAYIwJXxYIcsqRI7AWQ8aYcGeBIKdcAoHlCIwx4coCgb/Dh+HgwWMCQaVK0LhxKabLGGNCyAKBv9273atfINi8GU46CaKiSilNxhgTYhYI/CUkuNccOQIrFjLGhDMLBP6WLHGvnToBru85CwTGmHBngcDfwoVu9BnvqeLERDh0yFoMGWPCmwWCDD4ffP89nHNO5qTNm92r5QiMMeHMAkGGDRtgzx7o0SNzkjUdNcZEgpAGAhHpLSIbRWSziDwcYP79IrJORH4UkXkicmIo05OnhQvda45AUKECNG9eOkkyxpiSELJAICJRwBtAH6AtMFhE2uZYbCUQq6qnAx8BL4YqPflauBAaNMhWIbB5MzRtCpUrl1qqjDEm5EKZIzgD2KyqW1U1BZgC9PNfQFW/UdUj3sfFQJMQpidvCxe63IBfX9PWYsgYEwlCGQgaA9v8Pid403LzF2B2oBkicpuIxIlIXGJiYjEm0bN9u+ti1K9YCKz7aWNMZCgTlcUicgMQC7wUaL6qjlXVWFWNrV+/fvEn4Pvv3atfi6EDB1zzUcsRGGPCXXQIt70daOr3uYk3LRsR6QU8CpynqskhTE/uvvsOqlWDjh0zJ/30k3tt1apUUmSMMSUmlDmCZUBLEWkhIpWAQcBM/wVEpBPwH6Cvqv4ewrTkbeFCOOssiM6Ki/PmueoCv0yCMcaEpZAFAlVNA0YAXwLrgWmqulZEnhaRvt5iLwHVgekiskpEZuayudDZvx9+/PGY+oG5c6FLF6hTp8RTZIwxJSqURUOo6ixgVo5pT/i97xXK/Qdl6VL3VHH37pmTDh2CH36ABx8sxXQZY0wJKROVxaVq3Tr32qFD5qTvvoO0NOjZs5TSZIwxJcgCwaZNcNxx4Ncaae5c9xCZXybBGGPClgWCTZtcj6N+D5LNneuqDKpWLcV0GWNMCbFA8PPPcOqpmR9//93VHfcq/doLY4wpEZEdCJKT4bffXI7A8/XX7tXqB4wxkSKkrYbKvK1bXYshvxzB3LlQqxZ07lyK6TImSKmpqSQkJJCUlFTaSTFlRJUqVWjSpAkVK1YMep3IDgQ//+xevRyBKnz1FVx4oQ1Wb8qHhIQEatSoQfPmzRG/ei4TmVSVPXv2kJCQQIsWLYJeL7KLhjZtcq9eINiyxZUUWbGQKS+SkpKoW7euBQEDgIhQt27dAucQLRDUqwe1awPw5Zdu8kUXlWKajCkgCwLGX2F+D5EdCH7+OVtF8Zw5rrdRv0nGGBP2IjsQZDxDgGtA9PXX0Lt3KafJmHJkz549dOzYkY4dO3LCCSfQuHHjzM8pKSl5rhsXF8c999yT7z66detWXMk1uYjcyuLDh92ANF6LoYUL4cgRCwTGFETdunVZtWoVAKNHj6Z69eo86NdJV1paGtHRgS8zsbGxxMbG5ruPRYsWFU9iS1B6ejpR5ajFSeQGgs2b3auXI5g9GypVgvPPL70kGVMUI0eCd00uNh07wiuvFGydoUOHUqVKFVauXEn37t0ZNGgQ9957L0lJSVStWpVx48bRqlUr5s+fz5gxY/j8888ZPXo0v/32G1u3buW3335j5MiRmbmF6tWrc+jQIebPn8/o0aOpV68ea9asoUuXLkyePBkRYdasWdx///1Uq1aN7t27s3XrVj7//PNs6YqPj+fGG2/k8OHDALz++uuZuY0XXniByZMnU6FCBfr06cPzzz/P5s2bueOOO0hMTCQqKorp06ezbdu2zDQDjBgxgtjYWIYOHUrz5s259tpr+eqrrxg1ahQHDx5k7NixpKSkcMoppzBp0iRiYmLYvXs3d9xxB1u3bgXgrbfeYs6cOdSpU4eRI0cC8Oijj9KgQQPuvffeQp+7gojcQJCjxdCcOW7sgerVSzFNxoSJhIQEFi1aRFRUFAcOHOC7774jOjqauXPn8sgjj/Dxxx8fs86GDRv45ptvOHjwIK1atWL48OHHtIVfuXIla9eupVGjRnTv3p3vv/+e2NhYbr/9dr799ltatGjB4MGDA6apQYMGfPXVV1SpUoVNmzYxePBg4uLimD17Nv/9739ZsmQJMTEx7N27F4Drr7+ehx9+mP79+5OUlITP52Pbtm0Bt52hbt26rFixAnDFZrfeeisAjz32GO+++y53330399xzD+eddx6ffvop6enpHDp0iEaNGnHVVVcxcuRIfD4fU6ZMYenSpQX+3gvLAkHLlmzbBmvXws03l26SjCmKgt65h9I111yTWTSyf/9+hgwZwqZNmxARUlNTA65z2WWXUblyZSpXrkyDBg3YvXs3TZo0ybbMGWeckTmtY8eOxMfHU716dU466aTMdvODBw9m7Nixx2w/NTWVESNGsGrVKqKiovjZe45o7ty53HzzzcTExABQp04dDh48yPbt2+nfvz/gHtIKxrXXXpv5fs2aNTz22GPs27ePQ4cOcckllwDw9ddfM3HiRACioqKoWbMmNWvWpG7duqxcuZLdu3fTqVMn6tatG9Q+i0PkBoKff4aGDaF6db6c4iZZ/YAxxaNatWqZ7x9//HEuuOACPv30U+Lj4zk/l/LXypUrZ76PiooiLS2tUMvk5uWXX+b4449n9erV+Hy+oC/u/qKjo/H5fJmfc7bX9z/uoUOHMmPGDDp06MD48eOZP39+ntu+5ZZbGD9+PLt27WLYsGEFTltRRG6rIb8WQ7NnQ5Mm0LZtKafJmDC0f/9+GjduDMD48eOLffutWrVi69atxMfHAzB16tRc09GwYUMqVKjApEmTSE9PB+Ciiy5i3LhxHDlyBIC9e/dSo0YNmjRpwowZMwBITk7myJEjnHjiiaxbt47k5GT27dvHvHnzck3XwYMHadiwIampqbz//vuZ03v27Mlbb70FuErl/fv3A9C/f3/mzJnDsmXLMnMPJSWyA8Gpp5Ka6voX6t07W0/UxphiMmrUKP72t7/RqVOnAt3BB6tq1aq8+eab9O7dmy5dulCjRg1q1qx5zHJ33nknEyZMoEOHDmzYsCHz7r1379707duX2NhYOnbsyJgxYwCYNGkSr732GqeffjrdunVj165dNG3alIEDB3LaaacxcOBAOnXqlGu6nnnmGc4880y6d+9O69atM6e/+uqrfPPNN7Rv354uXbqwzhscq1KlSlxwwQUMHDiwxFsciaqW6A6LKjY2VuPi4oq2kf37Xc9yL7zAgjNHcf758NFHcPXVxZJEY0rM+vXradOmTWkno9QdOnSI6tWro6rcddddtGzZkvvuu6+0k1UgPp+Pzp07M336dFoW8anWQL8LEVmuqgHb60ZmjsCvovj99yEmBi6+uHSTZIwpvHfeeYeOHTvSrl079u/fz+23317aSSqQdevWccopp9CzZ88iB4HCiMzK4pUrAUhq2pKpU11OoEaNUk6TMabQ7rvvvnKXA/DXtm3bzOcKSkPk5QgOHYKnnoKOHfnvz204cACGDi3tRBljTOmJvBzBM8+4riWmT2fcU1E0a2ZPExtjIltk5QjWrYN//QuGDWN7s7P56iu46SaoEFnfgjHGZBM5l0BVuOsuVxnw/PNMnuxGqRwypLQTZowxpStyAsGHH8L8+fD3v6P16jNhAnTvDqecUtoJM6b8uuCCC/gyY0QnzyuvvMLw4cNzXef8888nown4pZdeyr59+45ZZvTo0Znt+XMzY8aMzDb4AE888QRz584tSPKNJ3ICQYMGcM01cOutLF0K69dbJbExRTV48GCmTJmSbdqUKVNy7fgtp1mzZlGrVq1C7TtnIHj66afp1atXobZVWjKebi5tkRMIevWCadMgKornnoOaNV1cMCZsjBzpWj4U55/XLXJuBgwYwBdffJE5CE18fDw7duzgnHPOYfjw4cTGxtKuXTuefPLJgOs3b96cP/74A4DnnnuOU089lR49erBx48bMZd555x26du1Khw4duPrqqzly5AiLFi1i5syZPPTQQ3Ts2JEtW7YwdOhQPvroIwDmzZtHp06daN++PcOGDSM5OTlzf08++SSdO3emffv2bNiw4Zg0xcfHc84559C5c2c6d+6cbTyEF154gfbt29OhQwcefvhhADZv3kyvXr3o0KEDnTt3ZsuWLcyfP5/LL788c70RI0Zkdq/RvHlz/vrXv2Y+PBbo+AB2795N//796dChAx06dGDRokU88cQTvOLXu+Cjjz7Kq6++muc5CkbkBALPokXw2WcwapQLBsaYwqtTpw5nnHEGs2fPBlxuYODAgYgIzz33HHFxcfz4448sWLCAH3/8MdftLF++nClTprBq1SpmzZrFsmXLMuddddVVLFu2jNWrV9OmTRveffddunXrRt++fXnppZdYtWoVJ598cubySUlJDB06lKlTp/LTTz+RlpaW2bcPQL169VixYgXDhw8PWPyU0V31ihUrmDp1aua4CP7dVa9evZpRo0YBrrvqu+66i9WrV7No0SIaNmyY7/eW0V31oEGDAh4fkNld9erVq1mxYgXt2rVj2LBhmT2XZnRXfcMNN+S7v/xEVPNRVXjkETj+eCih8R6MKTml1A91RvFQv379mDJlSuaFbNq0aYwdO5a0tDR27tzJunXrOP300wNu47vvvqN///6ZXUH37ds3c15u3TnnZuPGjbRo0YJTvdEHhwwZwhtvvJE56MtVV10FQJcuXfjkk0+OWT8Su6uOqEDwv//BggXw+uvg11usMaYI+vXrx3333ceKFSs4cuQIXbp04ZdffmHMmDEsW7aM2rVrM3To0GO6bA5WQbtzzk9GV9a5dWMdid1Vh7RoSER6i8hGEdksIg8HmF9ZRKZ685eISPNQpcXng7/9DVq0AG/QIGNMMahevToXXHABw4YNy6wkPnDgANWqVaNmzZrs3r07s+goN+eeey4zZszg6NGjHDx4kM8++yxzXm7dOdeoUYODBw8es61WrVoRHx/PZm842kmTJnHeeecFfTyR2F11yAKBiEQBbwB9gLbAYBHJ2eP/X4A/VfUU4GXghVCl56OPXBdDTz/txiY2xhSfwYMHs3r16sxA0KFDBzp16kTr1q257rrr6N69e57rd+7cmWuvvZYOHTrQp08funbtmjkvt+6cBw0axEsvvUSnTp3YsmVL5vQqVaowbtw4rrnmGtq3b0+FChW44447gj6WSOyuOmTdUIvI2cBoVb3E+/w3AFX9h98yX3rL/CAi0cAuoL7mkajCdkM9axaMHQsffwwl3NW3MSFj3VBHnmC6qy5L3VA3BvxHek7wpgVcRlXTgP1ASAbqvPRSmDHDgoAxpvwKVXfV5aKyWERuA24DaNasWSmnxhhjSkeouqsOZY5gO9DU73MTb1rAZbyioZrAnpwbUtWxqhqrqrH169cPUXKNKZ/K2yiDJrQK83sIZSBYBrQUkRYiUgkYBMzMscxMIKPbtwHA13nVDxhjsqtSpQp79uyxYGAAFwT27NlT4CavISsaUtU0ERkBfAlEAe+p6loReRqIU9WZwLvAJBHZDOzFBQtjTJCaNGlCQkICiYmJpZ0UU0ZUqVKFJk2aFGidyBy83hhjIowNXm+MMSZXFgiMMSbCWSAwxpgIV+7qCEQkEfi1AKvUA/4IUXLKskg87kg8ZojM447EY4aiHfeJqhqw/X25CwQFJSJxuVWQhLNIPO5IPGaIzOOOxGOG0B23FQ0ZY0yEs0BgjDERLhICwdjSTkApicTjjsRjhsg87kg8ZgjRcYd9HYExxpi8RUKOwBhjTB4sEBhjTIQL60CQ35jJ4UBEmorINyKyTkTWisi93vQ6IvKViGzyXmuXdlqLm4hEichKEfnc+9zCG/t6szcWdtgNSioitUTkIxHZICLrReTsCDnX93m/7zUi8qGIVAm38y0i74nI7yKyxm9awHMrzmvesf8oIp2Lsu+wDQRBjpkcDtKAB1S1LXAWcJd3nA8D81S1JTDP+xxu7gXW+31+AXjZGwP7T9yY2OHmVWCOqrYGOuCOP6zPtYg0Bu4BYlX1NFxvxoMIv/M9HuidY1pu57YP0NL7uw14qyg7DttAAJwBbFbVraqaAkwB+pVymoqdqu5U1RXe+4O4C0Nj3LFO8BabAFxZOikMDRFpAlwG/J/3WYALgY+8RcLxmGsC5+K6b0dVU1R1H2F+rj3RQFVvAKsYYCdhdr5V9Vtcd/z+cju3/YCJ6iwGaolIw8LuO5wDQTBjJocVEWkOdAKWAMer6k5v1i7g+FJKVqi8AowCfN7nusA+b+xrCM/z3QJIBMZ5RWL/JyLVCPNzrarbgTHAb7gAsB9YTvifb8j93Bbr9S2cA0FEEZHqwMfASFU94D/PG/UtbNoJi8jlwO+qury001LCooHOwFuq2gk4TI5ioHA71wBeuXg/XCBsBFTj2CKUsBfKcxvOgSCYMZPDgohUxAWB91X1E2/y7oysovf6e2mlLwS6A31FJB5X5Hchruy8lld0AOF5vhOABFVd4n3+CBcYwvlcA/QCflHVRFVNBT7B/QbC/XxD7ue2WK9v4RwIghkzudzzysbfBdar6r/8ZvmPBz0E+G9Jpy1UVPVvqtpEVZvjzuvXqno98A1u7GsIs2MGUNVdwDYRaeVN6gmsI4zPtec34CwRifF+7xnHHdbn25PbuZ0J3OS1HjoL2O9XhFRwqhq2f8ClwM/AFuDR0k5PiI6xBy67+COwyvu7FFdmPg/YBMwF6pR2WkN0/OcDn3vvTwKWApuB6UDl0k5fCI63IxDnne8ZQO1IONfAU8AGYA0wCagcbucb+BBXB5KKy/39JbdzCwiuVeQW4Cdci6pC79u6mDDGmAgXzkVDxhhjgmCBwBhjIpwFAmOMiXAWCIwxJsJZIDDGmAhngcCYEBOR8zN6SDWmLLJAYIwxEc4CgTEeEblBRJaKyCoR+Y833sEhEXnZ6wt/nojU95btKCKLvb7gP/XrJ/4UEZkrIqtFZIWInOxtvrrfOALve0/IIiLPe2NJ/CgiY0rp0E2Es0BgDCAibYBrge6q2hFIB67HdXAWp6rtgAXAk94qE4G/qurpuCc7M6a/D7yhqh2AbrgnRcH1CjsSNzbGSUB3EakL9Afaedt5NrRHaUxgFgiMcXoCXYBlIrLK+3wSrpvrqd4yk4Ee3rgAtVR1gTd9AnCuiNQAGqvqpwCqmqSqR7xllqpqgqr6cN2ANMd1p5wEvCsiVwEZyxpToiwQGOMIMEFVO3p/rVR1dIDlCtsnS7Lf+3QgWl1f+mfgehG9HJhTyG0bUyQWCIxx5gEDRKQBZI4VeyLufySjh8vrgIWquh/4U0TO8abfCCxQN0Jcgohc6W2jsojE5LZDbwyJmqo6C7gPN/SkMSUuOv9FjAl/qrpORB4D/iciFXA9QN6FG/zlDG/e77h6BHBdAr/tXei3Ajd7028E/iMiT3vbuCaP3dYA/isiVXA5kvuL+bCMCYr1PmpMHkTkkKpWL+10GBNKVjRkjDERznIExhgT4SxHYIwxEc4CgTHGRDgLBMYYE+EsEBhjTISzQGCMMRHu/wGCLjhfTfkVzQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbKdWsMkDuw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c05ff58c-0d09-4be0-d81d-0210984ffd3e"
      },
      "source": [
        "# classification report\n",
        "predicted_classes = np.argmax(np.round(model_cnn.predict(X_test[:,:,:,None])),axis=1)\n",
        "correct = np.where(predicted_classes==y_test)[0]\n",
        "target_names = [f\"Class {label}\" for label in range(len(np.unique(df['label'])))]\n",
        "\n",
        "print(f\"From {len(y_test)} labels're founding {len(correct)} correct labels.\")\n",
        "print(f'Accuracy: {len(correct)/len(y_test)}')\n",
        "print('')\n",
        "print(classification_report(y_test, predicted_classes, target_names=target_names))"
      ],
      "id": "wFbKdWsMkDuw",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From 280 labels're founding 266 correct labels.\n",
            "Accuracy: 0.95\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.38      1.00      0.56         5\n",
            "     Class 1       1.00      1.00      1.00         5\n",
            "     Class 2       1.00      1.00      1.00         5\n",
            "     Class 3       1.00      0.40      0.57         5\n",
            "     Class 4       1.00      1.00      1.00         5\n",
            "     Class 5       1.00      1.00      1.00         5\n",
            "     Class 6       1.00      1.00      1.00         5\n",
            "     Class 7       0.71      1.00      0.83         5\n",
            "     Class 8       1.00      1.00      1.00         5\n",
            "     Class 9       1.00      1.00      1.00         5\n",
            "    Class 10       1.00      1.00      1.00         5\n",
            "    Class 11       1.00      1.00      1.00         5\n",
            "    Class 12       1.00      1.00      1.00         5\n",
            "    Class 13       1.00      1.00      1.00         5\n",
            "    Class 14       1.00      1.00      1.00         5\n",
            "    Class 15       1.00      1.00      1.00         5\n",
            "    Class 16       1.00      1.00      1.00         5\n",
            "    Class 17       1.00      1.00      1.00         5\n",
            "    Class 18       1.00      1.00      1.00         5\n",
            "    Class 19       1.00      1.00      1.00         5\n",
            "    Class 20       0.80      0.80      0.80         5\n",
            "    Class 21       1.00      1.00      1.00         5\n",
            "    Class 22       1.00      1.00      1.00         5\n",
            "    Class 23       1.00      1.00      1.00         5\n",
            "    Class 24       1.00      1.00      1.00         5\n",
            "    Class 25       1.00      0.80      0.89         5\n",
            "    Class 26       1.00      1.00      1.00         5\n",
            "    Class 27       1.00      1.00      1.00         5\n",
            "    Class 28       1.00      1.00      1.00         5\n",
            "    Class 29       1.00      1.00      1.00         5\n",
            "    Class 30       0.80      0.80      0.80         5\n",
            "    Class 31       1.00      1.00      1.00         5\n",
            "    Class 32       1.00      0.80      0.89         5\n",
            "    Class 33       1.00      1.00      1.00         5\n",
            "    Class 34       1.00      1.00      1.00         5\n",
            "    Class 35       1.00      1.00      1.00         5\n",
            "    Class 36       1.00      1.00      1.00         5\n",
            "    Class 37       1.00      1.00      1.00         5\n",
            "    Class 38       0.80      0.80      0.80         5\n",
            "    Class 39       1.00      1.00      1.00         5\n",
            "    Class 40       1.00      0.60      0.75         5\n",
            "    Class 41       1.00      1.00      1.00         5\n",
            "    Class 42       1.00      1.00      1.00         5\n",
            "    Class 43       0.83      1.00      0.91         5\n",
            "    Class 44       1.00      1.00      1.00         5\n",
            "    Class 45       1.00      1.00      1.00         5\n",
            "    Class 46       1.00      1.00      1.00         5\n",
            "    Class 47       1.00      1.00      1.00         5\n",
            "    Class 48       1.00      1.00      1.00         5\n",
            "    Class 49       1.00      1.00      1.00         5\n",
            "    Class 50       1.00      1.00      1.00         5\n",
            "    Class 51       1.00      0.80      0.89         5\n",
            "    Class 52       1.00      0.60      0.75         5\n",
            "    Class 53       1.00      1.00      1.00         5\n",
            "    Class 54       1.00      0.80      0.89         5\n",
            "    Class 55       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.95       280\n",
            "   macro avg       0.97      0.95      0.95       280\n",
            "weighted avg       0.97      0.95      0.95       280\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExkYoE9LXq6I"
      },
      "source": [
        "## Other people \n",
        "men and woman each 10 audio file."
      ],
      "id": "ExkYoE9LXq6I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYxW624FZZdM",
        "outputId": "de05af18-c67f-4d6a-9b3e-1ccf71b1e7a7"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(77)\n",
        "\n",
        "random_fruit_veget = np.random.choice(np.unique(df['description']),10,replace=False).tolist()\n",
        "print(random_fruit_veget)"
      ],
      "id": "AYxW624FZZdM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ผักกุยช่าย', 'ทับทิม', 'ใบชะพลู1', 'พริกขี้หนู', 'มันสำปะหลัง', 'ฟักทอง', 'มะพลับ', 'หัวหอม2', 'ใบชะพลู2', 'บวบ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTRoVzSzX3XM"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Audio data source\n",
        "other_source = {'audio_time':[],\n",
        "                'sampling_rate':[],\n",
        "                'feature_extraction':[],\n",
        "                'label':[],\n",
        "                'description':[]}\n",
        "\n",
        "for types in random_fruit_veget:\n",
        "    audio_files = glob('/content/Project_499/Record_other_audio/' + str(types) + '/*.wav')\n",
        "\n",
        "    for audio in audio_files:\n",
        "        y,sr = librosa.load(audio,duration=5,offset=0)\n",
        "        other_source['description'].append(str(types))\n",
        "        other_source['audio_time'].append(y)\n",
        "        other_source['sampling_rate'].append(sr)\n",
        "        other_source['feature_extraction'].append(feature_mfcc(y,sr))\n",
        "\n",
        "other_source['label'] = [18,18,14,14,39,39,44,44,37,37,50,50,25,25,55,55,40,40,38,38]\n",
        "other_df = pd.DataFrame.from_dict(other_source)"
      ],
      "id": "XTRoVzSzX3XM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ6EclMVZCeE"
      },
      "source": [
        "other_X_test = np.array(other_df['feature_extraction'].to_list())\n",
        "other_y_test = np.array(other_df['label'].to_list())"
      ],
      "id": "rQ6EclMVZCeE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_isU-4c97L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce5e5ce2-49b4-426f-ae06-132d78ed414c"
      },
      "source": [
        "# classification report\n",
        "predicted_classes = np.argmax(np.round(model_cnn.predict(other_X_test[:,:,:,None])),axis=1)\n",
        "correct = np.where(predicted_classes==other_y_test)[0]\n",
        "target_names = [f\"Class {label}\" for label in np.unique(other_source['label'])]\n",
        "\n",
        "print(f\"From {len(other_y_test)} labels're founding {len(correct)} correct labels.\")\n",
        "print(f'Accuracy: {len(correct)/len(other_y_test)}')"
      ],
      "id": "uG_isU-4c97L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From 20 labels're founding 6 correct labels.\n",
            "Accuracy: 0.3\n"
          ]
        }
      ]
    }
  ]
}