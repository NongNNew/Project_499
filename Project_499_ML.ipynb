{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_499_ML.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOO7+pwTza3vo96hgUsx9ez",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NongNNew/Project_499/blob/main/Project_499_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E09RIQ3sEE0",
        "outputId": "36e6dadc-2f74-4618-8500-934184e0fe02"
      },
      "source": [
        "!pip install pythainlp\n",
        "!git clone https://github.com/NongNNew/Project_499.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-2.3.2-py3-none-any.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 2.6 MB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n",
            "Collecting tinydb>=3.0\n",
            "  Downloading tinydb-4.5.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Collecting typing-extensions<4.0.0,>=3.10.0\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: typing-extensions, tinydb, python-crfsuite, pythainlp\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.7.4.3\n",
            "    Uninstalling typing-extensions-3.7.4.3:\n",
            "      Successfully uninstalled typing-extensions-3.7.4.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\u001b[0m\n",
            "Successfully installed pythainlp-2.3.2 python-crfsuite-0.9.7 tinydb-4.5.1 typing-extensions-3.10.0.2\n",
            "Cloning into 'Project_499'...\n",
            "remote: Enumerating objects: 1550, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 1550 (delta 44), reused 93 (delta 38), pack-reused 1444\u001b[K\n",
            "Receiving objects: 100% (1550/1550), 237.68 MiB | 27.06 MiB/s, done.\n",
            "Resolving deltas: 100% (192/192), done.\n",
            "Checking out files: 100% (1144/1144), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWQl_FU7ux5V"
      },
      "source": [
        "from pythainlp import word_tokenize, Tokenizer, sent_tokenize ,syllable_tokenize\n",
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp.util import dict_trie\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlrSe-DOGtpR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "6dac7880-90fb-4d05-cc7b-3be0ff22016b"
      },
      "source": [
        "df = pd.read_csv('/content/Project_499/word_segment.csv')\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>thai word</th>\n",
              "      <th>native word</th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>word3</th>\n",
              "      <th>word4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>กล้วยน้ำว้า</td>\n",
              "      <td>ก้วยไต้</td>\n",
              "      <td>ก้วย</td>\n",
              "      <td>ไต้</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>กล้วยหอม</td>\n",
              "      <td>ก้วยค้าว</td>\n",
              "      <td>ก้วย</td>\n",
              "      <td>ค้าว</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>กระท้อน</td>\n",
              "      <td>บ่าตื๋น</td>\n",
              "      <td>บ่า</td>\n",
              "      <td>ตื๋น</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ตะขบไทย</td>\n",
              "      <td>บ่าเกว๋นควาย</td>\n",
              "      <td>บ่า</td>\n",
              "      <td>เกว๋น</td>\n",
              "      <td>ควาย</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ทับทิม</td>\n",
              "      <td>บ่าก๊อ</td>\n",
              "      <td>บ่า</td>\n",
              "      <td>ก๊อ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     thai word   native word word1  word2 word3 word4\n",
              "0  กล้วยน้ำว้า       ก้วยไต้  ก้วย    ไต้   NaN   NaN\n",
              "1     กล้วยหอม      ก้วยค้าว  ก้วย   ค้าว   NaN   NaN\n",
              "2      กระท้อน       บ่าตื๋น   บ่า   ตื๋น   NaN   NaN\n",
              "3      ตะขบไทย  บ่าเกว๋นควาย   บ่า  เกว๋น  ควาย   NaN\n",
              "4       ทับทิม        บ่าก๊อ   บ่า    ก๊อ   NaN   NaN"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzWt6RLN9OGW"
      },
      "source": [
        "words = list()\n",
        "\n",
        "for word in df.columns[2:]:\n",
        "    words += df[word].dropna().to_list()\n",
        "\n",
        "words = dict.fromkeys(np.unique(words).tolist(),0)\n",
        "\n",
        "custom_words_list = set(thai_words())\n",
        "custom_words_list.update(words)\n",
        "trie = dict_trie(dict_source=custom_words_list)\n",
        "custom_tokenizer = Tokenizer(custom_dict=trie, engine='newmm', keep_whitespace=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAL-RqcbFhrV",
        "outputId": "08ffa2f8-e64c-494b-c071-4bc400e6b98d"
      },
      "source": [
        "text = 'ฉันชอบกินก้วยไต้และบ่าเกว๋นควาย'\n",
        "segment = custom_tokenizer.word_tokenize(text)\n",
        "print(segment)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ฉัน', 'ชอบ', 'กิน', 'ก้วย', 'ไต้', 'และ', 'บ่า', 'เกว๋น', 'ควาย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi9zmXZpCOsa"
      },
      "source": [
        "count = list()\n",
        "for word in words:\n",
        "    num=0\n",
        "    for segment in custom_tokenizer.word_tokenize(text):\n",
        "        if segment == word:\n",
        "            num+=1\n",
        "    count.append(num)\n",
        "\n",
        "for key,values in words.items():\n",
        "    for segment in custom_tokenizer.word_tokenize(text):\n",
        "        if segment == key:\n",
        "            words[key]+=1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWK_DudZKxHx"
      },
      "source": [
        "words"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}